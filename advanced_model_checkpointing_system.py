#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ ADVANCED MODEL CHECKPOINTING SYSTEM
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint'–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º

–†–µ—à–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –∏–∑ exported-assets:
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∞–≤—Ç–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏ —Å–±–æ—è—Ö NaN/Inf
- –ü—Ä–æ—Å—Ç—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ validation loss)  
- –ù–µ—Ç –∑–∞—â–∏—Ç—ã –æ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
- –§—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å checkpoint —Å–∏—Å—Ç–µ–º
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–∏—Å–∫–æ–≤—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
1. IntelligentCheckpointManager - —É–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ checkpoint'–∞–º–∏
2. MultiCriteriaModelSelector - –≤—ã–±–æ—Ä –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
3. AutoRecoverySystem - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ —Å–±–æ—è—Ö
4. CheckpointHealthAnalyzer - –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ checkpoint'–æ–≤
"""

import os
import time
import json
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
import torch
import torch.nn as nn

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å existing —Å–∏—Å—Ç–µ–º–∞–º–∏
try:
    from unified_logging_system import UnifiedLoggingSystem
    UNIFIED_LOGGING_AVAILABLE = True
except ImportError:
    UNIFIED_LOGGING_AVAILABLE = False

class CheckpointQuality(Enum):
    """–£—Ä–æ–≤–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–∞ checkpoint'–æ–≤"""
    EXCELLENT = "excellent"    # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è production
    GOOD = "good"             # –•–æ—Ä–æ—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    ACCEPTABLE = "acceptable"  # –ü—Ä–∏–µ–º–ª–µ–º—ã–µ –º–æ–¥–µ–ª–∏
    POOR = "poor"             # –ü–ª–æ—Ö–∏–µ –º–æ–¥–µ–ª–∏ (–∫–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ)
    CRITICAL = "critical"     # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ (–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ)

@dataclass
class CheckpointMetrics:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ checkpoint'–∞"""
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    epoch: int
    global_step: int
    validation_loss: float
    training_loss: float
    learning_rate: float
    
    # TTS-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    attention_diagonality: float = 0.5
    gate_accuracy: float = 0.8
    mel_reconstruction_quality: float = 0.7
    attention_stability: float = 0.6
    
    # –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    gradient_norm: float = 2.0
    gradient_stability: float = 1.0
    loss_trend: float = -0.1
    convergence_score: float = 0.5
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    timestamp: str = ""
    training_time: float = 0.0
    model_size_mb: float = 100.0
    memory_usage_mb: float = 2048.0
    
    # –§–ª–∞–≥–∏ –ø—Ä–æ–±–ª–µ–º
    has_nan_weights: bool = False
    has_gradient_explosion: bool = False
    has_attention_collapse: bool = False
    is_stable: bool = True

@dataclass
class CheckpointInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ checkpoint'–µ"""
    path: str
    metrics: CheckpointMetrics
    quality: CheckpointQuality
    health_score: float
    is_best: bool = False
    is_emergency_backup: bool = False
    file_hash: str = ""
    creation_time: float = 0.0

class MultiCriteriaModelSelector:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ validation loss
    """
    
    def __init__(self, weights: Optional[Dict[str, float]] = None):
        # –í–µ—Å–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ (–Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è TTS –∑–∞–¥–∞—á)
        self.weights = weights or {
            'validation_loss': 0.25,      # –û—Å–Ω–æ–≤–Ω–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–π
            'attention_quality': 0.20,    # –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è TTS
            'stability': 0.15,            # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            'convergence': 0.15,          # –°–∫–æ—Ä–æ—Å—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏  
            'mel_quality': 0.10,          # –ö–∞—á–µ—Å—Ç–≤–æ mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º
            'gradient_health': 0.10,      # –ó–¥–æ—Ä–æ–≤—å–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            'gate_accuracy': 0.05         # –¢–æ—á–Ω–æ—Å—Ç—å gate –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        }
        
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("MultiCriteriaSelector")
        else:
            logger = logging.getLogger("MultiCriteriaSelector") 
            logger.setLevel(logging.INFO)
            return logger
    
    def calculate_model_score(self, metrics: CheckpointMetrics) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ score –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        
        Returns:
            float: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π score (0.0 - 1.0, –≥–¥–µ 1.0 = –∏–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)
        """
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ score
            components = {}
            
            # 1. Validation Loss (–∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
            val_loss_norm = max(0.0, min(1.0, (10.0 - metrics.validation_loss) / 10.0))
            components['validation_loss'] = val_loss_norm
            
            # 2. Attention Quality (–¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å)
            components['attention_quality'] = min(1.0, metrics.attention_diagonality / 0.8)
            
            # 3. Stability Score
            stability_score = self._calculate_stability_score(metrics)
            components['stability'] = stability_score
            
            # 4. Convergence Score  
            components['convergence'] = min(1.0, max(0.0, metrics.convergence_score))
            
            # 5. Mel Quality
            components['mel_quality'] = min(1.0, max(0.0, metrics.mel_reconstruction_quality))
            
            # 6. Gradient Health
            grad_health = 1.0 - min(1.0, abs(metrics.gradient_norm - 2.0) / 10.0)
            components['gradient_health'] = grad_health
            
            # 7. Gate Accuracy
            components['gate_accuracy'] = min(1.0, max(0.0, metrics.gate_accuracy))
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            total_score = sum(
                self.weights[key] * value 
                for key, value in components.items()
            )
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à—Ç—Ä–∞—Ñ–æ–≤ –∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            if metrics.has_nan_weights:
                total_score *= 0.1  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —à—Ç—Ä–∞—Ñ
            if metrics.has_gradient_explosion:
                total_score *= 0.3  # –°–µ—Ä—å–µ–∑–Ω—ã–π —à—Ç—Ä–∞—Ñ
            if metrics.has_attention_collapse:
                total_score *= 0.2  # –°–µ—Ä—å–µ–∑–Ω—ã–π —à—Ç—Ä–∞—Ñ
                
            return max(0.0, min(1.0, total_score))
            
        except Exception as e:
            self.logger.error(f"Error calculating model score: {e}")
            return 0.0
    
    def _calculate_stability_score(self, metrics: CheckpointMetrics) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ score —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è"""
        stability_factors = []
        
        # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        grad_stability = max(0.0, 1.0 - metrics.gradient_stability / 5.0)
        stability_factors.append(grad_stability)
        
        # –¢—Ä–µ–Ω–¥ loss (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π = —Ö–æ—Ä–æ—à–æ)
        loss_trend_score = max(0.0, 1.0 + metrics.loss_trend) if metrics.loss_trend < 0 else 0.5
        stability_factors.append(loss_trend_score)
        
        # Attention —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        stability_factors.append(min(1.0, metrics.attention_stability))
        
        # –û–±—â–∏–π —Ñ–ª–∞–≥ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        stability_factors.append(1.0 if metrics.is_stable else 0.3)
        
        return np.mean(stability_factors)
    
    def rank_checkpoints(self, checkpoints: List[CheckpointInfo]) -> List[CheckpointInfo]:
        """–†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ checkpoint'–æ–≤ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É"""
        if not checkpoints:
            return []
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ checkpoint'–∞
        for checkpoint in checkpoints:
            score = self.calculate_model_score(checkpoint.metrics)
            checkpoint.health_score = score
            
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score (—É–±—ã–≤–∞–Ω–∏–µ)
        ranked = sorted(checkpoints, key=lambda x: x.health_score, reverse=True)
        
        # –ú–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ –ª—É—á—à–µ–≥–æ checkpoint'–∞
        if ranked:
            ranked[0].is_best = True
            
        self.logger.info(f"Ranked {len(checkpoints)} checkpoints. Best score: {ranked[0].health_score:.4f}")
        return ranked

class CheckpointHealthAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–¥–æ—Ä–æ–≤—å—è checkpoint'–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø—Ä–æ–±–ª–µ–º"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("CheckpointHealthAnalyzer")
        else:
            logger = logging.getLogger("CheckpointHealthAnalyzer") 
            logger.setLevel(logging.INFO)
            return logger
    
    def analyze_checkpoint_health(self, checkpoint_path: str) -> Tuple[CheckpointQuality, List[str]]:
        """
        –ê–Ω–∞–ª–∏–∑ –∑–¥–æ—Ä–æ–≤—å—è checkpoint'–∞
        
        Returns:
            Tuple[CheckpointQuality, List[str]]: –ö–∞—á–µ—Å—Ç–≤–æ –∏ —Å–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
        """
        issues = []
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint'–∞
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            required_keys = ['model_state_dict', 'epoch', 'global_step']
            missing_keys = [key for key in required_keys if key not in checkpoint]
            if missing_keys:
                issues.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è: {missing_keys}")
                return CheckpointQuality.CRITICAL, issues
            
            # –ê–Ω–∞–ª–∏–∑ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏
            state_dict = checkpoint['model_state_dict']
            weight_issues = self._analyze_model_weights(state_dict)
            issues.extend(weight_issues)
            
            # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è
            metrics_issues = self._analyze_training_metrics(checkpoint)
            issues.extend(metrics_issues)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–±–ª–µ–º
            quality = self._determine_quality_from_issues(issues)
            
            if issues:
                self.logger.warning(f"Checkpoint health issues found: {len(issues)} problems")
            else:
                self.logger.info("Checkpoint health: No critical issues detected")
                
            return quality, issues
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ checkpoint: {e}"
            issues.append(error_msg)
            self.logger.error(error_msg)
            return CheckpointQuality.CRITICAL, issues
    
    def _analyze_model_weights(self, state_dict: Dict[str, torch.Tensor]) -> List[str]:
        """–ê–Ω–∞–ª–∏–∑ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–±–ª–µ–º"""
        issues = []
        
        for name, tensor in state_dict.items():
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
                if torch.isnan(tensor).any():
                    issues.append(f"NaN –≤–µ—Å–∞ –≤ —Å–ª–æ–µ: {name}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ Inf  
                if torch.isinf(tensor).any():
                    issues.append(f"Inf –≤–µ—Å–∞ –≤ —Å–ª–æ–µ: {name}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                tensor_max = tensor.abs().max().item()
                if tensor_max > 100.0:
                    issues.append(f"–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞ –≤ {name}: {tensor_max:.2f}")
                    
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "–º–µ—Ä—Ç–≤—ã–µ" –≤–µ—Å–∞
                if tensor_max < 1e-8:
                    issues.append(f"–ú–µ—Ä—Ç–≤—ã–µ –≤–µ—Å–∞ –≤ {name}: max={tensor_max:.2e}")
                    
            except Exception as e:
                issues.append(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ—Å–æ–≤ {name}: {e}")
                
        return issues
    
    def _analyze_training_metrics(self, checkpoint: Dict[str, Any]) -> List[str]:
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
        issues = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        if 'validation_loss' in checkpoint:
            val_loss = checkpoint['validation_loss']
            if val_loss > 50.0:
                issues.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤—ã—Å–æ–∫–∏–π validation loss: {val_loss:.2f}")
            elif val_loss > 20.0:
                issues.append(f"–í—ã—Å–æ–∫–∏–π validation loss: {val_loss:.2f}")
                
        return issues
    
    def _determine_quality_from_issues(self, issues: List[str]) -> CheckpointQuality:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ checkpoint'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º"""
        if not issues:
            return CheckpointQuality.EXCELLENT
        
        critical_keywords = ['NaN', 'Inf', '–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏', '–º–µ—Ä—Ç–≤—ã–µ']
        serious_keywords = ['–í—ã—Å–æ–∫–∏–π', '–ù–∏–∑–∫–æ–µ', '–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ']
        
        critical_count = sum(1 for issue in issues if any(kw in issue for kw in critical_keywords))
        serious_count = sum(1 for issue in issues if any(kw in issue for kw in serious_keywords))
        
        if critical_count > 0:
            return CheckpointQuality.CRITICAL
        elif serious_count >= 3:
            return CheckpointQuality.POOR
        elif serious_count >= 1:
            return CheckpointQuality.ACCEPTABLE
        else:
            return CheckpointQuality.GOOD

class AutoRecoverySystem:
    """–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–±–æ—è—Ö"""
    
    def __init__(self, checkpoint_manager):
        self.checkpoint_manager = checkpoint_manager
        self.logger = self._setup_logger()
        self.recovery_attempts = 0
        self.max_recovery_attempts = 3
        
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("AutoRecoverySystem")
        else:
            logger = logging.getLogger("AutoRecoverySystem")
            logger.setLevel(logging.INFO)
            return logger
    
    def detect_critical_failure(self, metrics: CheckpointMetrics) -> bool:
        """–î–µ—Ç–µ–∫—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–±–æ–µ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏"""
        critical_conditions = []
        
        # 1. NaN/Inf –≤ –º–µ—Ç—Ä–∏–∫–∞—Ö
        if (not np.isfinite(metrics.validation_loss) or 
            not np.isfinite(metrics.training_loss)):
            critical_conditions.append("NaN/Inf –≤ loss —Ñ—É–Ω–∫—Ü–∏—è—Ö")
        
        # 2. –í–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if metrics.gradient_norm > 1000.0:
            critical_conditions.append(f"–í–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {metrics.gradient_norm:.2f}")
        
        # 3. –ü–æ–ª–Ω—ã–π –∫–æ–ª–ª–∞–ø—Å attention
        if metrics.attention_diagonality < 0.01:
            critical_conditions.append(f"–ö–æ–ª–ª–∞–ø—Å attention: {metrics.attention_diagonality:.4f}")
        
        # 4. –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–π loss
        if metrics.validation_loss > 1000.0:
            critical_conditions.append(f"–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–π loss: {metrics.validation_loss:.2f}")
        
        if critical_conditions:
            self.logger.error(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –°–ë–û–ô –û–ë–ù–ê–†–£–ñ–ï–ù: {critical_conditions}")
            return True
            
        return False
    
    def attempt_recovery(self, model: nn.Module, optimizer: torch.optim.Optimizer) -> bool:
        """–ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
        if self.recovery_attempts >= self.max_recovery_attempts:
            self.logger.error(f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
            return False
        
        self.recovery_attempts += 1
        self.logger.warning(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è #{self.recovery_attempts}")
        
        # –ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ö–æ—Ä–æ—à–µ–≥–æ checkpoint'–∞
        good_checkpoint = self.checkpoint_manager.get_best_checkpoint(
            min_quality=CheckpointQuality.ACCEPTABLE
        )
        
        if not good_checkpoint:
            self.logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π checkpoint –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
            return False
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ö–æ—Ä–æ—à–µ–≥–æ checkpoint'–∞
            self.logger.info(f"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ checkpoint –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {good_checkpoint.path}")
            checkpoint = torch.load(good_checkpoint.path, map_location='cpu', weights_only=False)
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º learning rate
            if 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                
                # –£–º–µ–Ω—å—à–µ–Ω–∏–µ learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
                for param_group in optimizer.param_groups:
                    original_lr = param_group['lr']
                    param_group['lr'] = original_lr * 0.1  # –£–º–µ–Ω—å—à–∞–µ–º –≤ 10 —Ä–∞–∑
                    
            self.logger.info("‚úÖ –ê–≤—Ç–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
            return False

class IntelligentCheckpointManager:
    """–ì–ª–∞–≤–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è checkpoint'–∞–º–∏"""
    
    def __init__(self, 
                 checkpoint_dir: str = "intelligent_checkpoints",
                 max_checkpoints: int = 10,
                 min_save_interval: int = 500):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.max_checkpoints = max_checkpoints
        self.min_save_interval = min_save_interval
        self.last_save_step = 0
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.selector = MultiCriteriaModelSelector()
        self.health_analyzer = CheckpointHealthAnalyzer()
        self.auto_recovery = AutoRecoverySystem(self)
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ checkpoint'–æ–≤
        self.checkpoints: List[CheckpointInfo] = []
        self.metadata_file = self.checkpoint_dir / "checkpoints_metadata.json"
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        self._load_metadata()
        
        self.logger = self._setup_logger()
        self.logger.info(f"üéØ Intelligent Checkpoint Manager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.checkpoint_dir}")
    
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("IntelligentCheckpointManager")
        else:
            logger = logging.getLogger("IntelligentCheckpointManager")
            logger.setLevel(logging.INFO)
            return logger
    
    def save_checkpoint(self, 
                       model: nn.Module,
                       optimizer: torch.optim.Optimizer,
                       metrics: CheckpointMetrics,
                       force_save: bool = False,
                       is_emergency: bool = False) -> Optional[str]:
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint'–∞"""
        try:
            current_step = metrics.global_step
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            if not force_save and not is_emergency:
                if current_step - self.last_save_step < self.min_save_interval:
                    return None
            
            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ checkpoint'–∞ –ü–ï–†–ï–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            quality = self._predict_checkpoint_quality(metrics)
            
            # –ü—Ä–æ–ø—É—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–ª–æ—Ö–∏—Ö checkpoint'–æ–≤ (–∫—Ä–æ–º–µ emergency)
            if not is_emergency and quality == CheckpointQuality.CRITICAL:
                self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –ø–ª–æ—Ö–æ–≥–æ checkpoint")
                return None
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if is_emergency:
                filename = f"emergency_checkpoint_step_{current_step}_{timestamp}.pt"
            else:
                filename = f"checkpoint_step_{current_step}_{timestamp}.pt"
                
            checkpoint_path = self.checkpoint_dir / filename
            
            # –°–æ–∑–¥–∞–Ω–∏–µ checkpoint –¥–∞–Ω–Ω—ã—Ö
            checkpoint_data = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': metrics.epoch,
                'global_step': metrics.global_step,
                'metrics': asdict(metrics),
                'timestamp': timestamp,
                'torch_version': torch.__version__
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint'–∞
            torch.save(checkpoint_data, checkpoint_path)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ checkpoint'–µ
            file_hash = self._calculate_file_hash(checkpoint_path)
            checkpoint_info = CheckpointInfo(
                path=str(checkpoint_path),
                metrics=metrics,
                quality=quality,
                health_score=0.0,  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω –ø–æ–∑–∂–µ
                is_emergency_backup=is_emergency,
                file_hash=file_hash,
                creation_time=time.time()
            )
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ –∏ –ø–µ—Ä–µ—Å—á–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
            self.checkpoints.append(checkpoint_info)
            self._update_checkpoint_rankings()
            
            # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∏—Å–∫–æ–≤—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º
            self._manage_storage()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            self._save_metadata()
            
            self.last_save_step = current_step
            
            status_icon = "üö®" if is_emergency else "üíæ"
            self.logger.info(f"{status_icon} Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename} (–∫–∞—á–µ—Å—Ç–≤–æ: {quality.value})")
            
            return str(checkpoint_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è checkpoint: {e}")
            return None
    
    def get_best_checkpoint(self, 
                           min_quality: CheckpointQuality = CheckpointQuality.ACCEPTABLE) -> Optional[CheckpointInfo]:
            """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ checkpoint'–∞ –ø–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            quality_order = [
                CheckpointQuality.EXCELLENT,
                CheckpointQuality.GOOD, 
                CheckpointQuality.ACCEPTABLE,
                CheckpointQuality.POOR,
                CheckpointQuality.CRITICAL
            ]
            min_quality_idx = quality_order.index(min_quality)
            
            suitable_checkpoints = [
                cp for cp in self.checkpoints
                if quality_order.index(cp.quality) <= min_quality_idx
            ]
            
            if not suitable_checkpoints:
                return None
            
            # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ
            ranked = self.selector.rank_checkpoints(suitable_checkpoints)
            return ranked[0] if ranked else None
    
    def check_and_recover(self, 
                         model: nn.Module,
                         optimizer: torch.optim.Optimizer,
                         current_metrics: CheckpointMetrics) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–±–æ–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ"""
        if self.auto_recovery.detect_critical_failure(current_metrics):
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ checkpoint'–∞
            emergency_path = self.save_checkpoint(
                model, optimizer, current_metrics, 
                force_save=True, is_emergency=True
            )
            
            if emergency_path:
                self.logger.info(f"üö® –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {emergency_path}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            return self.auto_recovery.attempt_recovery(model, optimizer)
        
        return False
    
    def _predict_checkpoint_quality(self, metrics: CheckpointMetrics) -> CheckpointQuality:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ checkpoint'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫"""
        score = self.selector.calculate_model_score(metrics)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º
        if (metrics.has_nan_weights or 
            metrics.has_gradient_explosion or 
            not np.isfinite(metrics.validation_loss)):
            return CheckpointQuality.CRITICAL
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ score
        if score >= 0.8:
            return CheckpointQuality.EXCELLENT
        elif score >= 0.6:
            return CheckpointQuality.GOOD
        elif score >= 0.4:
            return CheckpointQuality.ACCEPTABLE
        elif score >= 0.2:
            return CheckpointQuality.POOR
        else:
            return CheckpointQuality.CRITICAL
    
    def _update_checkpoint_rankings(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –≤—Å–µ—Ö checkpoint'–æ–≤"""
        self.checkpoints = self.selector.rank_checkpoints(self.checkpoints)
    
    def _manage_storage(self):
        """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∏—Å–∫–æ–≤—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º"""
        if len(self.checkpoints) <= self.max_checkpoints:
            return
        
        # –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        # 1. –õ—É—á—à–∏–π checkpoint (–ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É)
        # 2. –ù–µ–¥–∞–≤–Ω–∏–µ —Ö–æ—Ä–æ—à–∏–µ checkpoint'—ã  
        # 3. Emergency backup'—ã (—Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ª—É—á—à–∏–µ, –ø–æ—Ç–æ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        sorted_checkpoints = sorted(
            self.checkpoints,
            key=lambda x: (
                x.quality == CheckpointQuality.CRITICAL,  # Critical - –≤ –∫–æ–Ω–µ—Ü
                -x.health_score,  # –õ—É—á—à–∏–π score - –≤ –Ω–∞—á–∞–ª–æ
                -x.creation_time  # –ù–æ–≤—ã–µ - –≤ –Ω–∞—á–∞–ª–æ
            )
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint'—ã —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –∏ —Å—Ç—Ä–æ–≥–æ–≥–æ –ª–∏–º–∏—Ç–∞
        to_keep = []
        emergency_count = 0
        max_emergency = min(2, self.max_checkpoints - 1)  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö checkpoint'–æ–≤
        
        for checkpoint in sorted_checkpoints:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π –ª–∏–º–∏—Ç
            if len(to_keep) >= self.max_checkpoints:
                break
                
            # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π checkpoint (–µ—Å–ª–∏ –æ–Ω –Ω–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π)
            if len(to_keep) == 0 and checkpoint.quality != CheckpointQuality.CRITICAL:
                checkpoint.is_best = True
                to_keep.append(checkpoint)
                continue
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ emergency backup'–æ–≤
            if checkpoint.is_emergency_backup:
                if emergency_count < max_emergency and len(to_keep) < self.max_checkpoints:
                    to_keep.append(checkpoint)
                    emergency_count += 1
                continue
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ã—á–Ω—ã–µ checkpoint'—ã –¥–æ –ª–∏–º–∏—Ç–∞
            if len(to_keep) < self.max_checkpoints:
                to_keep.append(checkpoint)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ checkpoint'—ã
        to_remove = [cp for cp in self.checkpoints if cp not in to_keep]
        
        for checkpoint in to_remove:
            try:
                # –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
                if os.path.exists(checkpoint.path):
                    os.remove(checkpoint.path)
                    self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω checkpoint: {os.path.basename(checkpoint.path)} (–∫–∞—á–µ—Å—Ç–≤–æ: {checkpoint.quality.value})")
                
                # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–∞
                self.checkpoints.remove(checkpoint)
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è checkpoint: {e}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è checkpoint'–æ–≤
        if self.checkpoints:
            self.checkpoints = self.selector.rank_checkpoints(self.checkpoints)
            self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.checkpoints)}/{self.max_checkpoints} checkpoint'–æ–≤")
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ MD5 —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return ""
    
    def _load_metadata(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö checkpoint'–æ–≤"""
        if not self.metadata_file.exists():
            return
        
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            self.checkpoints = []
            for item in data.get('checkpoints', []):
                # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ CheckpointMetrics
                metrics_data = item['metrics']
                metrics = CheckpointMetrics(**metrics_data)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ CheckpointInfo
                checkpoint_info = CheckpointInfo(
                    path=item['path'],
                    metrics=metrics,
                    quality=CheckpointQuality(item['quality']),
                    health_score=item['health_score'],
                    is_best=item.get('is_best', False),
                    is_emergency_backup=item.get('is_emergency_backup', False),
                    file_hash=item.get('file_hash', ''),
                    creation_time=item.get('creation_time', 0.0)
                )
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
                if os.path.exists(checkpoint_info.path):
                    self.checkpoints.append(checkpoint_info)
            
            self.logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.checkpoints)} checkpoint'–æ–≤")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            self.checkpoints = []
    
    def _save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö checkpoint'–æ–≤"""
        try:
            data = {
                'checkpoints': [],
                'last_updated': datetime.now().isoformat(),
                'manager_version': '1.0'
            }
            
            for checkpoint in self.checkpoints:
                checkpoint_data = {
                    'path': checkpoint.path,
                    'metrics': asdict(checkpoint.metrics),
                    'quality': checkpoint.quality.value,
                    'health_score': checkpoint.health_score,
                    'is_best': checkpoint.is_best,
                    'is_emergency_backup': checkpoint.is_emergency_backup,
                    'file_hash': checkpoint.file_hash,
                    'creation_time': checkpoint.creation_time
                }
                data['checkpoints'].append(checkpoint_data)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def get_status_report(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ checkpoint'–æ–≤"""
        if not self.checkpoints:
            return {
                'total_checkpoints': 0,
                'status': 'empty',
                'message': '–ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö checkpoint\'–æ–≤'
            }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        quality_stats = {}
        for quality in CheckpointQuality:
            count = sum(1 for cp in self.checkpoints if cp.quality == quality)
            quality_stats[quality.value] = count
        
        # –õ—É—á—à–∏–π checkpoint
        best = self.get_best_checkpoint()
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_size_mb = sum(
            os.path.getsize(cp.path) / (1024 * 1024) 
            for cp in self.checkpoints 
            if os.path.exists(cp.path)
        )
        
        return {
            'total_checkpoints': len(self.checkpoints),
            'quality_distribution': quality_stats,
            'best_checkpoint': {
                'path': best.path if best else None,
                'score': best.health_score if best else None,
                'quality': best.quality.value if best else None
            },
            'total_size_mb': round(total_size_mb, 2),
            'storage_usage': f"{len(self.checkpoints)}/{self.max_checkpoints}",
            'emergency_backups': sum(1 for cp in self.checkpoints if cp.is_emergency_backup),
            'status': 'healthy' if best and best.quality != CheckpointQuality.CRITICAL else 'warning'
        }

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def create_checkpoint_manager(checkpoint_dir: str = "intelligent_checkpoints",
                            max_checkpoints: int = 10) -> IntelligentCheckpointManager:
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ checkpoint manager'–∞"""
    return IntelligentCheckpointManager(
        checkpoint_dir=checkpoint_dir,
        max_checkpoints=max_checkpoints
    )

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Advanced Model Checkpointing System
    
    # –°–æ–∑–¥–∞–Ω–∏–µ manager'–∞
    checkpoint_manager = create_checkpoint_manager(
        checkpoint_dir="demo_checkpoints",
        max_checkpoints=5
    )
    
    print("üéØ Advanced Model Checkpointing System –¥–µ–º–æ")
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è checkpoint'–æ–≤: {checkpoint_manager.checkpoint_dir}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
    status = checkpoint_manager.get_status_report()
    print(f"üìä –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å: {status}")
    
    print("\n‚úÖ Advanced Model Checkpointing System –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    print("\nüîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –æ–±—É—á–µ–Ω–∏–µ–º:")
    print("1. –°–æ–∑–¥–∞–π—Ç–µ IntelligentCheckpointManager")
    print("2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ save_checkpoint() –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è")  
    print("3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ check_and_recover() –¥–ª—è –∞–≤—Ç–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
    print("4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ get_best_checkpoint() –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏") 