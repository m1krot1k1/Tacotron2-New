#!/usr/bin/env python3
"""
üî• –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Guided Attention –¥–ª—è Tacotron2

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª—É—á—à–µ–µ –∏–∑ –≤—Å–µ—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π:
‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (–±—ã—Å—Ç—Ä–æ—Ç–∞)
‚úÖ Location-Relative —Ñ–æ—Ä–º—É–ª–∞ (—Ç–æ—á–Ω–æ—Å—Ç—å)  
‚úÖ Context-Aware –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (—É–º–Ω–æ—Å—Ç—å)
‚úÖ Emergency recovery (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)

–ó–∞–º–µ–Ω—è–µ—Ç:
‚ùå Tacotron2Loss.guided_attention_loss() - –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
‚ùå GuidedAttentionLoss –∫–ª–∞—Å—Å - –º–µ–¥–ª–µ–Ω–Ω—ã–µ —Ü–∏–∫–ª—ã
‚ùå –ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã —Å Context-Aware Manager
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional, Tuple, Dict, Any


class UnifiedGuidedAttentionLoss(nn.Module):
    """
    üî• –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ Guided Attention Loss
    
    –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    1. üöÄ –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–ï –æ–ø–µ—Ä–∞—Ü–∏–∏ (–≤ 10x –±—ã—Å—Ç—Ä–µ–µ —Ü–∏–∫–ª–æ–≤)
    2. üéØ LOCATION-RELATIVE —Ñ–æ—Ä–º—É–ª–∞ –∏–∑ Very Attentive Tacotron 2025
    3. üß† –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø —Å Context-Aware Training Manager
    4. üõ°Ô∏è EMERGENCY recovery –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π
    5. üìä –ê–î–ê–ü–¢–ò–í–ù–´–ï –≤–µ—Å–∞ –∏ sigma –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
    6. üîÑ –ï–î–ò–ù–ê–Ø —Ç–æ—á–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    """
    
    def __init__(self, hparams):
        super(UnifiedGuidedAttentionLoss, self).__init__()
        self.hparams = hparams
        
        # üéØ –ë–ê–ó–û–í–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ hparams
        self.initial_weight = getattr(hparams, 'guide_loss_initial_weight', 5.0)
        self.min_weight = getattr(hparams, 'guide_loss_min_weight', 0.1)
        self.max_weight = getattr(hparams, 'guide_loss_max_weight', 15.0)
        
        # üìà –†–ê–°–ü–ò–°–ê–ù–ò–ï —Å–Ω–∏–∂–µ–Ω–∏—è –≤–µ—Å–∞
        self.decay_start = getattr(hparams, 'guide_loss_decay_start', 2000)
        self.decay_steps = getattr(hparams, 'guide_loss_decay_steps', 25000)
        self.decay_factor = getattr(hparams, 'guide_loss_decay_factor', 3.0)
        
        # üîß SIGMA –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è gaussian attention
        self.initial_sigma = getattr(hparams, 'guide_sigma_initial', 0.1)
        self.peak_sigma = getattr(hparams, 'guide_sigma_peak', 0.4)
        self.final_sigma = getattr(hparams, 'guide_sigma_final', 0.15)
        
        # üö® EMERGENCY recovery –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.emergency_weight = getattr(hparams, 'guide_emergency_weight', 25.0)
        self.emergency_threshold = getattr(hparams, 'attention_emergency_threshold', 0.02)
        self.recovery_threshold = getattr(hparams, 'attention_recovery_threshold', 0.5)
        
        # üéõÔ∏è –°–û–°–¢–û–Ø–ù–ò–ï —Å–∏—Å—Ç–µ–º—ã
        self.global_step = 0
        self.current_weight = self.initial_weight
        self.current_sigma = self.initial_sigma
        self.emergency_mode = False
        
        # üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.recent_diagonality = []
        self.recent_losses = []
        self.adaptation_history = []
        
        # üß† Context-Aware –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
        self.context_aware_manager = None
        self.use_context_aware = getattr(hparams, 'use_context_aware_attention', True)
        
    def set_context_aware_manager(self, manager):
        """–°–≤—è–∑—ã–≤–∞–µ—Ç —Å Context-Aware Training Manager –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        self.context_aware_manager = manager
        
    def forward(self, model_output: tuple, mel_lengths: Optional[torch.Tensor] = None, 
                text_lengths: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        üî• –ì–õ–ê–í–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è guided attention loss
        
        Args:
            model_output: –í—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏ (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª—é–±–æ–π —Ñ–æ—Ä–º–∞—Ç)
            mel_lengths: –î–ª–∏–Ω—ã mel –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π [optional]
            text_lengths: –î–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π [optional]
            
        Returns:
            torch.Tensor: Guided attention loss
        """
        # üîß –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ model_output
        alignments = self._extract_alignments(model_output)
        if alignments is None:
            return torch.tensor(0.0, requires_grad=True, device=self._get_device(model_output))
        
        # üìä –ê–ù–ê–õ–ò–ó attention –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        diagonality = self._calculate_batch_diagonality(alignments)
        self._update_statistics(diagonality)
        
        # üö® EMERGENCY mode –ø—Ä–æ–≤–µ—Ä–∫–∞
        self._check_and_update_emergency_mode(diagonality)
        
        # üß† CONTEXT-AWARE –∞–¥–∞–ø—Ç–∞—Ü–∏—è
        if self.use_context_aware and self.context_aware_manager:
            self._apply_context_aware_adaptation(diagonality, alignments)
        
        # üéØ –í–´–ß–ò–°–õ–ï–ù–ò–ï guided attention loss
        loss = self._compute_guided_loss(alignments, mel_lengths, text_lengths)
        
        # ‚öñÔ∏è –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –≤–µ—Å–∞
        weighted_loss = self._get_current_weight() * loss
        
        # üìà –û–ë–ù–û–í–õ–ï–ù–ò–ï —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
        self._update_system_state(loss.item(), diagonality)
        
        return weighted_loss
    
    def _extract_alignments(self, model_output: tuple) -> Optional[torch.Tensor]:
        """üîß –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ alignments –∏–∑ –ª—é–±–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ model_output"""
        if not isinstance(model_output, (tuple, list)) or len(model_output) < 4:
            return None
            
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        if len(model_output) == 7:
            # [decoder_outputs, mel_out, mel_out_postnet, gate_out, alignments, tpse_gst_outputs, gst_outputs]
            return model_output[4]
        elif len(model_output) == 6:
            # [mel_out, mel_out_postnet, gate_out, alignments, tpse_gst_outputs, gst_outputs]
            return model_output[3]
        elif len(model_output) == 5:
            # [mel_out, mel_out_postnet, gate_out, alignments, extra]
            return model_output[3]
        elif len(model_output) == 4:
            # [mel_out, mel_out_postnet, gate_out, alignments]
            return model_output[3]
        else:
            # Fallback: –∏—â–µ–º —Ç–µ–Ω–∑–æ—Ä —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º–∏
            for output in model_output:
                if isinstance(output, torch.Tensor) and len(output.shape) == 3:
                    batch_size, dim1, dim2 = output.shape
                    # Attention –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ [batch, mel_len, text_len]
                    if dim1 > 10 and dim2 > 5:  # –†–∞–∑—É–º–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è attention
                        return output
            return None
    
    def _get_device(self, model_output: tuple) -> torch.device:
        """–ü–æ–ª—É—á–∞–µ—Ç device –∏–∑ model_output"""
        for output in model_output:
            if isinstance(output, torch.Tensor):
                return output.device
        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def _compute_guided_loss(self, alignments: torch.Tensor, 
                           mel_lengths: Optional[torch.Tensor] = None,
                           text_lengths: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        üî• –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–û–ï –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ guided attention loss
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Location-Relative —Ñ–æ—Ä–º—É–ª—É –∏–∑ Very Attentive Tacotron 2025
        """
        batch_size, mel_len, text_len = alignments.shape
        device = alignments.device
        
        # üéØ –°–û–ó–î–ê–ù–ò–ï –æ–∂–∏–¥–∞–µ–º–æ–≥–æ alignment (–≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ)
        expected_alignment = self._create_expected_alignment(mel_len, text_len, device)
        
        # üé≠ –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–ï –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–ª–∏–Ω
        mask = self._create_sequence_mask(
            batch_size, mel_len, text_len, mel_lengths, text_lengths, device
        )
        
        # üî• –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–ô KL divergence loss
        loss = self._compute_kl_divergence_loss(alignments, expected_alignment, mask)
        
        return loss
    
    def _create_expected_alignment(self, mel_len: int, text_len: int, 
                                 device: torch.device) -> torch.Tensor:
        """üéØ –°–æ–∑–¥–∞–Ω–∏–µ –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–≥–æ alignment (–≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ)"""
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω—ã–µ —Å–µ—Ç–∫–∏
        mel_indices = torch.arange(mel_len, device=device, dtype=torch.float32).unsqueeze(1)
        text_indices = torch.arange(text_len, device=device, dtype=torch.float32).unsqueeze(0)
        
        # üî• LOCATION-RELATIVE –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        mel_normalized = mel_indices / max(mel_len - 1, 1)  # [mel_len, 1]
        text_normalized = text_indices / max(text_len - 1, 1)  # [1, text_len]
        
        # üìä –ê–î–ê–ü–¢–ò–í–ù–ê–Ø sigma
        current_sigma = self._get_current_sigma()
        
        # üéØ GAUSSIAN attention —Å location-relative –ø–æ–∑–∏—Ü–∏—è–º–∏
        distances = (mel_normalized - text_normalized) ** 2
        expected_alignment = torch.exp(-distances / (2 * current_sigma ** 2))
        
        # üîß –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¥–ª—è –∫–∞–∂–¥–æ–≥–æ mel —à–∞–≥–∞
        expected_alignment = expected_alignment / (expected_alignment.sum(dim=1, keepdim=True) + 1e-8)
        
        return expected_alignment
    
    def _create_sequence_mask(self, batch_size: int, mel_len: int, text_len: int,
                            mel_lengths: Optional[torch.Tensor],
                            text_lengths: Optional[torch.Tensor],
                            device: torch.device) -> torch.Tensor:
        """üé≠ –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        mask = torch.ones(batch_size, mel_len, text_len, device=device, dtype=torch.bool)
        
        if mel_lengths is not None and text_lengths is not None:
            for b in range(batch_size):
                actual_mel_len = min(int(mel_lengths[b].item()), mel_len)
                actual_text_len = min(int(text_lengths[b].item()), text_len)
                
                # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                mask[b, actual_mel_len:, :] = False
                mask[b, :, actual_text_len:] = False
        
        return mask
    
    def _compute_kl_divergence_loss(self, alignments: torch.Tensor, 
                                  expected_alignment: torch.Tensor,
                                  mask: torch.Tensor) -> torch.Tensor:
        """üî• –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π KL divergence loss"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º batch —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∫ expected_alignment
        batch_size = alignments.size(0)
        expected_alignment = expected_alignment.unsqueeze(0).expand(batch_size, -1, -1)
        
        # üõ°Ô∏è –ß–ò–°–õ–ï–ù–ù–ê–Ø —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        epsilon = 1e-8
        alignments_stable = alignments * mask.float() + epsilon
        expected_stable = expected_alignment * mask.float() + epsilon
        
        # üîß –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
        alignments_normalized = alignments_stable / alignments_stable.sum(dim=2, keepdim=True)
        expected_normalized = expected_stable / expected_stable.sum(dim=2, keepdim=True)
        
        # üìä KL divergence –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ
        kl_div = F.kl_div(
            torch.log(alignments_normalized + epsilon),
            expected_normalized,
            reduction='none'
        )
        
        # üé≠ –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–ï –∏ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ
        kl_div_masked = kl_div * mask.float()
        valid_elements = mask.float().sum()
        
        if valid_elements > 0:
            loss = kl_div_masked.sum() / valid_elements
        else:
            loss = torch.tensor(0.0, device=alignments.device, requires_grad=True)
        
        return loss
    
    def _calculate_batch_diagonality(self, alignments: torch.Tensor) -> float:
        """üìä –ë—ã—Å—Ç—Ä–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ–≥–æ batch"""
        
        try:
            batch_size, mel_len, text_len = alignments.shape
            
            # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª—è –≤—Å–µ–≥–æ batch –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ
            total_diagonality = 0.0
            valid_samples = 0
            
            for b in range(batch_size):
                attention = alignments[b].detach()
                total_sum = attention.sum().item()
                
                if total_sum > 1e-6:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–æ–ª—å
                    # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã—Ö –≤–µ—Å–æ–≤
                    mel_indices = torch.arange(mel_len, device=attention.device)
                    diagonal_indices = (mel_indices * text_len / mel_len).long()
                    diagonal_indices = torch.clamp(diagonal_indices, 0, text_len - 1)
                    
                    # –°—É–º–º–∏—Ä—É–µ–º –≤–µ—Å–∞ –ø–æ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
                    diagonal_weights = attention[mel_indices, diagonal_indices].sum().item()
                    diagonality = diagonal_weights / total_sum
                    
                    total_diagonality += diagonality
                    valid_samples += 1
            
            return total_diagonality / max(valid_samples, 1)
            
        except Exception:
            return 0.0
    
    def _update_statistics(self, diagonality: float):
        """üìà –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏"""
        self.recent_diagonality.append(diagonality)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 100 –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        if len(self.recent_diagonality) > 100:
            self.recent_diagonality = self.recent_diagonality[-100:]
    
    def _check_and_update_emergency_mode(self, diagonality: float):
        """üö® –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ emergency mode"""
        
        if diagonality < self.emergency_threshold and not self.emergency_mode:
            self.emergency_mode = True
            print(f"üö® UnifiedGuidedAttention: EMERGENCY MODE –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω! "
                  f"Diagonality: {diagonality:.4f} < {self.emergency_threshold}")
            
        elif diagonality > self.recovery_threshold and self.emergency_mode:
            self.emergency_mode = False
            print(f"‚úÖ UnifiedGuidedAttention: Emergency mode –¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω. "
                  f"Diagonality: {diagonality:.4f} > {self.recovery_threshold}")
    
    def _apply_context_aware_adaptation(self, diagonality: float, alignments: torch.Tensor):
        """üß† –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Context-Aware –∞–¥–∞–ø—Ç–∞—Ü–∏–∏"""
        
        if not self.context_aware_manager:
            return
            
        try:
            # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            from context_aware_training_manager import TrainingContext, TrainingPhase
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–∑—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            if diagonality < 0.1:
                phase = TrainingPhase.PRE_ALIGNMENT
            elif diagonality < 0.5:
                phase = TrainingPhase.ALIGNMENT_LEARNING
            elif diagonality < 0.7:
                phase = TrainingPhase.REFINEMENT
            else:
                phase = TrainingPhase.CONVERGENCE
            
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context = TrainingContext(
                phase=phase,
                step=self.global_step,
                epoch=0,  # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –≤ –¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                loss_trend=0.0,  # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ –≤ –¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
                attention_quality=diagonality,
                gradient_health=0.5,  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                learning_rate=1e-4,  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                convergence_score=diagonality,
                stability_index=diagonality,
                time_since_improvement=0,
                attention_diagonality=diagonality,
                gate_accuracy=0.5,  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                mel_loss=0.0,
                gate_loss=0.0,
                guided_attention_loss=np.mean(self.recent_losses[-10:]) if self.recent_losses else 0.0
            )
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç Context-Aware Manager
            recommendations = self.context_aware_manager.get_guided_attention_recommendations(context)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if 'emergency_mode' in recommendations and recommendations['emergency_mode']:
                if not self.emergency_mode:
                    self.emergency_mode = True
                    print(f"üö® Context-Aware: Emergency mode –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω! Diagonality: {diagonality:.4f}")
                    
            elif 'emergency_mode' in recommendations and not recommendations['emergency_mode']:
                if self.emergency_mode and diagonality > self.recovery_threshold:
                    self.emergency_mode = False
                    print(f"‚úÖ Context-Aware: Emergency mode –¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω. Diagonality: {diagonality:.4f}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π Context-Aware Manager
            if 'suggested_weight' in recommendations:
                suggested_weight = recommendations['suggested_weight']
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å –≤ Context-Aware Manager –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
                if hasattr(self.context_aware_manager, 'loss_controller'):
                    self.context_aware_manager.loss_controller.guided_attention_weight = suggested_weight
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Context-Aware –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: {e}")
    
    def _get_current_weight(self) -> float:
        """‚öñÔ∏è –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –≤–µ—Å–∞ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤"""
        
        # üö® Emergency mode –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        if self.emergency_mode:
            return self.emergency_weight
            
        # üß† Context-Aware –∞–¥–∞–ø—Ç–∞—Ü–∏—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
        if (self.use_context_aware and self.context_aware_manager and 
            hasattr(self.context_aware_manager, 'loss_controller')):
            
            try:
                context_weight = self.context_aware_manager.loss_controller.guided_attention_weight
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
                return np.clip(context_weight, self.min_weight, self.max_weight)
            except:
                pass
        
        # üìà –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ —Å–Ω–∏–∂–µ–Ω–∏—è
        return self._calculate_scheduled_weight()
    
    def _calculate_scheduled_weight(self) -> float:
        """üìà –†–∞—Å—á–µ—Ç –≤–µ—Å–∞ –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é"""
        
        if self.global_step < self.decay_start:
            # –§–∞–∑–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ guided attention
            return self.initial_weight
            
        elif self.global_step < self.decay_start + self.decay_steps:
            # –§–∞–∑–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è
            progress = (self.global_step - self.decay_start) / self.decay_steps
            decay_factor = math.exp(-progress * self.decay_factor)
            current_weight = self.min_weight + (self.initial_weight - self.min_weight) * decay_factor
            return max(self.min_weight, current_weight)
            
        else:
            # –§–∞–∑–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ guided attention
            return self.min_weight
    
    def _get_current_sigma(self) -> float:
        """üìä –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π sigma –¥–ª—è gaussian attention"""
        
        if self.global_step < 1000:
            # –ù–∞—á–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞: —É–∑–∫–∞—è sigma –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ alignment
            return self.initial_sigma
            
        elif self.global_step < 5000:
            # –†–∞—Å—à–∏—Ä—è—é—â–∞—è —Ñ–∞–∑–∞: —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º sigma –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
            progress = (self.global_step - 1000) / 4000
            return self.initial_sigma + (self.peak_sigma - self.initial_sigma) * progress
            
        else:
            # –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—â–∞—è —Ñ–∞–∑–∞: –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ–º –∫ final_sigma
            progress = min(1.0, (self.global_step - 5000) / 15000)
            return self.peak_sigma - (self.peak_sigma - self.final_sigma) * progress
    
    def _update_system_state(self, loss_value: float, diagonality: float):
        """üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        
        self.global_step += 1
        self.recent_losses.append(loss_value)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self.recent_losses) > 100:
            self.recent_losses = self.recent_losses[-100:]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–∞—Ü–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        adaptation_record = {
            'step': self.global_step,
            'weight': self._get_current_weight(),
            'sigma': self._get_current_sigma(),
            'diagonality': diagonality,
            'loss': loss_value,
            'emergency_mode': self.emergency_mode
        }
        self.adaptation_history.append(adaptation_record)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        if len(self.adaptation_history) > 1000:
            self.adaptation_history = self.adaptation_history[-1000:]
    
    def get_diagnostics(self) -> Dict[str, Any]:
        """üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        
        recent_diag = self.recent_diagonality[-10:] if self.recent_diagonality else [0.0]
        recent_loss = self.recent_losses[-10:] if self.recent_losses else [0.0]
        
        return {
            'current_weight': self._get_current_weight(),
            'current_sigma': self._get_current_sigma(),
            'emergency_mode': self.emergency_mode,
            'global_step': self.global_step,
            'avg_diagonality_10': np.mean(recent_diag),
            'avg_loss_10': np.mean(recent_loss),
            'min_diagonality': min(self.recent_diagonality) if self.recent_diagonality else 0.0,
            'max_diagonality': max(self.recent_diagonality) if self.recent_diagonality else 0.0,
            'adaptation_count': len(self.adaptation_history)
        }
    
    def force_emergency_mode(self, activate: bool = True):
        """üö® –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è/–¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è emergency mode"""
        self.emergency_mode = activate
        mode_str = "–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω" if activate else "–¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω"
        print(f"üõ°Ô∏è UnifiedGuidedAttention: Emergency mode –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ {mode_str}")
    
    def reset_statistics(self):
        """üîÑ –°–±—Ä–æ—Å –≤—Å–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        self.recent_diagonality = []
        self.recent_losses = []
        self.adaptation_history = []
        self.emergency_mode = False
        print("üîÑ UnifiedGuidedAttention: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–±—Ä–æ—à–µ–Ω–∞")


def create_unified_guided_attention(hparams) -> UnifiedGuidedAttentionLoss:
    """
    üè≠ –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã guided attention
    
    Args:
        hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        
    Returns:
        UnifiedGuidedAttentionLoss: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ guided attention
    """
    return UnifiedGuidedAttentionLoss(hparams)


# üß™ –§–£–ù–ö–¶–ò–ò –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ò –û–¢–õ–ê–î–ö–ò

def test_unified_guided_attention():
    """üß™ –¢–µ—Å—Ç —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã guided attention"""
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ UnifiedGuidedAttentionLoss...")
    
    # –°–æ–∑–¥–∞–µ–º mock hparams
    class MockHParams:
        guide_loss_initial_weight = 5.0
        guide_loss_min_weight = 0.1
        guide_loss_max_weight = 15.0
        use_context_aware_attention = True
    
    hparams = MockHParams()
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    guided_attention = create_unified_guided_attention(hparams)
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size, mel_len, text_len = 2, 100, 50
    alignments = torch.rand(batch_size, mel_len, text_len)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π model_output
    model_output = (
        torch.rand(batch_size, mel_len, 80),  # mel_out
        torch.rand(batch_size, mel_len, 80),  # mel_out_postnet  
        torch.rand(batch_size, mel_len, 1),   # gate_out
        alignments                            # alignments
    )
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass
    loss = guided_attention(model_output)
    
    print(f"‚úÖ Forward pass —É—Å–ø–µ—à–µ–Ω. Loss: {loss.item():.6f}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
    diagnostics = guided_attention.get_diagnostics()
    print(f"üìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: {diagnostics}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º emergency mode
    guided_attention.force_emergency_mode(True)
    emergency_loss = guided_attention(model_output)
    print(f"üö® Emergency mode loss: {emergency_loss.item():.6f}")
    
    print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
    return True


if __name__ == "__main__":
    test_unified_guided_attention() 