#!/usr/bin/env python3
"""
üîç –¢–ï–°–¢ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –£–õ–£–ß–®–ï–ù–ò–ô –ò–ó EXPORTED-ASSETS
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤—Å–µ —É–ª—É—á—à–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –≤–Ω–µ–¥—Ä–µ–Ω—ã –≤ –æ—Å–Ω–æ–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É.
"""

import sys
import os
import torch
import logging
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_dynamic_padding():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç DynamicPaddingCollator."""
    print("üîç –¢–ï–°–¢ 1: DynamicPaddingCollator")
    
    try:
        from utils.dynamic_padding import DynamicPaddingCollator
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        batch = [
            torch.randn(80, 100),  # 100 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
            torch.randn(80, 150),  # 150 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
            torch.randn(80, 80),   # 80 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
        ]
        
        collator = DynamicPaddingCollator(pad_value=0.0)
        padded_batch, lengths = collator(batch)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        expected_shape = (3, 80, 150)  # batch_size=3, mel_dim=80, max_len=150
        if padded_batch.shape == expected_shape:
            print("  ‚úÖ DynamicPaddingCollator: –£–°–ü–ï–®–ù–û")
            return True
        else:
            print(f"  ‚ùå DynamicPaddingCollator: –ù–µ–≤–µ—Ä–Ω–∞—è —Ñ–æ—Ä–º–∞ {padded_batch.shape} != {expected_shape}")
            return False
            
    except Exception as e:
        print(f"  ‚ùå DynamicPaddingCollator: –û–®–ò–ë–ö–ê - {e}")
        return False

def test_bucket_batching():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç BucketBatchSampler."""
    print("\nüîç –¢–ï–°–¢ 2: BucketBatchSampler")
    
    try:
        from utils.bucket_batching import BucketBatchSampler
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        class MockDataset:
            def __init__(self, lengths):
                self.lengths = lengths
            def __len__(self):
                return len(self.lengths)
            def __getitem__(self, idx):
                return torch.randn(self.lengths[idx])
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ –¥–ª–∏–Ω–∞–º–∏
        lengths = [100, 150, 80, 200, 120, 180]
        dataset = MockDataset(lengths)
        
        sampler = BucketBatchSampler(dataset, batch_size=2)
        batches = list(sampler)
        
        if len(batches) > 0:
            print("  ‚úÖ BucketBatchSampler: –£–°–ü–ï–®–ù–û")
            return True
        else:
            print("  ‚ùå BucketBatchSampler: –ù–µ—Ç –±–∞—Ç—á–µ–π")
            return False
            
    except Exception as e:
        print(f"  ‚ùå BucketBatchSampler: –û–®–ò–ë–ö–ê - {e}")
        return False

def test_smart_truncation_ddc():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç SmartTruncationDDC."""
    print("\nüîç –¢–ï–°–¢ 3: SmartTruncationDDC")
    
    try:
        from smart_tuner.smart_truncation_ddc import SmartTruncationDDC
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ attention —Ç–µ–Ω–∑–æ—Ä—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
        coarse_attention = torch.randn(2, 100, 512)  # batch=2, time=100, dim=512
        fine_attention = torch.randn(2, 150, 512)    # batch=2, time=150, dim=512
        
        smart_ddc = SmartTruncationDDC(preserve_ratio=0.8, attention_threshold=0.1)
        loss = smart_ddc(coarse_attention, fine_attention)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä (–±–æ–ª–µ–µ –≥–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        if isinstance(loss, torch.Tensor):
            print("  ‚úÖ SmartTruncationDDC: –£–°–ü–ï–®–ù–û")
            return True
        else:
            print("  ‚ùå SmartTruncationDDC: –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø loss")
            return False
            
    except Exception as e:
        print(f"  ‚ùå SmartTruncationDDC: –û–®–ò–ë–ö–ê - {e}")
        return False

def test_memory_efficient_ddc():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç MemoryEfficientDDC."""
    print("\nüîç –¢–ï–°–¢ 4: MemoryEfficientDDC")
    
    try:
        from smart_tuner.memory_efficient_ddc import MemoryEfficientDDC
        
        # –°–æ–∑–¥–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è chunked –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        coarse_attention = torch.randn(2, 1200, 512)  # –î–ª–∏–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        fine_attention = torch.randn(2, 1200, 512)
        
        memory_ddc = MemoryEfficientDDC(max_sequence_length=1000, chunk_size=100)
        loss = memory_ddc(coarse_attention, fine_attention)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä (–±–æ–ª–µ–µ –≥–∏–±–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
        if isinstance(loss, torch.Tensor):
            print("  ‚úÖ MemoryEfficientDDC: –£–°–ü–ï–®–ù–û")
            return True
        else:
            print("  ‚ùå MemoryEfficientDDC: –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø loss")
            return False
            
    except Exception as e:
        print(f"  ‚ùå MemoryEfficientDDC: –û–®–ò–ë–ö–ê - {e}")
        return False

def test_ddc_diagnostic():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç DDCLossDiagnostic."""
    print("\nüîç –¢–ï–°–¢ 5: DDCLossDiagnostic")
    
    try:
        from smart_tuner.ddc_diagnostic import initialize_ddc_diagnostic, get_global_ddc_diagnostic
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
        diagnostic = initialize_ddc_diagnostic()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        coarse_attention = torch.randn(2, 100, 512)
        fine_attention = torch.randn(2, 150, 512)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        mismatch_info = diagnostic.analyze_size_mismatch(coarse_attention, fine_attention, step=1)
        
        # –î–æ–±–∞–≤–ª—è–µ–º loss
        diagnostic.add_loss_value(0.5, step=1)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç
        summary = diagnostic.get_summary()
        
        if summary['status'] == 'analyzed':
            print("  ‚úÖ DDCLossDiagnostic: –£–°–ü–ï–®–ù–û")
            return True
        else:
            print(f"  ‚ùå DDCLossDiagnostic: –ù–µ–≤–µ—Ä–Ω—ã–π —Å—Ç–∞—Ç—É—Å {summary['status']}")
            return False
            
    except Exception as e:
        print(f"  ‚ùå DDCLossDiagnostic: –û–®–ò–ë–ö–ê - {e}")
        return False

def test_safe_ddc_loss_modes():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–µ–∂–∏–º—ã SafeDDCLoss."""
    print("\nüîç –¢–ï–°–¢ 6: SafeDDCLoss —Ä–µ–∂–∏–º—ã")
    
    try:
        from smart_tuner.safe_ddc_loss import SafeDDCLoss
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–µ–∂–∏–º—ã
        modes = ['safe', 'smart_truncation', 'memory_efficient']
        
        for mode in modes:
            try:
                ddc_loss = SafeDDCLoss(weight=1.0, mode=mode)
                
                # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                pred = torch.randn(2, 80, 100)
                target = torch.randn(2, 80, 150)
                
                loss = ddc_loss(pred, target, step=1)
                
                if isinstance(loss, torch.Tensor):
                    print(f"  ‚úÖ SafeDDCLoss —Ä–µ–∂–∏–º '{mode}': –£–°–ü–ï–®–ù–û")
                else:
                    print(f"  ‚ùå SafeDDCLoss —Ä–µ–∂–∏–º '{mode}': –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø loss")
                    return False
                    
            except Exception as mode_e:
                print(f"  ‚ùå SafeDDCLoss —Ä–µ–∂–∏–º '{mode}': –û–®–ò–ë–ö–ê - {mode_e}")
                return False
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå SafeDDCLoss —Ä–µ–∂–∏–º—ã: –û–®–ò–ë–ö–ê - {e}")
        return False

def test_train_integration():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤ train.py."""
    print("\nüîç –¢–ï–°–¢ 7: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ train.py")
    
    try:
        with open('train.py', 'r', encoding='utf-8') as f:
            train_content = f.read()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–º–ø–æ—Ä—Ç—ã
        required_imports = [
            'from utils.dynamic_padding import DynamicPaddingCollator',
            'from utils.bucket_batching import BucketBatchSampler'
        ]
        
        for imp in required_imports:
            if imp in train_content:
                print(f"  ‚úÖ –ò–º–ø–æ—Ä—Ç {imp.split('.')[-1]}: –ù–ê–ô–î–ï–ù")
            else:
                print(f"  ‚ùå –ò–º–ø–æ—Ä—Ç {imp.split('.')[-1]}: –ù–ï –ù–ê–ô–î–ï–ù")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ prepare_dataloaders
        if 'use_bucket_batching' in train_content and 'use_dynamic_padding' in train_content:
            print("  ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ prepare_dataloaders: –ù–ê–ô–î–ï–ù–û")
        else:
            print("  ‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ prepare_dataloaders: –ù–ï –ù–ê–ô–î–ï–ù–û")
            return False
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ train.py: –û–®–ò–ë–ö–ê - {e}")
        return False

def test_loss_function_integration():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤ loss_function.py."""
    print("\nüîç –¢–ï–°–¢ 8: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ loss_function.py")
    
    try:
        with open('loss_function.py', 'r', encoding='utf-8') as f:
            loss_content = f.read()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–º–ø–æ—Ä—Ç DDCLossDiagnostic
        if 'from smart_tuner.ddc_diagnostic import get_global_ddc_diagnostic' in loss_content:
            print("  ‚úÖ –ò–º–ø–æ—Ä—Ç DDCLossDiagnostic: –ù–ê–ô–î–ï–ù")
        else:
            print("  ‚ùå –ò–º–ø–æ—Ä—Ç DDCLossDiagnostic: –ù–ï –ù–ê–ô–î–ï–ù")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        if 'ddc_diagnostic.analyze_size_mismatch' in loss_content:
            print("  ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: –ù–ê–ô–î–ï–ù–û")
        else:
            print("  ‚ùå –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: –ù–ï –ù–ê–ô–î–ï–ù–û")
            return False
        
        return True
        
    except Exception as e:
        print(f"  ‚ùå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ loss_function.py: –û–®–ò–ë–ö–ê - {e}")
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    print("üöÄ –ó–ê–ü–£–°–ö –¢–ï–°–¢–ê –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –£–õ–£–ß–®–ï–ù–ò–ô –ò–ó EXPORTED-ASSETS")
    print("=" * 70)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
    test_functions = [
        test_dynamic_padding,
        test_bucket_batching,
        test_smart_truncation_ddc,
        test_memory_efficient_ddc,
        test_ddc_diagnostic,
        test_safe_ddc_loss_modes,
        test_train_integration,
        test_loss_function_integration
    ]
    
    successful_tests = 0
    total_tests = len(test_functions)
    
    for test_func in test_functions:
        try:
            if test_func():
                successful_tests += 1
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ {test_func.__name__}: {e}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 70)
    print("üìä –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 70)
    
    print(f"üìà –†–ï–ó–£–õ–¨–¢–ê–¢: {successful_tests}/{total_tests} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ")
    
    if successful_tests == total_tests:
        print("üéâ –í–°–ï –£–õ–£–ß–®–ï–ù–ò–Ø –ò–ó EXPORTED-ASSETS –£–°–ü–ï–®–ù–û –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–´!")
        print("\n‚úÖ –ß—Ç–æ –±—ã–ª–æ –≤–Ω–µ–¥—Ä–µ–Ω–æ:")
        print("  ‚Ä¢ DynamicPaddingCollator - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π padding –¥–ª—è –±–∞—Ç—á–µ–π")
        print("  ‚Ä¢ BucketBatchSampler - –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        print("  ‚Ä¢ SmartTruncationDDC - —É–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        print("  ‚Ä¢ MemoryEfficientDDC - chunked –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        print("  ‚Ä¢ DDCLossDiagnostic - –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º DDC loss")
        print("  ‚Ä¢ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤ train.py –∏ loss_function.py")
        return True
    else:
        print("‚ö†Ô∏è –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–ë–õ–ï–ú–´ –° –ò–ù–¢–ï–ì–†–ê–¶–ò–ï–ô!")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 