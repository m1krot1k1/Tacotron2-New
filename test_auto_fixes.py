#!/usr/bin/env python3
"""
üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É AutoFixManager –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from smart_tuner.auto_fix_manager import AutoFixManager, FixAction

class MockModel(nn.Module):
    """–ú–æ–∫ –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.linear(x)

class MockOptimizer:
    """–ú–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    def __init__(self):
        self.param_groups = [{'lr': 1e-3}]

class MockHParams:
    """–ú–æ–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    def __init__(self):
        self.grad_clip_thresh = 1.0
        self.guide_loss_weight = 1.0
        self.p_attention_dropout = 0.1
        self.gate_threshold = 0.5
        self.gate_loss_weight = 1.0
        self.use_guided_attn = True
        self.fp16_run = False

class MockTelegramMonitor:
    """–ú–æ–∫ Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    def send_message(self, message):
        print(f"üì± TELEGRAM: {message}")
        return True

def test_gradient_vanishing_fix():
    """–¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
    print("üß™ –¢–µ—Å—Ç 1: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
    
    model = MockModel()
    optimizer = MockOptimizer()
    hparams = MockHParams()
    telegram_monitor = MockTelegramMonitor()
    
    auto_fix = AutoFixManager(model, optimizer, hparams, telegram_monitor)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    metrics = {
        'grad_norm': 1e-10,  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        'attention_diagonality': 0.5,
        'gate_accuracy': 0.7,
        'loss': 10.0
    }
    
    loss = torch.tensor(10.0)
    
    fixes = auto_fix.analyze_and_fix(step=100, metrics=metrics, loss=loss)
    
    print(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(fixes)}")
    for fix in fixes:
        print(f"  - {fix.description} (—É—Å–ø–µ—Ö: {fix.success})")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ learning rate —Å–Ω–∏–∑–∏–ª—Å—è
    new_lr = optimizer.param_groups[0]['lr']
    print(f"Learning rate: {new_lr:.2e}")
    
    return len(fixes) > 0

def test_gradient_explosion_fix():
    """–¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
    print("\nüß™ –¢–µ—Å—Ç 2: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
    
    model = MockModel()
    optimizer = MockOptimizer()
    hparams = MockHParams()
    telegram_monitor = MockTelegramMonitor()
    
    auto_fix = AutoFixManager(model, optimizer, hparams, telegram_monitor)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    metrics = {
        'grad_norm': 500.0,  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤—ã—Å–æ–∫–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        'attention_diagonality': 0.5,
        'gate_accuracy': 0.7,
        'loss': 10.0
    }
    
    fixes = auto_fix.analyze_and_fix(step=200, metrics=metrics)
    
    print(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(fixes)}")
    for fix in fixes:
        print(f"  - {fix.description} (—É—Å–ø–µ—Ö: {fix.success})")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ gradient clipping —É—Å–∏–ª–∏–ª—Å—è
    new_clip = hparams.grad_clip_thresh
    print(f"Gradient clip threshold: {new_clip}")
    
    return len(fixes) > 0

def test_attention_problems_fix():
    """–¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å attention"""
    print("\nüß™ –¢–µ—Å—Ç 3: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å attention")
    
    model = MockModel()
    optimizer = MockOptimizer()
    hparams = MockHParams()
    telegram_monitor = MockTelegramMonitor()
    
    auto_fix = AutoFixManager(model, optimizer, hparams, telegram_monitor)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å attention
    metrics = {
        'grad_norm': 1.0,
        'attention_diagonality': 0.05,  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
        'gate_accuracy': 0.7,
        'loss': 10.0
    }
    
    fixes = auto_fix.analyze_and_fix(step=300, metrics=metrics)
    
    print(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(fixes)}")
    for fix in fixes:
        print(f"  - {fix.description} (—É—Å–ø–µ—Ö: {fix.success})")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ guided attention weight —É–≤–µ–ª–∏—á–∏–ª—Å—è
    new_weight = hparams.guide_loss_weight
    print(f"Guided attention weight: {new_weight}")
    
    return len(fixes) > 0

def test_nan_problems_fix():
    """–¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è NaN –ø—Ä–æ–±–ª–µ–º"""
    print("\nüß™ –¢–µ—Å—Ç 4: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ NaN –ø—Ä–æ–±–ª–µ–º")
    
    model = MockModel()
    optimizer = MockOptimizer()
    hparams = MockHParams()
    telegram_monitor = MockTelegramMonitor()
    
    auto_fix = AutoFixManager(model, optimizer, hparams, telegram_monitor)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º NaN –≤ loss
    metrics = {
        'grad_norm': 1.0,
        'attention_diagonality': 0.5,
        'gate_accuracy': 0.7,
        'loss': float('nan')
    }
    
    loss = torch.tensor(float('nan'))
    
    fixes = auto_fix.analyze_and_fix(step=400, metrics=metrics, loss=loss)
    
    print(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {len(fixes)}")
    for fix in fixes:
        print(f"  - {fix.description} (—É—Å–ø–µ—Ö: {fix.success})")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ learning rate —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ —Å–Ω–∏–∑–∏–ª—Å—è
    new_lr = optimizer.param_groups[0]['lr']
    print(f"Learning rate: {new_lr:.2e}")
    
    return len(fixes) > 0

def test_fix_statistics():
    """–¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
    print("\nüß™ –¢–µ—Å—Ç 5: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π")
    
    model = MockModel()
    optimizer = MockOptimizer()
    hparams = MockHParams()
    telegram_monitor = MockTelegramMonitor()
    
    auto_fix = AutoFixManager(model, optimizer, hparams, telegram_monitor)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    metrics_list = [
        {'grad_norm': 1e-10, 'attention_diagonality': 0.5, 'gate_accuracy': 0.7, 'loss': 10.0},
        {'grad_norm': 500.0, 'attention_diagonality': 0.5, 'gate_accuracy': 0.7, 'loss': 10.0},
        {'grad_norm': 1.0, 'attention_diagonality': 0.05, 'gate_accuracy': 0.7, 'loss': 10.0}
    ]
    
    for i, metrics in enumerate(metrics_list):
        auto_fix.analyze_and_fix(step=500+i, metrics=metrics)
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = auto_fix.get_fix_statistics()
    
    print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:")
    print(f"  –í—Å–µ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {stats['total_fixes']}")
    print(f"  –£—Å–ø–µ—à–Ω—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {stats['successful_fixes']}")
    print(f"  –°—á–µ—Ç—á–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º: {stats['problem_counters']}")
    
    return stats['total_fixes'] > 0

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π\n")
    
    tests = [
        test_gradient_vanishing_fix,
        test_gradient_explosion_fix,
        test_attention_problems_fix,
        test_nan_problems_fix,
        test_fix_statistics
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
                print("‚úÖ –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω")
            else:
                print("‚ùå –¢–µ—Å—Ç –Ω–µ –ø—Ä–æ–π–¥–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}")
    
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {passed}/{total} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ")
    
    if passed == total:
        print("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã! AutoFixManager —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")
    else:
        print("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ–π–¥–µ–Ω—ã. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞.")
    
    return passed == total

if __name__ == "__main__":
    main() 