#!/usr/bin/env python3
"""
üöÄ CHECKPOINT OPTIMIZATION SYSTEM
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ checkpoint —Ñ–∞–π–ª–∞–º–∏ –¥–ª—è production

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö checkpoint —Ñ–∞–π–ª–æ–≤
‚úÖ Compression checkpoint —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
‚úÖ Backup –≤–∞–∂–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞—Ä—Ö–∏–≤
‚úÖ Smart cleanup –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
‚úÖ Production-ready —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∏—Å–∫–æ–≤—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º
"""

import os
import sys
import shutil
import gzip
import pickle
import torch
import hashlib
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass, asdict

@dataclass
class CheckpointInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ checkpoint —Ñ–∞–π–ª–µ"""
    path: Path
    size_mb: float
    created: datetime
    model_quality: Optional[float] = None
    is_compressed: bool = False
    hash_md5: Optional[str] = None

class CheckpointOptimizer:
    """üöÄ –ì–ª–∞–≤–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä checkpoint —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.backup_dir = self.project_root / "checkpoint_backup"
        self.compressed_dir = self.project_root / "checkpoint_compressed"
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.max_checkpoints_keep = 3  # –ú–∞–∫—Å–∏–º—É–º checkpoint —Ñ–∞–π–ª–æ–≤
        self.max_size_gb = 5.0         # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—Å–µ—Ö checkpoint (GB)
        self.compression_threshold_mb = 100  # –°–∂–∏–º–∞—Ç—å —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ 100MB
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        self.backup_dir.mkdir(exist_ok=True)
        self.compressed_dir.mkdir(exist_ok=True)
        
        self.logger.info("üöÄ Checkpoint Optimizer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.logger.info(f"üìÅ Backup –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.backup_dir}")
        self.logger.info(f"üì¶ Compression –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.compressed_dir}")
    
    def analyze_checkpoints(self) -> List[CheckpointInfo]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö checkpoint —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ"""
        self.logger.info("üîç –ê–Ω–∞–ª–∏–∑ checkpoint —Ñ–∞–π–ª–æ–≤...")
        
        checkpoint_files = []
        
        # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö .pt –∏ .pth —Ñ–∞–π–ª–æ–≤
        for pattern in ["*.pt", "*.pth"]:
            checkpoint_files.extend(self.project_root.glob(pattern))
            checkpoint_files.extend(self.project_root.glob(f"**/{pattern}"))
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ venv
        checkpoint_files = [f for f in checkpoint_files if 'venv' not in str(f)]
        
        checkpoints = []
        total_size = 0
        
        for file_path in checkpoint_files:
            try:
                stat = file_path.stat()
                size_mb = stat.st_size / (1024 * 1024)
                created = datetime.fromtimestamp(stat.st_mtime)
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                hash_md5 = self._calculate_md5(file_path)
                
                checkpoint_info = CheckpointInfo(
                    path=file_path,
                    size_mb=size_mb,
                    created=created,
                    hash_md5=hash_md5
                )
                
                checkpoints.append(checkpoint_info)
                total_size += size_mb
                
                self.logger.info(f"  üìÑ {file_path.name}: {size_mb:.1f}MB ({created.strftime('%Y-%m-%d %H:%M')})")
                
            except Exception as e:
                self.logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {file_path}: {e}")
        
        self.logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(checkpoints)} checkpoint —Ñ–∞–π–ª–æ–≤, –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:.1f}MB")
        return checkpoints
    
    def optimize_checkpoints(self, dry_run: bool = False) -> Dict[str, int]:
        """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ checkpoint —Ñ–∞–π–ª–æ–≤"""
        self.logger.info("üöÄ –ù–∞—á–∞–ª–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ checkpoint —Ñ–∞–π–ª–æ–≤...")
        if dry_run:
            self.logger.info("üîç DRY RUN —Ä–µ–∂–∏–º - –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        
        checkpoints = self.analyze_checkpoints()
        
        results = {
            'cleaned': 0,
            'compressed': 0,
            'backed_up': 0,
            'duplicates_removed': 0,
            'space_saved_mb': 0
        }
        
        # 1. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        results.update(self._remove_duplicates(checkpoints, dry_run))
        
        # 2. Backup —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ deprecated –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        results.update(self._backup_deprecated_files(checkpoints, dry_run))
        
        # 3. Cleanup –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö/–Ω–µ–Ω—É–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        results.update(self._cleanup_temporary_files(checkpoints, dry_run))
        
        # 4. Compression –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
        results.update(self._compress_large_files(checkpoints, dry_run))
        
        # 5. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö checkpoint —Ñ–∞–π–ª–æ–≤
        results.update(self._cleanup_old_checkpoints(checkpoints, dry_run))
        
        self.logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        self._print_optimization_summary(results)
        
        return results
    
    def _remove_duplicates(self, checkpoints: List[CheckpointInfo], dry_run: bool) -> Dict[str, int]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ MD5 —Ö–µ—à–∞"""
        self.logger.info("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        
        hash_to_files = {}
        for checkpoint in checkpoints:
            if checkpoint.hash_md5:
                if checkpoint.hash_md5 not in hash_to_files:
                    hash_to_files[checkpoint.hash_md5] = []
                hash_to_files[checkpoint.hash_md5].append(checkpoint)
        
        duplicates_removed = 0
        space_saved = 0
        
        for hash_value, files in hash_to_files.items():
            if len(files) > 1:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π
                files.sort(key=lambda x: x.created, reverse=True)
                
                for duplicate in files[1:]:  # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ (–Ω–æ–≤–µ–π—à–µ–≥–æ)
                    self.logger.info(f"  üóëÔ∏è –î—É–±–ª–∏–∫–∞—Ç: {duplicate.path.name}")
                    if not dry_run:
                        duplicate.path.unlink()
                    duplicates_removed += 1
                    space_saved += duplicate.size_mb
        
        return {'duplicates_removed': duplicates_removed, 'space_saved_mb': space_saved}
    
    def _backup_deprecated_files(self, checkpoints: List[CheckpointInfo], dry_run: bool) -> Dict[str, int]:
        """Backup —Ñ–∞–π–ª–æ–≤ –∏–∑ deprecated –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
        self.logger.info("üì¶ Backup deprecated —Ñ–∞–π–ª–æ–≤...")
        
        deprecated_patterns = [
            'output_auto_fixes',  # –°—Ç–∞—Ä–∞—è —Å–∏—Å—Ç–µ–º–∞ AutoFixManager
            'old_',
            'backup_',
            'temp_'
        ]
        
        backed_up = 0
        space_saved = 0
        
        for checkpoint in checkpoints:
            should_backup = False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ deprecated –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in deprecated_patterns:
                if pattern in str(checkpoint.path):
                    should_backup = True
                    break
            
            if should_backup:
                backup_path = self.backup_dir / checkpoint.path.name
                self.logger.info(f"  üì¶ Backup: {checkpoint.path.name} ‚Üí {backup_path}")
                
                if not dry_run:
                    shutil.move(str(checkpoint.path), str(backup_path))
                    # Compress backup
                    self._compress_file(backup_path)
                
                backed_up += 1
                space_saved += checkpoint.size_mb * 0.7  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
        
        return {'backed_up': backed_up, 'space_saved_mb': space_saved}
    
    def _cleanup_temporary_files(self, checkpoints: List[CheckpointInfo], dry_run: bool) -> Dict[str, int]:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        self.logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        temporary_patterns = [
            'interrupted_model.pth',  # –ü—Ä–µ—Ä–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å - –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å
            'temp_',
            'tmp_',
            '_temp.',
            'checkpoint_epoch_0.pth'  # –ù–∞—á–∞–ª—å–Ω—ã–π checkpoint - –æ–±—ã—á–Ω–æ –Ω–µ –Ω—É–∂–µ–Ω
        ]
        
        cleaned = 0
        space_saved = 0
        
        for checkpoint in checkpoints:
            should_clean = False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in temporary_patterns:
                if pattern in checkpoint.path.name:
                    should_clean = True
                    break
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã (—Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)
            if checkpoint.created < datetime.now() - timedelta(days=30):
                if 'checkpoint_epoch' in checkpoint.path.name:
                    should_clean = True
            
            if should_clean:
                self.logger.info(f"  üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ: {checkpoint.path.name}")
                
                if not dry_run:
                    try:
                        checkpoint.path.unlink()
                        cleaned += 1
                        space_saved += checkpoint.size_mb
                    except FileNotFoundError:
                        self.logger.info(f"    ‚ÑπÔ∏è –§–∞–π–ª —É–∂–µ —É–¥–∞–ª–µ–Ω: {checkpoint.path.name}")
                else:
                    cleaned += 1
                    space_saved += checkpoint.size_mb
        
        return {'cleaned': cleaned, 'space_saved_mb': space_saved}
    
    def _compress_large_files(self, checkpoints: List[CheckpointInfo], dry_run: bool) -> Dict[str, int]:
        """–°–∂–∞—Ç–∏–µ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        self.logger.info(f"üì¶ –°–∂–∞—Ç–∏–µ —Ñ–∞–π–ª–æ–≤ –±–æ–ª—å—à–µ {self.compression_threshold_mb}MB...")
        
        compressed = 0
        space_saved = 0
        
        for checkpoint in checkpoints:
            if (checkpoint.size_mb > self.compression_threshold_mb and 
                not checkpoint.is_compressed and
                checkpoint.path.exists()):
                
                compressed_path = self.compressed_dir / f"{checkpoint.path.name}.gz"
                
                self.logger.info(f"  üì¶ –°–∂–∞—Ç–∏–µ: {checkpoint.path.name} ‚Üí {compressed_path.name}")
                
                if not dry_run:
                    original_size = self._compress_file(checkpoint.path, compressed_path)
                    if compressed_path.exists():
                        # –£–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–∂–∞—Ç–∏–µ —É—Å–ø–µ—à–Ω–æ
                        checkpoint.path.unlink()
                        compressed_size = compressed_path.stat().st_size / (1024 * 1024)
                        space_saved += original_size - compressed_size
                
                compressed += 1
        
        return {'compressed': compressed, 'space_saved_mb': space_saved}
    
    def _cleanup_old_checkpoints(self, checkpoints: List[CheckpointInfo], dry_run: bool) -> Dict[str, int]:
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö checkpoint —Ñ–∞–π–ª–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ"""
        self.logger.info(f"üßπ –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö checkpoint (–æ—Å—Ç–∞–≤–ª—è–µ–º {self.max_checkpoints_keep})...")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ checkpoint —Ñ–∞–π–ª—ã (–∏—Å–∫–ª—é—á–∞–µ–º –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏)
        checkpoint_files = [
            cp for cp in checkpoints 
            if ('checkpoint' in cp.path.name.lower() and 
                'best' not in cp.path.name.lower() and
                cp.path.exists())
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        checkpoint_files.sort(key=lambda x: x.created, reverse=True)
        
        cleaned = 0
        space_saved = 0
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ checkpoint —Ñ–∞–π–ª—ã
        for old_checkpoint in checkpoint_files[self.max_checkpoints_keep:]:
            self.logger.info(f"  üóëÔ∏è –°—Ç–∞—Ä—ã–π checkpoint: {old_checkpoint.path.name}")
            
            if not dry_run:
                old_checkpoint.path.unlink()
            
            cleaned += 1
            space_saved += old_checkpoint.size_mb
        
        return {'cleaned': cleaned, 'space_saved_mb': space_saved}
    
    def _compress_file(self, source_path: Path, target_path: Optional[Path] = None) -> float:
        """–°–∂–∞—Ç–∏–µ —Ñ–∞–π–ª–∞ —Å –ø–æ–º–æ—â—å—é gzip"""
        if target_path is None:
            target_path = source_path.parent / f"{source_path.name}.gz"
        
        original_size = source_path.stat().st_size / (1024 * 1024)
        
        try:
            with open(source_path, 'rb') as f_in:
                with gzip.open(target_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            compressed_size = target_path.stat().st_size / (1024 * 1024)
            compression_ratio = (1 - compressed_size / original_size) * 100
            
            self.logger.info(f"    üì¶ –°–∂–∞—Ç–æ: {original_size:.1f}MB ‚Üí {compressed_size:.1f}MB ({compression_ratio:.1f}% —ç–∫–æ–Ω–æ–º–∏–∏)")
            return original_size
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è {source_path}: {e}")
            return 0
    
    def _calculate_md5(self, file_path: Path) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ MD5 —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —á–∞—Å—Ç—è–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception:
            return None
    
    def _print_optimization_summary(self, results: Dict[str, int]):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        print("\n" + "=" * 60)
        print("üéâ CHECKPOINT OPTIMIZATION COMPLETED!")
        print("=" * 60)
        print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {results.get('cleaned', 0)}")
        print(f"üì¶ –°–∂–∞—Ç–æ —Ñ–∞–π–ª–æ–≤: {results.get('compressed', 0)}")
        print(f"üíæ Backup —Ñ–∞–π–ª–æ–≤: {results.get('backed_up', 0)}")
        print(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ —É–¥–∞–ª–µ–Ω–æ: {results.get('duplicates_removed', 0)}")
        print(f"üíΩ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ –º–µ—Å—Ç–∞: {results.get('space_saved_mb', 0):.1f}MB")
        print("=" * 60)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        remaining_checkpoints = self.analyze_checkpoints()
        total_size = sum(cp.size_mb for cp in remaining_checkpoints)
        print(f"üìä –û—Å—Ç–∞–µ—Ç—Å—è checkpoint —Ñ–∞–π–ª–æ–≤: {len(remaining_checkpoints)}")
        print(f"üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size:.1f}MB")
        
        if total_size < self.max_size_gb * 1024:
            print("‚úÖ –†–∞–∑–º–µ—Ä checkpoint —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã!")
        else:
            print("‚ö†Ô∏è –†–∞–∑–º–µ—Ä checkpoint —Ñ–∞–π–ª–æ–≤ –≤—Å–µ –µ—â–µ –≤–µ–ª–∏–∫, —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è")

def create_production_checkpoint_config():
    """–°–æ–∑–¥–∞–Ω–∏–µ production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è checkpoint management"""
    config = {
        "checkpoint_optimization": {
            "enabled": True,
            "max_checkpoints_keep": 3,
            "max_size_gb": 5.0,
            "compression_threshold_mb": 100,
            "auto_cleanup_interval_hours": 24,
            "backup_old_files": True,
            "compress_backups": True
        },
        "retention_policy": {
            "keep_best_models": True,
            "keep_latest_n_checkpoints": 3,
            "delete_checkpoints_older_than_days": 30,
            "compress_files_older_than_days": 7
        },
        "monitoring": {
            "alert_on_disk_usage_percent": 85,
            "alert_on_total_size_gb": 10
        }
    }
    
    config_path = Path("checkpoint_optimization_config.json")
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f"‚úÖ Production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {config_path}")
    return config_path

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Checkpoint Optimization System')
    parser.add_argument('--dry-run', action='store_true', help='–ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ –±–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π')
    parser.add_argument('--create-config', action='store_true', help='–°–æ–∑–¥–∞—Ç—å production –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é')
    parser.add_argument('--project-root', default='.', help='–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞')
    
    args = parser.parse_args()
    
    if args.create_config:
        create_production_checkpoint_config()
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    optimizer = CheckpointOptimizer(args.project_root)
    results = optimizer.optimize_checkpoints(dry_run=args.dry_run)
    
    if args.dry_run:
        print("\nüí° –î–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–∑ --dry-run —Ñ–ª–∞–≥–∞")

if __name__ == "__main__":
    main() 