#!/usr/bin/env python3
"""
üß† META-LEARNING ENGINE - –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ–ø—ã—Ç–µ
=======================================================

–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–º –æ–ø—ã—Ç–µ –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç
—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
1. EpisodicMemory - –ø–∞–º—è—Ç—å –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–ø–∏–∑–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è
2. PatternAnalyzer - –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—Å–ø–µ—Ö–∞ –∏ –Ω–µ—É–¥–∞—á
3. StrategyAdaptor - –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞
4. MAMLOptimizer - Model-Agnostic Meta-Learning –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

–ê–≤—Ç–æ—Ä: Enhanced Tacotron2 AI System
–í–µ—Ä—Å–∏—è: 1.0.0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, asdict
from enum import Enum
from collections import deque, defaultdict
import logging
from datetime import datetime
import sqlite3
import time

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
try:
    from unified_logging_system import UnifiedLoggingSystem
    UNIFIED_LOGGING_AVAILABLE = True
except ImportError:
    UNIFIED_LOGGING_AVAILABLE = False

try:
    from context_aware_training_manager import TrainingPhase
    CONTEXT_MANAGER_AVAILABLE = True
except ImportError:
    # Fallback enum
    class TrainingPhase(Enum):
        PRE_ALIGNMENT = "pre_alignment"
        ALIGNMENT_LEARNING = "alignment"
        REFINEMENT = "refinement"
        CONVERGENCE = "convergence"
    CONTEXT_MANAGER_AVAILABLE = False


class LearningStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
    AGGRESSIVE = "aggressive"      # –ë—ã—Å—Ç—Ä—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    CONSERVATIVE = "conservative"  # –û—Å—Ç–æ—Ä–æ–∂–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    BALANCED = "balanced"         # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥
    ADAPTIVE = "adaptive"         # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è


@dataclass
class TrainingEpisode:
    """–≠–ø–∏–∑–æ–¥ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è episodic memory"""
    episode_id: str
    start_time: float
    end_time: float
    initial_phase: TrainingPhase
    final_phase: TrainingPhase
    
    # –ò—Å—Ö–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    initial_loss: float
    initial_attention_quality: float
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    final_loss: float
    final_attention_quality: float
    
    # –î–µ–π—Å—Ç–≤–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è
    strategy_used: LearningStrategy
    decisions_made: List[Dict[str, Any]]
    parameters_changed: Dict[str, float]
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç
    success: bool
    improvement_score: float
    convergence_achieved: bool
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    model_architecture: str = "tacotron2"
    dataset_size: int = 0
    total_steps: int = 0


@dataclass
class MetaLearningState:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ meta-learning —Å–∏—Å—Ç–µ–º—ã"""
    total_episodes: int
    successful_episodes: int
    preferred_strategies: Dict[str, float]  # strategy -> success_rate
    learned_patterns: Dict[str, Any]
    adaptation_history: List[Dict[str, Any]]
    last_updated: str


class EpisodicMemory:
    """üß† Episodic Memory - –ø–∞–º—è—Ç—å –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —ç–ø–∏–∑–æ–¥–∞—Ö –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, memory_dir: str = "meta_learning_memory", max_episodes: int = 1000):
        self.memory_dir = Path(memory_dir)
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        self.max_episodes = max_episodes
        self.episodes: List[TrainingEpisode] = []
        self.db_path = self.memory_dir / "episodes.db"
        
        self.logger = self._setup_logger()
        self._init_database()
        self._load_episodes()
        
        self.logger.info(f"üß† Episodic Memory –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {len(self.episodes)} —ç–ø–∏–∑–æ–¥–æ–≤")
    
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("EpisodicMemory")
        else:
            logger = logging.getLogger("EpisodicMemory")
            logger.setLevel(logging.INFO)
            return logger
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS episodes (
                    episode_id TEXT PRIMARY KEY,
                    data BLOB,
                    success INTEGER,
                    improvement_score REAL,
                    strategy_used TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_success ON episodes(success)
            ''')
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_strategy ON episodes(strategy_used)
            ''')
    
    def _load_episodes(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —ç–ø–∏–∑–æ–¥–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('SELECT data FROM episodes ORDER BY created_at')
                
                for (episode_data,) in cursor.fetchall():
                    try:
                        episode_dict = pickle.loads(episode_data)
                        episode = TrainingEpisode(**episode_dict)
                        self.episodes.append(episode)
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ø–∏–∑–æ–¥–∞: {e}")
                        
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ø–∏–∑–æ–¥–æ–≤: {e}")
    
    def _cleanup_old_episodes(self):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞"""
        if len(self.episodes) <= self.max_episodes:
            return
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (—Å—Ç–∞—Ä—ã–µ –ø–µ—Ä–≤—ã–º–∏)
        sorted_episodes = sorted(self.episodes, key=lambda x: x.start_time)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        episodes_to_remove = sorted_episodes[:len(self.episodes) - self.max_episodes]
        
        for episode in episodes_to_remove:
            try:
                # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute('DELETE FROM episodes WHERE episode_id = ?', (episode.episode_id,))
                
                # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑ –ø–∞–º—è—Ç–∏
                self.episodes.remove(episode)
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞: {e}")
        
        self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {len(episodes_to_remove)} —Å—Ç–∞—Ä—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤")
    
    def add_episode(self, episode: TrainingEpisode):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –≤ –ø–∞–º—è—Ç—å"""
        try:
            # –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–ø–∏–∑–æ–¥–∞
            episode_data = pickle.dumps(asdict(episode))
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO episodes 
                    (episode_id, data, success, improvement_score, strategy_used)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    episode.episode_id,
                    episode_data,
                    int(episode.success),
                    episode.improvement_score,
                    episode.strategy_used.value
                ))
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–∞–º—è—Ç—å
            self.episodes.append(episode)
            
            # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º –ø–∞–º—è—Ç–∏
            if len(self.episodes) > self.max_episodes:
                self._cleanup_old_episodes()
            
            self.logger.info(f"üìù –î–æ–±–∞–≤–ª–µ–Ω —ç–ø–∏–∑–æ–¥: {episode.episode_id} (—É—Å–ø–µ—Ö: {episode.success})")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —ç–ø–∏–∑–æ–¥–∞: {e}")
    
    def get_similar_episodes(self, 
                           current_phase: TrainingPhase,
                           current_loss: float,
                           current_attention: float,
                           top_k: int = 5) -> List[TrainingEpisode]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if not self.episodes:
            return []
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ similarity score –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞
        scored_episodes = []
        
        for episode in self.episodes:
            # Similarity –ø–æ phase
            phase_similarity = 1.0 if episode.initial_phase == current_phase else 0.5
            
            # Similarity –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
            loss_diff = abs(episode.initial_loss - current_loss)
            loss_similarity = 1.0 / (1.0 + loss_diff / 10.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            
            attention_diff = abs(episode.initial_attention_quality - current_attention)
            attention_similarity = 1.0 / (1.0 + attention_diff * 2.0)
            
            # –û–±—â–∏–π score —Å –≤–µ—Å–∞–º–∏
            total_similarity = (
                0.4 * phase_similarity +
                0.3 * loss_similarity +
                0.3 * attention_similarity
            )
            
            # –ë–æ–Ω—É—Å –∑–∞ —É—Å–ø–µ—à–Ω—ã–µ —ç–ø–∏–∑–æ–¥—ã
            if episode.success:
                total_similarity *= 1.2
                
            scored_episodes.append((total_similarity, episode))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é similarity
        scored_episodes.sort(reverse=True, key=lambda x: x[0])
        
        # –í–æ–∑–≤—Ä–∞—Ç top-k —ç–ø–∏–∑–æ–¥–æ–≤
        similar_episodes = [episode for _, episode in scored_episodes[:top_k]]
        
        self.logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(similar_episodes)} –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤")
        return similar_episodes
    
    def get_success_statistics(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –∏ —Ñ–∞–∑–∞–º"""
        if not self.episodes:
            return {}
        
        stats = {
            'total_episodes': len(self.episodes),
            'successful_episodes': sum(1 for ep in self.episodes if ep.success),
            'strategies': defaultdict(lambda: {'total': 0, 'successful': 0}),
            'phases': defaultdict(lambda: {'total': 0, 'successful': 0, 'avg_improvement': 0.0})
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
        for episode in self.episodes:
            strategy = episode.strategy_used.value
            stats['strategies'][strategy]['total'] += 1
            if episode.success:
                stats['strategies'][strategy]['successful'] += 1
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ñ–∞–∑–∞–º
        phase_improvements = defaultdict(list)
        for episode in self.episodes:
            phase = episode.initial_phase.value
            stats['phases'][phase]['total'] += 1
            if episode.success:
                stats['phases'][phase]['successful'] += 1
            phase_improvements[phase].append(episode.improvement_score)
        
        # –°—Ä–µ–¥–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –ø–æ —Ñ–∞–∑–∞–º
        for phase, improvements in phase_improvements.items():
            if improvements:
                stats['phases'][phase]['avg_improvement'] = np.mean(improvements)
        
        return dict(stats)


class PatternAnalyzer:
    """üîç Pattern Analyzer - –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—Å–ø–µ—Ö–∞ –∏ –Ω–µ—É–¥–∞—á"""
    
    def __init__(self, episodic_memory: EpisodicMemory):
        self.memory = episodic_memory
        self.logger = self._setup_logger()
        
        # –û–±—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        self.success_patterns = {}
        self.failure_patterns = {}
        self.decision_patterns = {}
        
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("PatternAnalyzer")
        else:
            logger = logging.getLogger("PatternAnalyzer")
            logger.setLevel(logging.INFO)
            return logger
    
    def analyze_patterns(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏"""
        episodes = self.memory.episodes
        if len(episodes) < 10:  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            return {}
        
        successful_episodes = [ep for ep in episodes if ep.success]
        failed_episodes = [ep for ep in episodes if not ep.success]
        
        patterns = {
            'success_patterns': self._analyze_success_patterns(successful_episodes),
            'failure_patterns': self._analyze_failure_patterns(failed_episodes),
            'decision_patterns': self._analyze_decision_patterns(episodes),
            'temporal_patterns': self._analyze_temporal_patterns(episodes)
        }
        
        self.logger.info(f"üîç –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(patterns)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        return patterns
    
    def _analyze_success_patterns(self, successful_episodes: List[TrainingEpisode]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—Å–ø–µ—à–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤"""
        if not successful_episodes:
            return {}
        
        patterns = {}
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
        strategy_counts = defaultdict(int)
        for ep in successful_episodes:
            strategy_counts[ep.strategy_used.value] += 1
        
        if strategy_counts:
            most_successful_strategy = max(strategy_counts.items(), key=lambda x: x[1])
            patterns['best_strategy'] = {
                'strategy': most_successful_strategy[0],
                'count': most_successful_strategy[1],
                'success_rate': most_successful_strategy[1] / len(successful_episodes)
            }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        lr_changes = [ep.parameters_changed.get('learning_rate', 0) for ep in successful_episodes]
        if lr_changes:
            patterns['lr_change_pattern'] = {
                'mean': np.mean(lr_changes),
                'std': np.std(lr_changes),
                'median': np.median(lr_changes)
            }
        
        return patterns
    
    def _analyze_failure_patterns(self, failed_episodes: List[TrainingEpisode]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ—É–¥–∞—á–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤"""
        if not failed_episodes:
            return {}
        
        patterns = {}
        
        # –ß–∞—Å—Ç—ã–µ –ø—Ä–∏—á–∏–Ω—ã –Ω–µ—É–¥–∞—á
        failure_strategies = defaultdict(int)
        for ep in failed_episodes:
            failure_strategies[ep.strategy_used.value] += 1
        
        if failure_strategies:
            worst_strategy = max(failure_strategies.items(), key=lambda x: x[1])
            patterns['worst_strategy'] = {
                'strategy': worst_strategy[0],
                'count': worst_strategy[1]
            }
        
        return patterns
    
    def _analyze_decision_patterns(self, episodes: List[TrainingEpisode]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π"""
        decision_outcomes = defaultdict(list)
        
        for episode in episodes:
            for decision in episode.decisions_made:
                decision_type = decision.get('type', 'unknown')
                decision_outcomes[decision_type].append(episode.success)
        
        patterns = {}
        for decision_type, outcomes in decision_outcomes.items():
            if len(outcomes) >= 3:  # –ú–∏–Ω–∏–º—É–º 3 –ø—Ä–∏–º–µ—Ä–∞
                success_rate = sum(outcomes) / len(outcomes)
                patterns[decision_type] = {
                    'total_decisions': len(outcomes),
                    'success_rate': success_rate,
                    'confidence': 'high' if len(outcomes) >= 10 else 'low'
                }
        
        return patterns
    
    def _analyze_temporal_patterns(self, episodes: List[TrainingEpisode]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —ç–ø–∏–∑–æ–¥–∞—Ö"""
        if len(episodes) < 5:
            return {}
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        sorted_episodes = sorted(episodes, key=lambda x: x.start_time)
        
        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success_trend = []
        window_size = min(5, len(sorted_episodes) // 2)
        
        for i in range(len(sorted_episodes) - window_size + 1):
            window_episodes = sorted_episodes[i:i + window_size]
            success_rate = sum(1 for ep in window_episodes if ep.success) / len(window_episodes)
            success_trend.append(success_rate)
        
        patterns = {}
        
        if len(success_trend) >= 2:
            # –û–±—â–∏–π —Ç—Ä–µ–Ω–¥
            trend_slope = (success_trend[-1] - success_trend[0]) / len(success_trend)
            
            if trend_slope > 0.05:
                patterns['success_trend'] = 'improving'
            elif trend_slope < -0.05:
                patterns['success_trend'] = 'declining'
            else:
                patterns['success_trend'] = 'stable'
            
            patterns['trend_slope'] = trend_slope
        
        return patterns


class StrategyAdaptor:
    """üéØ Strategy Adaptor - –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞"""
    
    def __init__(self, pattern_analyzer: PatternAnalyzer):
        self.pattern_analyzer = pattern_analyzer
        self.logger = self._setup_logger()
        
        # –¢–µ–∫—É—â–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        self.current_strategies = {
            'learning_rate_adaptation': 'balanced',
            'attention_weight_adaptation': 'conservative',
            'loss_weight_adaptation': 'adaptive'
        }
        
        # –ò—Å—Ç–æ—Ä–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–π
        self.adaptation_history = []
        
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("StrategyAdaptor")
        else:
            logger = logging.getLogger("StrategyAdaptor")
            logger.setLevel(logging.INFO)
            return logger
    
    def adapt_strategies(self, current_context: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–ø—ã—Ç–∞"""
        patterns = self.pattern_analyzer.analyze_patterns()
        
        if not patterns:
            self.logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π")
            return self.current_strategies.copy()
        
        adapted_strategies = self.current_strategies.copy()
        adaptations_made = []
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        success_patterns = patterns.get('success_patterns', {})
        if 'best_strategy' in success_patterns:
            best_strategy_info = success_patterns['best_strategy']
            if best_strategy_info['success_rate'] > 0.7:  # –í—ã—Å–æ–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
                best_strategy = best_strategy_info['strategy']
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—É—á—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
                for key in adapted_strategies:
                    if adapted_strategies[key] != best_strategy:
                        adapted_strategies[key] = best_strategy
                        adaptations_made.append(f"{key} -> {best_strategy}")
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        current_phase = current_context.get('phase', TrainingPhase.PRE_ALIGNMENT)
        current_loss = current_context.get('loss', 10.0)
        
        # –§–∞–∑–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        if current_phase == TrainingPhase.PRE_ALIGNMENT and current_loss > 15.0:
            adapted_strategies['learning_rate_adaptation'] = 'aggressive'
            adaptations_made.append("LR adaptation -> aggressive (–≤—ã—Å–æ–∫–∏–π initial loss)")
        
        elif current_phase == TrainingPhase.CONVERGENCE and current_loss < 3.0:
            adapted_strategies['learning_rate_adaptation'] = 'conservative'
            adaptations_made.append("LR adaptation -> conservative (–±–ª–∏–∑–∫–æ –∫ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏)")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–π
        if adaptations_made:
            adaptation_record = {
                'timestamp': datetime.now().isoformat(),
                'adaptations': adaptations_made,
                'context': current_context,
                'patterns_used': list(patterns.keys())
            }
            self.adaptation_history.append(adaptation_record)
            
            self.logger.info(f"üéØ –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {', '.join(adaptations_made)}")
        
        self.current_strategies = adapted_strategies
        return adapted_strategies.copy()


class MetaLearningEngine:
    """üß† Meta-Learning Engine - –≥–ª–∞–≤–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç —Å–∏—Å—Ç–µ–º—ã –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, 
                 memory_dir: str = "meta_learning_memory",
                 max_episodes: int = 1000):
        self.memory_dir = Path(memory_dir)
        self.memory_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.episodic_memory = EpisodicMemory(memory_dir, max_episodes)
        self.pattern_analyzer = PatternAnalyzer(self.episodic_memory)
        self.strategy_adaptor = StrategyAdaptor(self.pattern_analyzer)
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
        self.state = self._load_state()
        self.current_episode = None
        
        self.logger = self._setup_logger()
        self.logger.info("üß† Meta-Learning Engine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _setup_logger(self):
        if UNIFIED_LOGGING_AVAILABLE:
            return UnifiedLoggingSystem().register_component("MetaLearningEngine")
        else:
            logger = logging.getLogger("MetaLearningEngine")
            logger.setLevel(logging.INFO)
            return logger
    
    def start_episode(self, 
                     initial_context: Dict[str, Any],
                     episode_id: Optional[str] = None) -> str:
        """–ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if episode_id is None:
            episode_id = f"episode_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        self.current_episode = TrainingEpisode(
            episode_id=episode_id,
            start_time=time.time(),
            end_time=0.0,
            initial_phase=initial_context.get('phase', TrainingPhase.PRE_ALIGNMENT),
            final_phase=TrainingPhase.PRE_ALIGNMENT,
            initial_loss=initial_context.get('loss', 10.0),
            initial_attention_quality=initial_context.get('attention_quality', 0.1),
            final_loss=0.0,
            final_attention_quality=0.0,
            strategy_used=LearningStrategy.BALANCED,
            decisions_made=[],
            parameters_changed={},
            success=False,
            improvement_score=0.0,
            convergence_achieved=False,
            total_steps=0
        )
        
        self.logger.info(f"üöÄ –ù–∞—á–∞—Ç —ç–ø–∏–∑–æ–¥ –æ–±—É—á–µ–Ω–∏—è: {episode_id}")
        return episode_id
    
    def record_decision(self, decision: Dict[str, Any]):
        """–ó–∞–ø–∏—Å—å —Ä–µ—à–µ–Ω–∏—è –≤ —Ç–µ–∫—É—â–∏–π —ç–ø–∏–∑–æ–¥"""
        if self.current_episode:
            self.current_episode.decisions_made.append({
                **decision,
                'timestamp': time.time()
            })
    
    def get_recommended_strategy(self, current_context: Dict[str, Any]) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞"""
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        adapted_strategies = self.strategy_adaptor.adapt_strategies(current_context)
        
        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        similar_episodes = self.episodic_memory.get_similar_episodes(
            current_phase=current_context.get('phase', TrainingPhase.PRE_ALIGNMENT),
            current_loss=current_context.get('loss', 10.0),
            current_attention=current_context.get('attention_quality', 0.1)
        )
        
        recommendations = {
            'strategies': adapted_strategies,
            'similar_episodes_count': len(similar_episodes),
            'confidence': 'high' if len(similar_episodes) >= 3 else 'medium',
            'learning_insights': self._extract_insights(similar_episodes)
        }
        
        return recommendations
    
    def _extract_insights(self, similar_episodes: List[TrainingEpisode]) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ insights –∏–∑ –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤"""
        if not similar_episodes:
            return ["–ù–µ—Ç –æ–ø—ã—Ç–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏"]
        
        insights = []
        
        # –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ—Ö–æ–∂–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        successful_episodes = [ep for ep in similar_episodes if ep.success]
        success_rate = len(successful_episodes) / len(similar_episodes)
        
        if success_rate > 0.7:
            insights.append(f"–í—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ ({success_rate:.1%}) –≤ –ø–æ—Ö–æ–∂–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö")
        elif success_rate < 0.3:
            insights.append(f"–ù–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ ({success_rate:.1%}) - —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        if successful_episodes:
            strategies = [ep.strategy_used for ep in successful_episodes]
            most_common = max(set(strategies), key=strategies.count)
            insights.append(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {most_common.value}")
        
        return insights
    
    def end_episode(self, final_context: Dict[str, Any]) -> bool:
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —ç–ø–∏–∑–æ–¥–∞"""
        if not self.current_episode:
            self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
            return False
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        self.current_episode.end_time = time.time()
        self.current_episode.final_phase = final_context.get('phase', TrainingPhase.PRE_ALIGNMENT)
        self.current_episode.final_loss = final_context.get('loss', self.current_episode.initial_loss)
        self.current_episode.final_attention_quality = final_context.get('attention_quality', 
                                                                        self.current_episode.initial_attention_quality)
        self.current_episode.total_steps = final_context.get('total_steps', 0)
        
        # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è
        loss_improvement = (self.current_episode.initial_loss - self.current_episode.final_loss) / self.current_episode.initial_loss
        attention_improvement = self.current_episode.final_attention_quality - self.current_episode.initial_attention_quality
        
        self.current_episode.improvement_score = loss_improvement * 0.6 + attention_improvement * 0.4
        self.current_episode.success = (
            loss_improvement > 0.1 and  # –ú–∏–Ω–∏–º—É–º 10% —É–ª—É—á—à–µ–Ω–∏–µ loss
            attention_improvement > 0.05 and  # –ú–∏–Ω–∏–º—É–º 5% —É–ª—É—á—à–µ–Ω–∏–µ attention
            self.current_episode.final_loss < 10.0  # –†–∞–∑—É–º–Ω—ã–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π loss
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞ –≤ –ø–∞–º—è—Ç—å
        self.episodic_memory.add_episode(self.current_episode)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self.state.total_episodes += 1
        if self.current_episode.success:
            self.state.successful_episodes += 1
        
        episode_id = self.current_episode.episode_id
        success = self.current_episode.success
        
        self.current_episode = None
        self._save_state()
        
        self.logger.info(f"üèÅ –≠–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω: {episode_id} (—É—Å–ø–µ—Ö: {success})")
        return success
    
    def _load_state(self) -> MetaLearningState:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è"""
        state_file = self.memory_dir / "meta_learning_state.json"
        
        if state_file.exists():
            try:
                with open(state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                return MetaLearningState(
                    total_episodes=data.get('total_episodes', 0),
                    successful_episodes=data.get('successful_episodes', 0),
                    preferred_strategies=data.get('preferred_strategies', {}),
                    learned_patterns=data.get('learned_patterns', {}),
                    adaptation_history=data.get('adaptation_history', []),
                    last_updated=data.get('last_updated', datetime.now().isoformat())
                )
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return MetaLearningState(
            total_episodes=0,
            successful_episodes=0,
            preferred_strategies={},
            learned_patterns={},
            adaptation_history=[],
            last_updated=datetime.now().isoformat()
        )
    
    def _save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è"""
        state_file = self.memory_dir / "meta_learning_state.json"
        
        try:
            self.state.last_updated = datetime.now().isoformat()
            
            with open(state_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.state), f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
    
    def get_learning_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        memory_stats = self.episodic_memory.get_success_statistics()
        
        success_rate = 0.0
        if self.state.total_episodes > 0:
            success_rate = self.state.successful_episodes / self.state.total_episodes
        
        return {
            'meta_learning_stats': {
                'total_episodes': self.state.total_episodes,
                'successful_episodes': self.state.successful_episodes,
                'success_rate': success_rate,
                'adaptations_made': len(self.strategy_adaptor.adaptation_history)
            },
            'memory_stats': memory_stats,
            'current_strategies': self.strategy_adaptor.current_strategies.copy(),
            'system_maturity': self._assess_system_maturity()
        }
    
    def _assess_system_maturity(self) -> str:
        """–û—Ü–µ–Ω–∫–∞ –∑—Ä–µ–ª–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏—è"""
        episodes = self.state.total_episodes
        
        if episodes < 10:
            return "novice"  # –ù–æ–≤–∏—á–æ–∫
        elif episodes < 50:
            return "learning"  # –û–±—É—á–∞–µ—Ç—Å—è
        elif episodes < 100:
            return "experienced"  # –û–ø—ã—Ç–Ω–∞—è
        else:
            return "expert"  # –≠–∫—Å–ø–µ—Ä—Ç


# –§—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
def create_meta_learning_engine(memory_dir: str = "meta_learning_memory") -> MetaLearningEngine:
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ Meta-Learning Engine"""
    return MetaLearningEngine(memory_dir=memory_dir)


if __name__ == "__main__":
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("üß† Meta-Learning Engine –¥–µ–º–æ")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ engine
    meta_engine = create_meta_learning_engine()
    
    # –°–∏–º—É–ª—è—Ü–∏—è —ç–ø–∏–∑–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è
    initial_context = {
        'phase': TrainingPhase.PRE_ALIGNMENT,
        'loss': 15.0,
        'attention_quality': 0.1
    }
    
    episode_id = meta_engine.start_episode(initial_context)
    print(f"üìù –ù–∞—á–∞—Ç —ç–ø–∏–∑–æ–¥: {episode_id}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    recommendations = meta_engine.get_recommended_strategy(initial_context)
    print(f"üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {recommendations}")
    
    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞
    final_context = {
        'phase': TrainingPhase.ALIGNMENT_LEARNING,
        'loss': 8.0,
        'attention_quality': 0.4,
        'total_steps': 1000
    }
    
    success = meta_engine.end_episode(final_context)
    print(f"üèÅ –≠–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ: {success}")
    
    print("‚úÖ Meta-Learning Engine –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!") 