import os
import time
import argparse
import math
import random
import numpy as np

import torch
from distributed import apply_gradient_allreduce
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader

import gradient_adaptive_factor
from model import Tacotron2
from data_utils import TextMelLoader, TextMelCollate
from loss_function import Tacotron2Loss
from hparams import create_hparams
from plotting_utils import plot_alignment_to_numpy, plot_spectrogram_to_numpy, plot_gate_outputs_to_numpy
from text import symbol_to_id
from utils import to_gpu
# from logger import Tacotron2Logger  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
from auto_param_controller import AutoParamController

# MLflow for experiment tracking
try:
    import mlflow
    import mlflow.pytorch  # –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ PyTorch
    MLFLOW_AVAILABLE = True
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    try:
        from mlflow_metrics_enhancer import log_enhanced_training_metrics, log_system_metrics
        ENHANCED_LOGGING = True
        print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–æ–µ MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ")
    except ImportError:
        ENHANCED_LOGGING = False
        # MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Å–∫—Ä—ã—Ç–æ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã –ª–æ–≥–æ–≤)
        
except ImportError:
    MLFLOW_AVAILABLE = False
    ENHANCED_LOGGING = False

# –ü–æ–¥–∞–≤–ª—è–µ–º –ª–∏—à–Ω–∏–µ warning'–∏
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, message=".*torch.cuda.*DtypeTensor.*")
warnings.filterwarnings("ignore", message=".*deprecated.*")


def reduce_tensor(tensor, n_gpus):
    rt = tensor.clone()
    dist.all_reduce(rt, op=dist.ReduceOp.SUM)
    rt /= n_gpus
    return rt


def init_distributed(hparams, n_gpus, rank, group_name):
    assert torch.cuda.is_available(), "Distributed mode requires CUDA."
    print("Initializing Distributed")

    # Set cuda device so everything is done on the right GPU.
    torch.cuda.set_device(rank % torch.cuda.device_count())

    # Initialize distributed communication
    dist.init_process_group(
        backend=hparams.dist_backend, init_method=hparams.dist_url,
        world_size=n_gpus, rank=rank, group_name=group_name)

    print("Done initializing distributed")


def prepare_dataloaders(hparams):
    # Get data, data loaders and collate function ready
    trainset = TextMelLoader(hparams.training_files, hparams)
    valset = TextMelLoader(hparams.validation_files, hparams)
    collate_fn = TextMelCollate(hparams.n_frames_per_step)

    if hparams.distributed_run:
        train_sampler = DistributedSampler(trainset)
        shuffle = False
    else:
        train_sampler = None
        shuffle = True

    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,
                              sampler=train_sampler,
                              batch_size=hparams.batch_size, pin_memory=False,
                              drop_last=True, collate_fn=collate_fn)
    return train_loader, valset, collate_fn


def load_model(hparams):
    model = Tacotron2(hparams).cuda()
    # FP16 –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ AMP, –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–µ—Ç—Å—è –≤ FP32
    
    if hparams.distributed_run:
        model = apply_gradient_allreduce(model)

    return model


def warm_start_model(checkpoint_path, model, ignore_layers, exclude=None):
    assert os.path.isfile(checkpoint_path)
    print("Warm starting model from checkpoint '{}'".format(checkpoint_path))
    # –î–æ–±–∞–≤–ª—è–µ–º weights_only=False –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å PyTorch 2.6+
    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    model_dict = checkpoint_dict['state_dict']
    # model_dict['embedding.embedding.weight'] = model_dict['embedding.weight']
    # del model_dict['embedding.weight']
    print("ignoring layers:",ignore_layers)
    if len(ignore_layers) > 0 or exclude:
        model_dict = {k: v for k, v in model_dict.items()
                      if k not in ignore_layers and (not exclude or exclude not in k)}
        
        dummy_dict = model.state_dict()
        dummy_dict.update(model_dict)
        model_dict = dummy_dict
    model.load_state_dict(model_dict, strict=False)
    return model


def load_checkpoint(checkpoint_path, model, optimizer):
    assert os.path.isfile(checkpoint_path)
    print("Loading checkpoint '{}'".format(checkpoint_path))
    # –î–æ–±–∞–≤–ª—è–µ–º weights_only=False –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å PyTorch 2.6+
    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    model.load_state_dict(checkpoint_dict['state_dict'])
    optimizer.load_state_dict(checkpoint_dict['optimizer'])
    learning_rate = checkpoint_dict['learning_rate']
    iteration = checkpoint_dict['iteration']
    print("Loaded checkpoint '{}' from iteration {}" .format(
        checkpoint_path, iteration))
    return model, optimizer, learning_rate, iteration


def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):
    print("Saving model and optimizer state at iteration {} to {}".format(
        iteration, filepath))
    torch.save({'iteration': iteration,
                'state_dict': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'learning_rate': learning_rate}, filepath)


def validate(model, criterion, valset, iteration, batch_size, n_gpus,
             collate_fn, writer, distributed_run=False, rank=1, minimize=False):
    """Handles all the validation scoring and printing"""
    model.eval()
    with torch.no_grad():
        val_sampler = DistributedSampler(valset) if distributed_run else None
        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,
                                shuffle=False, batch_size=batch_size,
                                pin_memory=False, collate_fn=collate_fn)
        model.decoder.p_teacher_forcing = 0.0
        val_loss = 0.0
        for i, batch in enumerate(val_loader):
            x, y = model.parse_batch(batch)
            y_pred = model(x, minimize)
            _loss = criterion(y_pred, y)
            loss = sum(_loss)
            if distributed_run:
                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()
            else:
                reduced_val_loss = loss.item()
            val_loss += reduced_val_loss
        val_loss = val_loss / (i + 1)

    # üî• –§–ò–ù–ê–õ–¨–ù–û–ï –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –≤ train —Ä–µ–∂–∏–º
    model.train()
    model.decoder.p_teacher_forcing = 1.0
    if rank == 0:
        print("Validation loss {}: {:9f}  ".format(iteration, val_loss))
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ writer
        writer.add_scalar("validation.loss", val_loss, iteration)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–≤–∑—è—Ç–æ –∏–∑ Tacotron2Logger)
        try:
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π inference —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏
            with torch.no_grad():
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º eval —Ä–µ–∂–∏–º –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è BatchNorm –æ—à–∏–±–æ–∫
                model.eval()
                
                # üî• –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ validation –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                # –í–º–µ—Å—Ç–æ inference –∏—Å–ø–æ–ª—å–∑—É–µ–º training forward pass –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ validation batch
                    validation_outputs = model(x, minimize=False)
                    
                    # validation_outputs: [decoder_outputs, mel_outputs, mel_outputs_postnet, gate_outputs, alignments, ...]
                    if len(validation_outputs) >= 5:
                        decoder_outputs_val, mel_outputs_val, mel_outputs_postnet_val, gate_outputs_val, alignments_val = validation_outputs[:5]
                        
                        # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º validation outputs (–æ–Ω–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
                        inference_outputs = [None, mel_outputs_val, mel_outputs_postnet_val, gate_outputs_val, alignments_val]
                        print(f"‚úÖ Validation forward pass: mel={mel_outputs_postnet_val.shape if mel_outputs_postnet_val is not None else 'None'}, "
                              f"gate={gate_outputs_val.shape if gate_outputs_val is not None else 'None'}, "
                              f"align={alignments_val.shape if alignments_val is not None else 'None'}")
                    else:
                        print(f"‚ö†Ô∏è Validation outputs –Ω–µ–ø–æ–ª–Ω—ã–µ: {len(validation_outputs)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                        inference_outputs = None
                        
                except Exception as val_e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ validation forward pass: {val_e}")
                    
                    # Fallback –∫ inference —Å –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    try:
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
                        input_text = x[0][:1] if x[0].size(0) > 0 else x[0]
                        
                        if input_text.size(0) == 0:
                            print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –±–∞—Ç—á –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                            inference_outputs = None
                        else:
                            inference_outputs = model.inference(input_text)
                            print(f"üìù Fallback inference –∑–∞–≤–µ—Ä—à–µ–Ω")
                    except Exception as inf_e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ fallback inference: {inf_e}")
                        inference_outputs = None
            
            # inference –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [None, mel_outputs, mel_outputs_postnet, gate_outputs, alignments, emb_gst]
            if inference_outputs is not None and len(inference_outputs) >= 5:
                _, mel_outputs_inf, mel_outputs_postnet_inf, gate_outputs_inf, alignments_inf = inference_outputs[:5]
                mel_targets, gate_targets = y[0], y[1]
                
                print(f"üñºÔ∏è –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è TensorBoard (iteration {iteration})")
                
                # plot distribution of parameters (—Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 500 –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏)
                if iteration % 500 == 0:
                    for tag, value in model.named_parameters():
                        tag = tag.replace('.', '/')
                        writer.add_histogram(tag, value.data.cpu().numpy(), iteration)
                
                idx = 0  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –±–∞—Ç—á–∞
                
                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤
                
                # Alignment –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if alignments_inf is not None and alignments_inf.size(0) > idx:
                    try:
                        alignment_data = alignments_inf[idx].data.cpu().numpy()
                        if alignment_data.shape[0] > 1 and alignment_data.shape[1] > 1:
                            alignment_img = plot_alignment_to_numpy(alignment_data.T)
                            writer.add_image("alignment", alignment_img, iteration, dataformats='HWC')
                            print(f"‚úÖ Alignment –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {alignment_img.shape}")
                        else:
                            print(f"‚ö†Ô∏è Alignment –º–∞—Ç—Ä–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è: {alignment_data.shape}")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è alignment –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                
                # Mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if mel_targets.size(0) > idx:
                    try:
                        mel_target_data = mel_targets[idx].data.cpu().numpy()
                        if mel_target_data.shape[0] > 1 and mel_target_data.shape[1] > 1:
                            mel_target_img = plot_spectrogram_to_numpy(mel_target_data)
                            writer.add_image("mel_target", mel_target_img, iteration, dataformats='HWC')
                            print(f"‚úÖ Mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {mel_target_img.shape}")
                        else:
                            print(f"‚ö†Ô∏è Mel target —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {mel_target_data.shape}")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                
                # Mel predicted –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if mel_outputs_inf is not None and mel_outputs_inf.size(0) > idx:
                    try:
                        mel_pred_data = mel_outputs_inf[idx].data.cpu().numpy()
                        if mel_pred_data.shape[0] > 1 and mel_pred_data.shape[1] > 1:
                            mel_pred_img = plot_spectrogram_to_numpy(mel_pred_data)
                            writer.add_image("mel_predicted", mel_pred_img, iteration, dataformats='HWC')
                            print(f"‚úÖ Mel predicted –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {mel_pred_img.shape}")
                        else:
                            print(f"‚ö†Ô∏è Mel predicted —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {mel_pred_data.shape}")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è mel predicted –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                
                # Gate outputs –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if gate_outputs_inf is not None and gate_outputs_inf.size(0) > idx and gate_targets.size(0) > idx:
                    try:
                        gate_target_data = gate_targets[idx].data.cpu().numpy()
                        gate_pred_data = torch.sigmoid(gate_outputs_inf[idx]).data.cpu().numpy()
                        
                        if len(gate_target_data) > 1 and len(gate_pred_data) > 1:
                            gate_img = plot_gate_outputs_to_numpy(gate_target_data, gate_pred_data)
                            writer.add_image("gate", gate_img, iteration, dataformats='HWC')
                            print(f"‚úÖ Gate –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {gate_img.shape}")
                        else:
                            print(f"‚ö†Ô∏è Gate –¥–∞–Ω–Ω—ã–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ: target={len(gate_target_data)}, pred={len(gate_pred_data)}")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è gate –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                        
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ TensorBoard
                writer.flush()
                print(f"üîÑ TensorBoard –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}")
                
            else:
                print(f"‚ö†Ô∏è Inference –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                
            # üî• –í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ train —Ä–µ–∂–∏–º
            model.train()
                
        except Exception as e:
            print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            # Fallback - —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            try:
                mel_targets, gate_targets = y[0], y[1]
                if mel_targets.size(0) > 0:
                    mel_target_img = plot_spectrogram_to_numpy(mel_targets[0].data.cpu().numpy())
                    writer.add_image("mel_target_fallback", mel_target_img, iteration, dataformats='HWC')
                    print(f"‚úÖ Fallback mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ")
            except Exception as fallback_e:
                print(f"‚ùå –î–∞–∂–µ fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å: {fallback_e}")
                
        # üî• –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û: –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –≤ train —Ä–µ–∂–∏–º–µ
        model.train()

        if MLFLOW_AVAILABLE:
            validation_metrics = {
                "validation.loss": val_loss,
                "validation.step": iteration
            }
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –º–æ–¥–µ–ª–∏
            if hasattr(model, 'decoder') and hasattr(model.decoder, 'attention_weights'):
                try:
                    # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ attention
                    attention_weights = model.decoder.attention_weights
                    if attention_weights is not None:
                        validation_metrics["validation.attention_entropy"] = float(
                            torch.mean(torch.sum(-attention_weights * torch.log(attention_weights + 1e-8), dim=-1))
                        )
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ attention entropy: {e}")
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ alignments
            if alignments_inf is not None:
                try:
                    # –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å alignment –º–∞—Ç—Ä–∏—Ü—ã
                    alignment_diag = torch.diagonal(alignments_inf[0], dim1=-2, dim2=-1)
                    align_score = float(torch.mean(alignment_diag))
                    validation_metrics["validation.alignment_score"] = align_score
                    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è AutoParamController
                    try:
                        model.last_validation_alignment_score = align_score
                    except Exception:
                        pass
                    # –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ attention (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏)
                    attention_focus = torch.max(alignments_inf[0], dim=-1)[0]
                    validation_metrics["validation.attention_focus"] = float(torch.mean(attention_focus))
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ attention –º–µ—Ç—Ä–∏–∫: {e}")
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∞—Ç—Ä–∏–±—É—Ç –º–æ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–∞–∂–µ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ align_score
            if not hasattr(model, 'last_validation_alignment_score'):
                model.last_validation_alignment_score = None
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ gate outputs
            if gate_outputs_inf is not None:
                try:
                    gate_probs = torch.sigmoid(gate_outputs_inf[0])
                    validation_metrics["validation.gate_mean"] = float(torch.mean(gate_probs))
                    validation_metrics["validation.gate_std"] = float(torch.std(gate_probs))
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ gate –º–µ—Ç—Ä–∏–∫: {e}")
            
            if ENHANCED_LOGGING:
                # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ validation –º–µ—Ç—Ä–∏–∫
                log_enhanced_training_metrics(validation_metrics, iteration)
            else:
                # –ë–∞–∑–æ–≤–æ–µ MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                for metric_name, metric_value in validation_metrics.items():
                    mlflow.log_metric(metric_name, metric_value, step=iteration)
    
    return val_loss

def calculate_global_mean(data_loader, global_mean_npy):
    if global_mean_npy and os.path.exists(global_mean_npy):
        global_mean = np.load(global_mean_npy)
        return to_gpu(torch.tensor(global_mean))
    sums = []
    frames = []
    print('\nCalculating global mean...\n')
    for i, batch in enumerate(data_loader):
        (text_padded, input_lengths, mel_padded, gate_padded,
         output_lengths, ctc_text, ctc_text_lengths, _guide_mask) = batch
        # padded values are 0.
        sums.append(mel_padded.double().sum(dim=(0, 2)))
        frames.append(output_lengths.double().sum())
    global_mean = sum(sums) / sum(frames)
    global_mean = global_mean.float()
    if global_mean_npy:
        np.save(global_mean_npy, global_mean)
    return to_gpu(global_mean)

def train(output_directory, log_directory, checkpoint_path, warm_start, ignore_mmi_layers, ignore_gst_layers, ignore_tsgst_layers, n_gpus,
          rank, group_name, hparams,
          # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–æ Smart Tuner
          smart_tuner_trial=None,
          smart_tuner_logger=None,
          tensorboard_writer=None,
          telegram_monitor=None):
    """Training and validation logging results to tensorboard and stdout

    Params
    ------
    output_directory (string): directory to save checkpoints
    log_directory (string) directory to save tensorboard logs
    checkpoint_path(string): checkpoint path
    warm_start(bool): load model from checkpoint
    n_gpus (int): number of gpus
    rank (int): rank of current gpu
    hparams (object): comma separated list of "name=value" pairs.
    """
    if hparams.distributed_run:
        init_distributed(hparams, n_gpus, rank, group_name)

    torch.manual_seed(hparams.seed)
    torch.cuda.manual_seed(hparams.seed)
    random.seed(hparams.seed)
    np.random.seed(hparams.seed)

    model = load_model(hparams)
    learning_rate = hparams.learning_rate
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,
                                 weight_decay=hparams.weight_decay)

    # ---------- FP16 / Mixed Precision ---------
    apex_available = False  # –§–ª–∞–≥ –Ω–∞–ª–∏—á–∏—è NVIDIA Apex
    use_native_amp = False  # –§–ª–∞–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è torch.cuda.amp
    scaler = None           # GradScaler –¥–ª—è native AMP

    if hparams.fp16_run:
        try:
            from apex import amp
            model, optimizer = amp.initialize(
                model, optimizer, opt_level='O2')
            apex_available = True
            print("‚úÖ NVIDIA Apex —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è FP16 –æ–±—É—á–µ–Ω–∏—è")
        except ImportError:
            # Apex –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äì –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π AMP PyTorch
            try:
                from torch.amp import GradScaler, autocast
                # –û—Ç–∫–ª—é—á–∞–µ–º FP16 –¥–ª—è –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ AMP
                model = model.float()  # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ FP32
                scaler = GradScaler('cuda')
                use_native_amp = True
                print("‚úÖ NVIDIA Apex –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ torch.amp (PyTorch Native AMP)")
            except ImportError as e:
                # –î–∞–∂–µ native AMP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äì –æ—Ç–∫–ª—é—á–∞–µ–º FP16
                hparams.fp16_run = False
                print(f"‚ùå Mixed precision –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}. FP16 –æ—Ç–∫–ª—é—á—ë–Ω.")
    # -------------------------------------------

    if hparams.distributed_run:
        model = apply_gradient_allreduce(model)

    criterion = Tacotron2Loss(hparams)

    train_loader, valset, collate_fn = prepare_dataloaders(hparams)

    # Load checkpoint if one exists
    iteration = 0
    epoch_offset = 0
    if checkpoint_path is not None:
        if warm_start:
            model = warm_start_model(
                checkpoint_path, model, hparams.ignore_layers)
        else:
            model, optimizer, _learning_rate, iteration = load_checkpoint(
                checkpoint_path, model, optimizer)
            if hparams.use_saved_learning_rate:
                learning_rate = _learning_rate
            iteration += 1  # next iteration is iteration + 1
            epoch_offset = max(0, int(iteration / len(train_loader)))

    model.train()
    is_main_node = (rank == 0)

    # üí° –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π writer, –∞ –Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
    writer = tensorboard_writer if is_main_node else None

    if is_main_node and writer is None:
        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –µ—Å–ª–∏ train.py –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é
        from torch.utils.tensorboard import SummaryWriter
        writer = SummaryWriter(log_directory)

    if hparams.use_mmi:
        from mmi_loss import MMI_loss
        mmi_loss = MMI_loss(hparams.mmi_map, hparams.mmi_weight)
        print("‚úÖ MMI loss –∑–∞–≥—Ä—É–∂–µ–Ω")

    if hparams.use_guided_attn:
        from loss_function import GuidedAttentionLoss
        guide_loss = GuidedAttentionLoss(alpha=hparams.guided_attn_weight)
        print("‚úÖ Guided Attention Loss –∑–∞–≥—Ä—É–∂–µ–Ω")

    # --- Auto Hyper-parameter Controller ---
    auto_ctrl = None
    if is_main_node and hparams.use_guided_attn:
        try:
            auto_ctrl = AutoParamController(optimizer=optimizer,
                                            guide_loss=guide_loss,
                                            hparams=hparams,
                                            writer=writer)
            print("ü§ñ AutoParamController –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å AutoParamController: {e}")

    global_mean = calculate_global_mean(train_loader, hparams.global_mean_npy)

    # ================ MAIN TRAINNIG LOOP ===================
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ: epochs={hparams.epochs}, batch_size={hparams.batch_size}, dataset_size={len(train_loader)}")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤ MLflow
    if is_main_node and MLFLOW_AVAILABLE:
        model_params = {
            "model.total_params": sum(p.numel() for p in model.parameters()),
            "model.trainable_params": sum(p.numel() for p in model.parameters() if p.requires_grad),
            "hparams.batch_size": hparams.batch_size,
            "hparams.learning_rate": hparams.learning_rate,
            "hparams.epochs": hparams.epochs,
            "hparams.grad_clip_thresh": hparams.grad_clip_thresh,
            "hparams.fp16_run": hparams.fp16_run,
            "hparams.use_mmi": hparams.use_mmi,
            "hparams.use_guided_attn": hparams.use_guided_attn,
            "dataset.train_size": len(train_loader),
            "dataset.val_size": len(valset)
        }
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        mlflow.log_params(model_params)
        print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow: {model_params['model.total_params']} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    for epoch in range(epoch_offset, hparams.epochs):
        print("Epoch: {} / {}".format(epoch, hparams.epochs))
        for i, batch in enumerate(train_loader):
            start = time.perf_counter()
            model.zero_grad()
            
            x, y = model.parse_batch(batch)

            # Forward pass —Å —É—á—ë—Ç–æ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ö–µ–º—ã mixed precision
            if hparams.fp16_run and use_native_amp:
                with autocast('cuda'):
                    y_pred = model(x)
                    
                    # total loss
                    loss_taco, loss_gate, loss_atten, loss_emb = criterion(y_pred, y)
                    loss_guide = guide_loss(y_pred) if hparams.use_guided_attn else torch.tensor(0.0, device=y_pred[0].device)
                    loss_mmi = mmi_loss(y_pred[1], y[0]) if hparams.use_mmi else torch.tensor(0.0, device=y_pred[0].device)
                    loss = loss_taco + loss_gate + loss_atten + loss_guide + loss_mmi + loss_emb
            else:
                y_pred = model(x)
                # total loss
                loss_taco, loss_gate, loss_atten, loss_emb = criterion(y_pred, y)
                loss_guide = guide_loss(y_pred) if hparams.use_guided_attn else torch.tensor(0.0, device=y_pred[0].device)
                loss_mmi = mmi_loss(y_pred[1], y[0]) if hparams.use_mmi else torch.tensor(0.0, device=y_pred[0].device)
                loss = loss_taco + loss_gate + loss_atten + loss_guide + loss_mmi + loss_emb

            if hparams.distributed_run:
                reduced_loss = reduce_tensor(loss.data, n_gpus).item()
            else:
                reduced_loss = loss.item()
                reduced_taco_loss = loss_taco.item()
                reduced_atten_loss = loss_atten.item()
                reduced_mi_loss = loss_mmi.item()
                reduced_guide_loss = loss_guide.item() if hparams.use_guided_attn else 0.0
                reduced_gate_loss = loss_gate.item()
                reduced_emb_loss = loss_emb.item()

            # Backward pass
            if hparams.fp16_run and apex_available:
                with amp.scale_loss(loss, optimizer) as scaled_loss:
                    scaled_loss.backward()
            elif hparams.fp16_run and use_native_amp:
                scaler.scale(loss).backward()
            else:
                loss.backward()

            grad_norm = torch.nn.utils.clip_grad_norm_(
                model.parameters(), hparams.grad_clip_thresh)
            
            # Optimizer step —Å —É—á—ë—Ç–æ–º —Å—Ö–µ–º—ã mixed precision
            if hparams.fp16_run and use_native_amp:
                scaler.step(optimizer)
                scaler.update()
            else:
                optimizer.step()

            if is_main_node:
                duration = time.perf_counter() - start
                print("Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it".format(
                    iteration, reduced_loss, grad_norm, duration))
                
                # –û–±–Ω–æ–≤–ª—è–µ–º learning_rate –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ optimizer (–º–æ–≥ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –∞–≤—Ç–æ-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–º)
                learning_rate = optimizer.param_groups[0]['lr']

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard
                writer.add_scalar("training.loss", reduced_loss, iteration)
                writer.add_scalar("training.taco_loss", reduced_taco_loss, iteration)
                writer.add_scalar("training.atten_loss", reduced_atten_loss, iteration)
                writer.add_scalar("training.mi_loss", reduced_mi_loss, iteration)
                writer.add_scalar("training.guide_loss", reduced_guide_loss, iteration)
                writer.add_scalar("training.gate_loss", reduced_gate_loss, iteration)
                writer.add_scalar("training.emb_loss", reduced_emb_loss, iteration)
                writer.add_scalar("grad.norm", grad_norm, iteration)
                writer.add_scalar("learning.rate", learning_rate, iteration)
                writer.add_scalar("duration", duration, iteration)
                if hparams.use_guided_attn:
                     writer.add_scalar("training.guide_loss_weight", guide_loss.get_weight(), iteration)

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
                if MLFLOW_AVAILABLE:
                    training_metrics = {
                        "training.loss": reduced_loss,
                        "training.taco_loss": reduced_taco_loss,
                        "training.atten_loss": reduced_atten_loss,
                        "training.mi_loss": reduced_mi_loss,
                        "training.guide_loss": reduced_guide_loss,
                        "training.gate_loss": reduced_gate_loss,
                        "training.emb_loss": reduced_emb_loss,
                        "grad.norm": grad_norm,
                        "learning.rate": learning_rate,
                        "duration": duration,
                        "epoch": epoch,
                        "iteration": iteration
                    }
                    
                    if hparams.use_guided_attn:
                        training_metrics["training.guide_loss_weight"] = guide_loss.get_weight()
                    
                    if ENHANCED_LOGGING:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                        log_enhanced_training_metrics(training_metrics, iteration)
                        log_system_metrics(iteration)
                    else:
                        # –ë–∞–∑–æ–≤–æ–µ MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                        for metric_name, metric_value in training_metrics.items():
                            mlflow.log_metric(metric_name, metric_value, step=iteration)
                
                # üì± Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤ (—á–∞—â–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
                if telegram_monitor:
                    try:

                        
                        if iteration % 100 == 0:
                            print(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –¥–ª—è —à–∞–≥–∞ {iteration}")
                            
                            # –ü–æ–ª—É—á–∞–µ–º attention weights –∏–∑ y_pred
                            attention_weights = None
                            gate_outputs = None
                            
                            if len(y_pred) >= 5:
                                attention_weights = y_pred[4] if y_pred[4] is not None else None
                            if len(y_pred) >= 4:
                                gate_outputs = y_pred[3] if y_pred[3] is not None else None
                            
                            print(f"   - attention_weights: {attention_weights.shape if attention_weights is not None else 'None'}")
                            print(f"   - gate_outputs: {gate_outputs.shape if gate_outputs is not None else 'None'}")
                            
                            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Telegram
                            telegram_metrics = {
                                "loss": reduced_loss,
                                "mel_loss": reduced_taco_loss,
                                "gate_loss": reduced_gate_loss,
                                "guide_loss": reduced_guide_loss,
                                "grad_norm": grad_norm,
                                "learning_rate": learning_rate,
                                "epoch": epoch
                            }
                            
                            print(f"   - telegram_metrics: {telegram_metrics}")
                            
                            result = telegram_monitor.send_training_update(
                                step=iteration,
                                metrics=telegram_metrics,
                                attention_weights=attention_weights,
                                gate_outputs=gate_outputs
                            )
                            
                            print(f"üì± Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ {'–£–°–ü–ï–®–ù–û' if result else '–ù–ï'} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–ª—è —à–∞–≥–∞ {iteration}")
                        else:
                            print(f"   - –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à–∞–≥ {iteration} (–Ω–µ –∫—Ä–∞—Ç–µ–Ω 100)")
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
                        import traceback
                        print(f"   Traceback: {traceback.format_exc()}")

            if (iteration % hparams.validation_freq == 0):
                print(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}")
                val_loss = validate(model, criterion, valset, iteration, hparams.batch_size, n_gpus, collate_fn, writer, hparams.distributed_run, rank)
                print(f"üìä Validation loss: {val_loss}")
                # Auto hyper-parameter tuning (on main node)
                if is_main_node and auto_ctrl:
                    align_score = getattr(model, 'last_validation_alignment_score', None)
                    auto_ctrl.after_validation(iteration, val_loss, align_score)

            if is_main_node and (iteration % hparams.iters_per_checkpoint == 0):
                checkpoint_path = os.path.join(
                    output_directory, "checkpoint_{}".format(iteration))
                save_checkpoint(model, optimizer, learning_rate, iteration,
                                checkpoint_path)

            iteration += 1

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Smart Tuner
    if is_main_node:
        print(f"üèÅ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –ø–æ—Å–ª–µ {iteration} –∏—Ç–µ—Ä–∞—Ü–∏–π")
        val_loss = validate(model, criterion, valset, iteration, hparams.batch_size, n_gpus, collate_fn, writer, hparams.distributed_run, rank)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π checkpoint
        final_checkpoint_path = os.path.join(output_directory, f"checkpoint_final_{iteration}")
        save_checkpoint(model, optimizer, learning_rate, iteration, final_checkpoint_path)
        
        final_metrics = {
            "validation_loss": val_loss,
            "iteration": iteration,
            "checkpoint_path": final_checkpoint_path
        }
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}")
        
        if writer:
            writer.close()
        return final_metrics
    
    return None


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-o', '--output_directory', type=str,
                        help='directory to save checkpoints')
    parser.add_argument('-l', '--log_directory', type=str,
                        help='directory to save tensorboard logs')
    parser.add_argument('-c', '--checkpoint_path', type=str, default=None,
                        required=False, help='checkpoint path')
    parser.add_argument('--warm-start', action='store_true',
                        help='load model weights only, ignore specified layers')
    parser.add_argument('--ignore-mmi-layers', action='store_true',
                        help='load model weights only, ignore specified layers')
    parser.add_argument('--ignore-gst-layers', action='store_true',
                        help='load model weights only, ignore specified layers')
    parser.add_argument('--ignore-tsgst-layers', action='store_true',
                        help='load model weights only, ignore specified layers')
    parser.add_argument('--no-dga', action='store_true',
                        help='do not use DGA')
    parser.add_argument('--n_gpus', type=int, default=1,
                        required=False, help='number of gpus')
    parser.add_argument('--rank', type=int, default=0,
                        required=False, help='rank of current gpu')
    parser.add_argument('--group_name', type=str, default='group_name',
                        required=False, help='Distributed group name')
    parser.add_argument('--hparams', type=str,
                        required=False, help='comma separated name=value pairs')

    args = parser.parse_args()
    hparams = create_hparams(args.hparams)
    hparams.no_dga = True

    torch.backends.cudnn.enabled = hparams.cudnn_enabled
    torch.backends.cudnn.benchmark = hparams.cudnn_benchmark

    print("FP16 Run:", hparams.fp16_run)
    print("Dynamic Loss Scaling:", hparams.dynamic_loss_scaling)
    print("Use MMI:", hparams.use_mmi)
    print("Distributed Run:", hparams.distributed_run)
    print("cuDNN Enabled:", hparams.cudnn_enabled)
    print("cuDNN Benchmark:", hparams.cudnn_benchmark)
    
    train(args.output_directory, args.log_directory, args.checkpoint_path,
          args.warm_start, args.ignore_mmi_layers, args.ignore_gst_layers, args.ignore_tsgst_layers, args.n_gpus, args.rank, args.group_name, hparams)
