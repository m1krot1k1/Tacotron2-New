import os
import time
import argparse
import math
import random
import numpy as np
import copy

import torch
from distributed import apply_gradient_allreduce
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader

import gradient_adaptive_factor
from model import Tacotron2
from data_utils import TextMelLoader, TextMelCollate
from loss_function import Tacotron2Loss
from hparams import create_hparams
from plotting_utils import (
    plot_alignment_to_numpy,
    plot_spectrogram_to_numpy,
    plot_gate_outputs_to_numpy,
)
from text import symbol_to_id
from utils import to_gpu

# from logger import Tacotron2Logger  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
from smart_tuner.advanced_quality_controller import AdvancedQualityController
from smart_tuner.intelligent_epoch_optimizer import IntelligentEpochOptimizer
from smart_tuner.param_scheduler import ParamScheduler
from smart_tuner.early_stop_controller import EarlyStopController
from gradient_stability_monitor import GradientStabilityMonitor

# MLflow for experiment tracking
try:
    import mlflow
    import mlflow.pytorch  # –ø–æ–∑–≤–æ–ª—è–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ PyTorch

    MLFLOW_AVAILABLE = True

    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    try:
        from mlflow_metrics_enhancer import (
            log_enhanced_training_metrics,
            log_system_metrics,
        )

        ENHANCED_LOGGING = True
        print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–æ–µ MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–æ")
    except ImportError:
        ENHANCED_LOGGING = False
        # MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (—Å–∫—Ä—ã—Ç–æ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã –ª–æ–≥–æ–≤)

except ImportError:
    MLFLOW_AVAILABLE = False
    ENHANCED_LOGGING = False

# –ü–æ–¥–∞–≤–ª—è–µ–º –ª–∏—à–Ω–∏–µ warning'–∏
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings(
    "ignore", category=UserWarning, message=".*torch.cuda.*DtypeTensor.*"
)
warnings.filterwarnings("ignore", message=".*deprecated.*")


def reduce_tensor(tensor, n_gpus):
    rt = tensor.clone()
    dist.all_reduce(rt, op=dist.ReduceOp.SUM)
    rt /= n_gpus
    return rt


def init_distributed(hparams, n_gpus, rank, group_name):
    assert torch.cuda.is_available(), "Distributed mode requires CUDA."
    print("Initializing Distributed")

    # Set cuda device so everything is done on the right GPU.
    torch.cuda.set_device(rank % torch.cuda.device_count())

    # Initialize distributed communication
    dist.init_process_group(
        backend=hparams.dist_backend,
        init_method=hparams.dist_url,
        world_size=n_gpus,
        rank=rank,
        group_name=group_name,
    )

    print("Done initializing distributed")


def prepare_dataloaders(hparams):
    # Get data, data loaders and collate function ready
    trainset = TextMelLoader(hparams.training_files, hparams)
    valset = TextMelLoader(hparams.validation_files, hparams)
    collate_fn = TextMelCollate(hparams.n_frames_per_step)

    if hparams.distributed_run:
        train_sampler = DistributedSampler(trainset)
        shuffle = False
    else:
        train_sampler = None
        shuffle = True

    train_loader = DataLoader(
        trainset,
        num_workers=1,
        shuffle=shuffle,
        sampler=train_sampler,
        batch_size=hparams.batch_size,
        pin_memory=False,
        drop_last=True,
        collate_fn=collate_fn,
    )
    return train_loader, valset, collate_fn


def load_model(hparams):
    model = Tacotron2(hparams).cuda()
    # FP16 –±—É–¥–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ AMP, –º–æ–¥–µ–ª—å –æ—Å—Ç–∞–µ—Ç—Å—è –≤ FP32

    if hparams.distributed_run:
        model = apply_gradient_allreduce(model)

    return model


def warm_start_model(checkpoint_path, model, ignore_layers, exclude=None):
    assert os.path.isfile(checkpoint_path)
    print("Warm starting model from checkpoint '{}'".format(checkpoint_path))
    # –î–æ–±–∞–≤–ª—è–µ–º weights_only=False –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å PyTorch 2.6+
    checkpoint_dict = torch.load(
        checkpoint_path, map_location="cpu", weights_only=False
    )
    model_dict = checkpoint_dict["state_dict"]
    # model_dict['embedding.embedding.weight'] = model_dict['embedding.weight']
    # del model_dict['embedding.weight']
    print("ignoring layers:", ignore_layers)
    if len(ignore_layers) > 0 or exclude:
        model_dict = {
            k: v
            for k, v in model_dict.items()
            if k not in ignore_layers and (not exclude or exclude not in k)
        }

        dummy_dict = model.state_dict()
        dummy_dict.update(model_dict)
        model_dict = dummy_dict
    model.load_state_dict(model_dict, strict=False)
    return model


def load_checkpoint(checkpoint_path, model, optimizer):
    assert os.path.isfile(checkpoint_path)
    print("Loading checkpoint '{}'".format(checkpoint_path))
    # –î–æ–±–∞–≤–ª—è–µ–º weights_only=False –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å PyTorch 2.6+
    checkpoint_dict = torch.load(
        checkpoint_path, map_location="cpu", weights_only=False
    )
    model.load_state_dict(checkpoint_dict["state_dict"])
    optimizer.load_state_dict(checkpoint_dict["optimizer"])
    learning_rate = checkpoint_dict["learning_rate"]
    iteration = checkpoint_dict["iteration"]
    print("Loaded checkpoint '{}' from iteration {}".format(checkpoint_path, iteration))
    return model, optimizer, learning_rate, iteration


def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):
    print(
        "Saving model and optimizer state at iteration {} to {}".format(
            iteration, filepath
        )
    )
    torch.save(
        {
            "iteration": iteration,
            "state_dict": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "learning_rate": learning_rate,
        },
        filepath,
    )


def validate(
    model,
    criterion,
    valset,
    iteration,
    batch_size,
    n_gpus,
    collate_fn,
    writer,
    distributed_run=False,
    rank=1,
    minimize=False,
):
    """Handles all the validation scoring and printing"""
    model.eval()
    with torch.no_grad():
        val_sampler = DistributedSampler(valset) if distributed_run else None
        val_loader = DataLoader(
            valset,
            sampler=val_sampler,
            num_workers=1,
            shuffle=False,
            batch_size=batch_size,
            pin_memory=False,
            collate_fn=collate_fn,
        )
        model.decoder.p_teacher_forcing = 0.0
        val_loss = 0.0
        for i, batch in enumerate(val_loader):
            x, y = model.parse_batch(batch)
            y_pred = model(x, minimize)
            _loss = criterion(y_pred, y)
            loss = sum(_loss)
            if distributed_run:
                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()
            else:
                reduced_val_loss = loss.item()
            val_loss += reduced_val_loss
        val_loss = val_loss / (i + 1)

    # üî• –§–ò–ù–ê–õ–¨–ù–û–ï –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –≤ train —Ä–µ–∂–∏–º
    model.train()
    model.decoder.p_teacher_forcing = 1.0
    if rank == 0:
        print("Validation loss {}: {:9f}  ".format(iteration, val_loss))

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ writer
        writer.add_scalar("validation.loss", val_loss, iteration)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–≤–∑—è—Ç–æ –∏–∑ Tacotron2Logger)
        try:
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π inference —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏
            with torch.no_grad():
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º eval —Ä–µ–∂–∏–º –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è BatchNorm –æ—à–∏–±–æ–∫
                model.eval()

                # üî• –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ validation –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                # –í–º–µ—Å—Ç–æ inference –∏—Å–ø–æ–ª—å–∑—É–µ–º training forward pass –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ validation batch
                    validation_outputs = model(x, minimize=False)

                    # validation_outputs: [decoder_outputs, mel_outputs, mel_outputs_postnet, gate_outputs, alignments, ...]
                    if len(validation_outputs) >= 5:
                        (
                            decoder_outputs_val,
                            mel_outputs_val,
                            mel_outputs_postnet_val,
                            gate_outputs_val,
                            alignments_val,
                        ) = validation_outputs[:5]

                        # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º validation outputs (–æ–Ω–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞)
                        inference_outputs = [
                            None,
                            mel_outputs_val,
                            mel_outputs_postnet_val,
                            gate_outputs_val,
                            alignments_val,
                        ]
                        print(
                            f"‚úÖ Validation forward pass: mel={mel_outputs_postnet_val.shape if mel_outputs_postnet_val is not None else 'None'}, "
                            f"gate={gate_outputs_val.shape if gate_outputs_val is not None else 'None'}, "
                            f"align={alignments_val.shape if alignments_val is not None else 'None'}"
                        )
                    else:
                        print(
                            f"‚ö†Ô∏è Validation outputs –Ω–µ–ø–æ–ª–Ω—ã–µ: {len(validation_outputs)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤"
                        )
                        inference_outputs = None

                except Exception as val_e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ validation forward pass: {val_e}")

                    # Fallback –∫ inference —Å –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    try:
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
                        input_text = x[0][:1] if x[0].size(0) > 0 else x[0]

                        if input_text.size(0) == 0:
                            print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –±–∞—Ç—á –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                            inference_outputs = None
                        else:
                            inference_outputs = model.inference(input_text)
                            print(f"üìù Fallback inference –∑–∞–≤–µ—Ä—à–µ–Ω")
                    except Exception as inf_e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ fallback inference: {inf_e}")
                        inference_outputs = None

            # inference –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [None, mel_outputs, mel_outputs_postnet, gate_outputs, alignments, emb_gst]
            if inference_outputs is not None and len(inference_outputs) >= 5:
                (
                    _,
                    mel_outputs_inf,
                    mel_outputs_postnet_inf,
                    gate_outputs_inf,
                    alignments_inf,
                ) = inference_outputs[:5]
                mel_targets, gate_targets = y[0], y[1]

                print(f"üñºÔ∏è –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è TensorBoard (iteration {iteration})")

                # plot distribution of parameters (—Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 500 –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏)
                if iteration % 500 == 0:
                    for tag, value in model.named_parameters():
                        tag = tag.replace(".", "/")
                        writer.add_histogram(tag, value.data.cpu().numpy(), iteration)

                idx = 0  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –±–∞—Ç—á–∞

                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤

                # Alignment –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if alignments_inf is not None and alignments_inf.size(0) > idx:
                    try:
                        alignment_data = alignments_inf[idx].data.cpu().numpy()
                        if alignment_data.shape[0] > 1 and alignment_data.shape[1] > 1:
                            alignment_img = plot_alignment_to_numpy(alignment_data.T)
                            writer.add_image(
                                "alignment", alignment_img, iteration, dataformats="HWC"
                            )
                            print(
                                f"‚úÖ Alignment –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {alignment_img.shape}"
                            )
                        else:
                            print(
                                f"‚ö†Ô∏è Alignment –º–∞—Ç—Ä–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è: {alignment_data.shape}"
                            )
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è alignment –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

                # Mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if mel_targets.size(0) > idx:
                    try:
                        mel_target_data = mel_targets[idx].data.cpu().numpy()
                        if (
                            mel_target_data.shape[0] > 1
                            and mel_target_data.shape[1] > 1
                        ):
                            mel_target_img = plot_spectrogram_to_numpy(mel_target_data)
                            writer.add_image(
                                "mel_target",
                                mel_target_img,
                                iteration,
                                dataformats="HWC",
                            )
                            print(
                                f"‚úÖ Mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {mel_target_img.shape}"
                            )
                        else:
                            print(
                                f"‚ö†Ô∏è Mel target —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {mel_target_data.shape}"
                            )
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

                # Mel predicted –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if mel_outputs_inf is not None and mel_outputs_inf.size(0) > idx:
                    try:
                        mel_pred_data = mel_outputs_inf[idx].data.cpu().numpy()
                        if mel_pred_data.shape[0] > 1 and mel_pred_data.shape[1] > 1:
                            mel_pred_img = plot_spectrogram_to_numpy(mel_pred_data)
                            writer.add_image(
                                "mel_predicted",
                                mel_pred_img,
                                iteration,
                                dataformats="HWC",
                            )
                            print(
                                f"‚úÖ Mel predicted –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {mel_pred_img.shape}"
                            )
                        else:
                            print(
                                f"‚ö†Ô∏è Mel predicted —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {mel_pred_data.shape}"
                            )
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è mel predicted –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

                # Gate outputs –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if (
                    gate_outputs_inf is not None
                    and gate_outputs_inf.size(0) > idx
                    and gate_targets.size(0) > idx
                ):
                    try:
                        gate_target_data = gate_targets[idx].data.cpu().numpy()
                        gate_pred_data = (
                            torch.sigmoid(gate_outputs_inf[idx]).data.cpu().numpy()
                        )

                        if len(gate_target_data) > 1 and len(gate_pred_data) > 1:
                            gate_img = plot_gate_outputs_to_numpy(
                                gate_target_data, gate_pred_data
                            )
                            writer.add_image(
                                "gate", gate_img, iteration, dataformats="HWC"
                            )
                            print(f"‚úÖ Gate –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ: {gate_img.shape}")
                        else:
                            print(
                                f"‚ö†Ô∏è Gate –¥–∞–Ω–Ω—ã–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ: target={len(gate_target_data)}, pred={len(gate_pred_data)}"
                            )
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è gate –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")

                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ TensorBoard
                writer.flush()
                print(f"üîÑ TensorBoard –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–≤
                try:
                    model.last_validation_alignments = alignments_inf
                    model.last_validation_gate_outputs = gate_outputs_inf
                    model.last_validation_mel_outputs = mel_outputs_postnet_inf
                except Exception:
                    pass

            else:
                print(f"‚ö†Ô∏è Inference –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

            # üî• –í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–æ–¥–µ–ª—å –≤ train —Ä–µ–∂–∏–º
            model.train()

        except Exception as e:
            print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            # Fallback - —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            try:
                mel_targets, gate_targets = y[0], y[1]
                if mel_targets.size(0) > 0:
                    mel_target_img = plot_spectrogram_to_numpy(
                        mel_targets[0].data.cpu().numpy()
                    )
                    writer.add_image(
                        "mel_target_fallback",
                        mel_target_img,
                        iteration,
                        dataformats="HWC",
                    )
                    print(f"‚úÖ Fallback mel target –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ")
            except Exception as fallback_e:
                print(f"‚ùå –î–∞–∂–µ fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å: {fallback_e}")

        # üî• –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û: –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –≤ train —Ä–µ–∂–∏–º–µ
        model.train()

        if MLFLOW_AVAILABLE:
            validation_metrics = {
                "validation.loss": val_loss,
                "validation.step": iteration,
            }

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –º–æ–¥–µ–ª–∏
            if hasattr(model, "decoder") and hasattr(
                model.decoder, "attention_weights"
            ):
                try:
                    # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ attention
                    attention_weights = model.decoder.attention_weights
                    if attention_weights is not None:
                        validation_metrics["validation.attention_entropy"] = float(
                            torch.mean(
                                torch.sum(
                                    -attention_weights
                                    * torch.log(attention_weights + 1e-8),
                                    dim=-1,
                                )
                            )
                        )
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ attention entropy: {e}")

            # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ alignments
            if alignments_inf is not None:
                try:
                    # –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å alignment –º–∞—Ç—Ä–∏—Ü—ã
                    alignment_diag = torch.diagonal(alignments_inf[0], dim1=-2, dim2=-1)
                    align_score = float(torch.mean(alignment_diag))
                    validation_metrics["validation.alignment_score"] = align_score
                    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è AutoParamController
                    try:
                        model.last_validation_alignment_score = align_score
                    except Exception:
                        pass
                    # –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ attention (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏)
                    attention_focus = torch.max(alignments_inf[0], dim=-1)[0]
                    validation_metrics["validation.attention_focus"] = float(
                        torch.mean(attention_focus)
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ attention –º–µ—Ç—Ä–∏–∫: {e}")

            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –∞—Ç—Ä–∏–±—É—Ç –º–æ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –¥–∞–∂–µ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ align_score
            if not hasattr(model, "last_validation_alignment_score"):
                model.last_validation_alignment_score = None

            # –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ gate outputs
            if gate_outputs_inf is not None:
                try:
                    gate_probs = torch.sigmoid(gate_outputs_inf[0])
                    validation_metrics["validation.gate_mean"] = float(
                        torch.mean(gate_probs)
                    )
                    validation_metrics["validation.gate_std"] = float(
                        torch.std(gate_probs)
                    )
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ gate –º–µ—Ç—Ä–∏–∫: {e}")

            if ENHANCED_LOGGING:
                # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ validation –º–µ—Ç—Ä–∏–∫
                log_enhanced_training_metrics(validation_metrics, iteration)
            else:
                # –ë–∞–∑–æ–≤–æ–µ MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                for metric_name, metric_value in validation_metrics.items():
                    mlflow.log_metric(metric_name, metric_value, step=iteration)

    return val_loss


def calculate_global_mean(data_loader, global_mean_npy):
    if global_mean_npy and os.path.exists(global_mean_npy):
        global_mean = np.load(global_mean_npy)
        return to_gpu(torch.tensor(global_mean))
    sums = []
    frames = []
    print("\nCalculating global mean...\n")
    for i, batch in enumerate(data_loader):
        (
            text_padded,
            input_lengths,
            mel_padded,
            gate_padded,
            output_lengths,
            ctc_text,
            ctc_text_lengths,
            _guide_mask,
        ) = batch
        # padded values are 0.
        sums.append(mel_padded.double().sum(dim=(0, 2)))
        frames.append(output_lengths.double().sum())
    global_mean = sum(sums) / sum(frames)
    global_mean = global_mean.float()
    if global_mean_npy:
        np.save(global_mean_npy, global_mean)
    return to_gpu(global_mean)


def train(
    output_directory,
    log_directory,
    checkpoint_path,
    warm_start,
    ignore_mmi_layers,
    ignore_gst_layers,
    ignore_tsgst_layers,
    n_gpus,
    rank,
    group_name,
    hparams,
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å–æ Smart Tuner
    smart_tuner_trial=None,
    smart_tuner_logger=None,
    tensorboard_writer=None,
    telegram_monitor=None,
):
    """Training and validation logging results to tensorboard and stdout

    Params
    ------
    output_directory (string): directory to save checkpoints
    log_directory (string) directory to save tensorboard logs
    checkpoint_path(string): checkpoint path
    warm_start(bool): load model from checkpoint
    n_gpus (int): number of gpus
    rank (int): rank of current gpu
    hparams (object): comma separated list of "name=value" pairs.
    """
    if hparams.distributed_run:
        init_distributed(hparams, n_gpus, rank, group_name)

    torch.manual_seed(hparams.seed)
    torch.cuda.manual_seed(hparams.seed)
    random.seed(hparams.seed)
    np.random.seed(hparams.seed)

    model = load_model(hparams)
    learning_rate = hparams.learning_rate
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=hparams.weight_decay
    )

    # ---------- FP16 / Mixed Precision ---------
    apex_available = False  # –§–ª–∞–≥ –Ω–∞–ª–∏—á–∏—è NVIDIA Apex
    use_native_amp = False  # –§–ª–∞–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è torch.cuda.amp
    scaler = None  # GradScaler –¥–ª—è native AMP

    if hparams.fp16_run:
        try:
            from apex import amp

            model, optimizer = amp.initialize(model, optimizer, opt_level="O2")
            apex_available = True
            print("‚úÖ NVIDIA Apex —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è FP16 –æ–±—É—á–µ–Ω–∏—è")
        except ImportError:
            # Apex –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äì –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π AMP PyTorch
            try:
                from torch.amp import GradScaler, autocast

                # –û—Ç–∫–ª—é—á–∞–µ–º FP16 –¥–ª—è –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ AMP
                model = model.float()  # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –º–æ–¥–µ–ª—å –≤ FP32
                scaler = GradScaler("cuda")
                use_native_amp = True
                print(
                    "‚úÖ NVIDIA Apex –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ torch.amp (PyTorch Native AMP)"
                )
            except ImportError as e:
                # –î–∞–∂–µ native AMP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äì –æ—Ç–∫–ª—é—á–∞–µ–º FP16
                hparams.fp16_run = False
                print(f"‚ùå Mixed precision –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}. FP16 –æ—Ç–∫–ª—é—á—ë–Ω.")
    # -------------------------------------------

    if hparams.distributed_run:
        model = apply_gradient_allreduce(model)

    criterion = Tacotron2Loss(hparams)

    train_loader, valset, collate_fn = prepare_dataloaders(hparams)

    # Load checkpoint if one exists
    iteration = 0
    epoch_offset = 0
    if checkpoint_path is not None:
        if warm_start:
            model = warm_start_model(checkpoint_path, model, hparams.ignore_layers)
        else:
            model, optimizer, _learning_rate, iteration = load_checkpoint(
                checkpoint_path, model, optimizer
            )
            if hparams.use_saved_learning_rate:
                learning_rate = _learning_rate
            iteration += 1  # next iteration is iteration + 1
            epoch_offset = max(0, int(iteration / len(train_loader)))

    model.train()
    is_main_node = rank == 0

    # üí° –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π writer, –∞ –Ω–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
    writer = tensorboard_writer if is_main_node else None

    if is_main_node and writer is None:
        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –µ—Å–ª–∏ train.py –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é
        from torch.utils.tensorboard import SummaryWriter

        writer = SummaryWriter(log_directory)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è loss —Ñ—É–Ω–∫—Ü–∏–π
    mmi_loss = None
    guide_loss = None
    
    if hparams.use_mmi:
        from mmi_loss import MMI_loss

        mmi_loss = MMI_loss(hparams.mmi_map, hparams.mmi_weight)
        print("‚úÖ MMI loss –∑–∞–≥—Ä—É–∂–µ–Ω")

    if hparams.use_guided_attn:
        from loss_function import GuidedAttentionLoss

        guide_loss = GuidedAttentionLoss(alpha=hparams.guided_attn_weight)
        print("‚úÖ Guided Attention Loss –∑–∞–≥—Ä—É–∂–µ–Ω")

    # --- Auto Hyper-parameter Controller ---
    quality_ctrl = None
    if is_main_node:
        try:
            quality_ctrl = AdvancedQualityController()
            print("ü§ñ AdvancedQualityController –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å AdvancedQualityController: {e}")

    # --- ParamScheduler –∏ EarlyStopController ---
    sched_ctrl = None
    stop_ctrl = None
    if is_main_node:
        try:
            sched_ctrl = ParamScheduler()
            print("üìÖ ParamScheduler –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å ParamScheduler: {e}")

        try:
            stop_ctrl = EarlyStopController()
            print("üõë EarlyStopController –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å EarlyStopController: {e}")

    global_mean = calculate_global_mean(train_loader, hparams.global_mean_npy)

    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è validation loss –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ
    last_validation_loss = None
    last_audio_step = 0

    # ================ MAIN TRAINNIG LOOP ===================
    print(
        f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ: epochs={hparams.epochs}, batch_size={hparams.batch_size}, dataset_size={len(train_loader)}"
    )

    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤ MLflow
    model_params = {
        "model.total_params": sum(p.numel() for p in model.parameters()),
        "model.trainable_params": sum(
            p.numel() for p in model.parameters() if p.requires_grad
        ),
        "hparams.epochs": hparams.epochs,
        "hparams.grad_clip_thresh": hparams.grad_clip_thresh,
        "hparams.fp16_run": hparams.fp16_run,
        "hparams.use_mmi": hparams.use_mmi,
        "hparams.use_guided_attn": hparams.use_guided_attn,
        "dataset.train_size": len(train_loader),
        "dataset.val_size": len(valset),
    }

    # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∏–∑–º–µ–Ω—è–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    mlflow.log_params(model_params)

    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑–º–µ–Ω—è–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    # (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –µ—â–µ –Ω–µ –±—ã–ª–∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ —ç—Ç–æ–º run)
    try:
        mlflow.log_param("hparams.batch_size_init", hparams.batch_size)
        mlflow.log_param("hparams.learning_rate_init", hparams.learning_rate)
    except Exception as e:
        # –ï—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
        print(f"üìä –ù–∞—á–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã: {e}")

        print(
            f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow: {model_params['model.total_params']} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
        )

    # --- Intelligent Epoch Optimizer ---
    optimizer_epochs = None
    if is_main_node:
        try:
            optimizer_epochs = IntelligentEpochOptimizer()
            # –°–æ–∑–¥–∞–µ–º dataset_meta –∏–∑ –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            dataset_meta = {
                "total_duration_hours": len(train_loader)
                * hparams.batch_size
                * 0.1,  # –ø—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                "quality_metrics": {
                    "background_noise_level": 0.3,
                    "voice_consistency": 0.8,
                    "speech_clarity": 0.85,
                },
                "voice_features": {
                    "has_accent": False,
                    "emotional_range": "neutral",
                    "speaking_style": "normal",
                    "pitch_range_semitones": 12,
                },
            }
            analysis = optimizer_epochs.analyze_dataset(dataset_meta)
            if "recommended_epochs" in analysis:
                hparams.epochs = analysis["recommended_epochs"]
                print(f"üîß Epochs set to {hparams.epochs} (–±—ã–ª–æ {hparams.epochs})")
        except Exception as e:
            print(f"‚ö†Ô∏è IntelligentEpochOptimizer –æ—à–∏–±–∫–∞: {e}")

    gradient_monitor = GradientStabilityMonitor()
    restart_attempts = 0
    max_restart_attempts = 3
    safe_hparams_history = []
    
    def get_safe_hparams(hparams, attempt):
        """
        üõ°Ô∏è –£–õ–¨–¢–†–ê-–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ NaN/Inf
        –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        """
        new_hparams = copy.deepcopy(hparams)
        
        # üî• –†–ê–î–ò–ö–ê–õ–¨–ù–û–ï —Å–Ω–∏–∂–µ–Ω–∏–µ learning rate (–∫–∞–∂–¥–∞—è –ø–æ–ø—ã—Ç–∫–∞ –≤ 5 —Ä–∞–∑ –º–µ–Ω—å—à–µ)
        new_hparams.learning_rate = max(new_hparams.learning_rate * (0.2 ** (attempt + 1)), 1e-7)
        
        # üì¶ –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ batch size –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        new_hparams.batch_size = max(2, int(new_hparams.batch_size * (0.5 ** (attempt + 1))))
        
        # üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è alignment
        if hasattr(new_hparams, 'guide_loss_initial_weight'):
            new_hparams.guide_loss_initial_weight = min(100.0, max(5.0, new_hparams.guide_loss_initial_weight * (2.0 ** (attempt + 1))))
        else:
            new_hparams.guide_loss_initial_weight = 5.0 * (2.0 ** (attempt + 1))
        
        # ‚úÇÔ∏è –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å—Ç—Ä–æ–≥–æ–µ –∫–ª–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        new_hparams.grad_clip_thresh = max(0.01, new_hparams.grad_clip_thresh * (0.3 ** (attempt + 1)))
        
        # üö´ –û—Ç–∫–ª—é—á–∞–µ–º –≤—Å–µ "–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ" —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        if hasattr(new_hparams, 'use_mmi'):
            new_hparams.use_mmi = False
        if hasattr(new_hparams, 'use_audio_quality_enhancement'):
            new_hparams.use_audio_quality_enhancement = False
        
        # üõ°Ô∏è –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –∞–∫—Ç–∏–≤–∞—Ü–∏—è guided attention
        new_hparams.use_guided_attn = True
        
        # üéõÔ∏è –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º dropout –¥–ª—è –±–æ–ª—å—à–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏
        if hasattr(new_hparams, 'dropout_rate'):
            new_hparams.dropout_rate = min(0.005, new_hparams.dropout_rate * 0.1)
        if hasattr(new_hparams, 'encoder_dropout_rate'):
            new_hparams.encoder_dropout_rate = min(0.001, new_hparams.encoder_dropout_rate * 0.1)
        if hasattr(new_hparams, 'postnet_dropout_rate'):
            new_hparams.postnet_dropout_rate = min(0.001, new_hparams.postnet_dropout_rate * 0.1)
        if hasattr(new_hparams, 'p_attention_dropout'):
            new_hparams.p_attention_dropout = min(0.01, new_hparams.p_attention_dropout * 0.1)
        if hasattr(new_hparams, 'p_decoder_dropout'):
            new_hparams.p_decoder_dropout = min(0.01, new_hparams.p_decoder_dropout * 0.1)
        
        # üö™ –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π gate threshold
        if hasattr(new_hparams, 'gate_threshold'):
            new_hparams.gate_threshold = 0.4  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥
        
        # üìä –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        print(f"\nüõ°Ô∏è [–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï] –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}:")
        print(f"  üî• learning_rate: {hparams.learning_rate:.8f} ‚Üí {new_hparams.learning_rate:.8f} (—Å–Ω–∏–∂–µ–Ω–æ –≤ {hparams.learning_rate/new_hparams.learning_rate:.1f}x)")
        print(f"  üì¶ batch_size: {hparams.batch_size} ‚Üí {new_hparams.batch_size}")
        print(f"  üéØ guide_loss_weight: {getattr(hparams, 'guide_loss_initial_weight', 1.0):.2f} ‚Üí {new_hparams.guide_loss_initial_weight:.2f}")
        print(f"  ‚úÇÔ∏è grad_clip_thresh: {hparams.grad_clip_thresh:.4f} ‚Üí {new_hparams.grad_clip_thresh:.4f}")
        print(f"  üõ°Ô∏è use_guided_attn: –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –≤–∫–ª—é—á–µ–Ω")
        print(f"  üö´ –û—Ç–∫–ª—é—á–µ–Ω—ã: MMI, audio_enhancement")
        print(f"  üéõÔ∏è –ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≤—Å–µ dropout –∑–Ω–∞—á–µ–Ω–∏—è")
        
        return new_hparams

    while restart_attempts <= max_restart_attempts:
        for epoch in range(epoch_offset, hparams.epochs):
            print("Epoch: {} / {}".format(epoch, hparams.epochs))
            for i, batch in enumerate(train_loader):
                # üî• –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ó–ê–©–ò–¢–ê: –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –≤ train —Ä–µ–∂–∏–º–µ
                model.train()

                start = time.perf_counter()
                model.zero_grad()

                x, y = model.parse_batch(batch)

                # Forward pass —Å —É—á—ë—Ç–æ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Å—Ö–µ–º—ã mixed precision
                if hparams.fp16_run and use_native_amp:
                    with autocast("cuda"):
                        try:
                            y_pred = model(x)
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ forward pass –º–æ–¥–µ–ª–∏: {e}")
                            y_pred = None

                        # total loss
                        if y_pred is not None:
                            try:
                                loss_taco, loss_gate, loss_atten, loss_emb = criterion(y_pred, y)
                            except Exception as e:
                                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ criterion: {e}")
                                device = x.device
                                loss_taco = torch.tensor(0.0, device=device)
                                loss_gate = torch.tensor(0.0, device=device)
                                loss_atten = torch.tensor(0.0, device=device)
                                loss_emb = torch.tensor(0.0, device=device)
                        else:
                            # –ï—Å–ª–∏ y_pred None, —Å–æ–∑–¥–∞–µ–º –Ω—É–ª–µ–≤—ã–µ loss
                            device = x.device
                            loss_taco = torch.tensor(0.0, device=device)
                            loss_gate = torch.tensor(0.0, device=device)
                            loss_atten = torch.tensor(0.0, device=device)
                            loss_emb = torch.tensor(0.0, device=device)
                        
                        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ device
                        if y_pred is not None and len(y_pred) > 1 and y_pred[1] is not None:
                            device = y_pred[1].device  # mel_outputs –≤—Å–µ–≥–¥–∞ —Ç–µ–Ω–∑–æ—Ä
                        else:
                            device = x.device
                        try:
                            loss_guide = (
                                guide_loss(y_pred)
                                if hparams.use_guided_attn and guide_loss is not None and y_pred is not None
                                else torch.tensor(0.0, device=device)
                            )
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ guide_loss: {e}")
                            loss_guide = torch.tensor(0.0, device=device)
                        
                        try:
                            loss_mmi = (
                                mmi_loss(y_pred[1], y[0])
                                if hparams.use_mmi and mmi_loss is not None and y_pred is not None and y_pred[1] is not None
                                else torch.tensor(0.0, device=device)
                            )
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ mmi_loss: {e}")
                            loss_mmi = torch.tensor(0.0, device=device)
                        try:
                            loss = (
                                loss_taco
                                + loss_gate
                                + loss_atten
                                + loss_guide
                                + loss_mmi
                                + loss_emb
                            )
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss: {e}")
                            loss = torch.tensor(0.0, device=device)
                else:
                    try:
                        y_pred = model(x)
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ forward pass –º–æ–¥–µ–ª–∏: {e}")
                        y_pred = None
                    # total loss
                    if y_pred is not None:
                        try:
                            loss_taco, loss_gate, loss_atten, loss_emb = criterion(y_pred, y)
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ criterion: {e}")
                            device = x.device
                            loss_taco = torch.tensor(0.0, device=device)
                            loss_gate = torch.tensor(0.0, device=device)
                            loss_atten = torch.tensor(0.0, device=device)
                            loss_emb = torch.tensor(0.0, device=device)
                    else:
                        # –ï—Å–ª–∏ y_pred None, —Å–æ–∑–¥–∞–µ–º –Ω—É–ª–µ–≤—ã–µ loss
                        device = x.device
                        loss_taco = torch.tensor(0.0, device=device)
                        loss_gate = torch.tensor(0.0, device=device)
                        loss_atten = torch.tensor(0.0, device=device)
                        loss_emb = torch.tensor(0.0, device=device)
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ device
                    if y_pred is not None and len(y_pred) > 1 and y_pred[1] is not None:
                        device = y_pred[1].device  # mel_outputs –≤—Å–µ–≥–¥–∞ —Ç–µ–Ω–∑–æ—Ä
                    else:
                        device = x.device
                    try:
                        loss_guide = (
                            guide_loss(y_pred)
                            if hparams.use_guided_attn and guide_loss is not None and y_pred is not None
                            else torch.tensor(0.0, device=device)
                        )
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ guide_loss: {e}")
                        loss_guide = torch.tensor(0.0, device=device)
                    
                    try:
                        loss_mmi = (
                            mmi_loss(y_pred[1], y[0])
                            if hparams.use_mmi and mmi_loss is not None and y_pred is not None and y_pred[1] is not None
                            else torch.tensor(0.0, device=device)
                        )
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ mmi_loss: {e}")
                        loss_mmi = torch.tensor(0.0, device=device)
                    try:
                        loss = (
                            loss_taco
                            + loss_gate
                            + loss_atten
                            + loss_guide
                            + loss_mmi
                            + loss_emb
                        )
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss: {e}")
                        loss = torch.tensor(0.0, device=device)

                if hparams.distributed_run:
                    reduced_loss = reduce_tensor(loss.data, n_gpus).item() if loss is not None else 0.0
                else:
                    reduced_loss = loss.item() if loss is not None else 0.0
                    reduced_taco_loss = loss_taco.item() if loss_taco is not None else 0.0
                    reduced_atten_loss = loss_atten.item() if loss_atten is not None else 0.0
                    reduced_mi_loss = loss_mmi.item() if loss_mmi is not None else 0.0
                    reduced_guide_loss = (
                        loss_guide.item() if hparams.use_guided_attn and loss_guide is not None else 0.0
                    )
                    reduced_gate_loss = loss_gate.item() if loss_gate is not None else 0.0
                    reduced_emb_loss = loss_emb.item() if loss_emb is not None else 0.0

                # Backward pass
                if hparams.fp16_run and apex_available:
                    if loss is not None:
                        with amp.scale_loss(loss, optimizer) as scaled_loss:
                            scaled_loss.backward()
                    else:
                        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: loss is None, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º backward pass")
                elif hparams.fp16_run and use_native_amp:
                    if loss is not None:
                        scaler.scale(loss).backward()
                    else:
                        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: loss is None, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º backward pass")
                else:
                    if loss is not None:
                        loss.backward()
                    else:
                        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: loss is None, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º backward pass")

                if loss is not None:
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        model.parameters(), hparams.grad_clip_thresh
                    )
                else:
                    grad_norm = 0.0

                # Optimizer step —Å —É—á—ë—Ç–æ–º —Å—Ö–µ–º—ã mixed precision
                if loss is not None:
                    if hparams.fp16_run and use_native_amp:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                else:
                    print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: loss is None, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º optimizer step")

                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                if loss is not None:
                    monitor_result = gradient_monitor.check_gradient_stability(model, loss, i + epoch * len(train_loader))
                else:
                    monitor_result = {'explosion_detected': False, 'nan_detected': False, 'recommendations': ['Loss is None']}
                if monitor_result['explosion_detected'] or monitor_result['nan_detected']:
                    print("\nüö® [Smart Restart] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ —à–∞–≥–µ {}!".format(i + epoch * len(train_loader)))
                    print("–ü—Ä–∏—á–∏–Ω–∞: {}".format('–í–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤' if monitor_result['explosion_detected'] else 'NaN/Inf –≤ loss'))
                    print("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                    for rec in monitor_result['recommendations']:
                        print("  - ", rec)
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —à–∞–≥
                    safe_hparams_history.append(copy.deepcopy(hparams))
                    # –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å –±–æ–ª–µ–µ –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    restart_attempts += 1
                    if restart_attempts > max_restart_attempts:
                        print("‚ùå –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞. –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è.")
                        return
                    hparams = get_safe_hparams(hparams, restart_attempts)
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    save_checkpoint(model, optimizer, hparams.learning_rate, i + epoch * len(train_loader), os.path.join(output_directory, f"restart_checkpoint_{restart_attempts}.pt"))
                    print(f"[Smart Restart] –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–ø–æ–ø—ã—Ç–∫–∞ {restart_attempts})...\n")
                    # break –æ–±–∞ —Ü–∏–∫–ª–∞
                    break

                # üõ°Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ó–ê–©–ò–¢–ê –û–¢ NaN LOSS
                if loss is not None:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã loss –Ω–∞ NaN/Inf
                    loss_components = {
                        'total_loss': reduced_loss,
                        'taco_loss': reduced_taco_loss,
                        'atten_loss': reduced_atten_loss,
                        'mi_loss': reduced_mi_loss,
                        'guide_loss': reduced_guide_loss,
                        'gate_loss': reduced_gate_loss,
                        'emb_loss': reduced_emb_loss
                    }
                    
                    nan_detected = False
                    problematic_components = []
                    
                    for component_name, component_value in loss_components.items():
                        if torch.isnan(torch.tensor(component_value)) or torch.isinf(torch.tensor(component_value)):
                            nan_detected = True
                            problematic_components.append(f"{component_name}: {component_value}")
                    
                    if nan_detected:
                        print("\nüö® [–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê] NaN/Inf –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ loss!")
                        print(f"–ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {problematic_components}")
                        print(f"–®–∞–≥: {i + epoch * len(train_loader)}")
                        
                        # üì± –°–†–û–ß–ù–û–ï Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ
                        if telegram_monitor:
                            try:
                                critical_message = f"üö® **–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê!**\n\n"
                                critical_message += f"‚ùå **Loss —Å—Ç–∞–ª NaN –Ω–∞ —à–∞–≥–µ {iteration}**\n"
                                critical_message += f"üî• **–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å –æ—à–∏–±–∫–∞–º–∏:**\n"
                                for comp in problematic_components:
                                    critical_message += f"  ‚Ä¢ {comp}\n"
                                critical_message += f"\nüîÑ **–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–ï–†–ï–ó–ê–ü–£–°–ö –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ...**\n"
                                critical_message += f"‚öôÔ∏è –°–Ω–∏–∂–∞–µ–º learning_rate –∏ —É–∫—Ä–µ–ø–ª—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å"
                                
                                telegram_monitor._send_text_message(critical_message)
                                print("üì± –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram")
                            except Exception as e:
                                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
                        
                        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç—Ç–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
                        print("üîÑ –ò–Ω–∏—Ü–∏–∏—Ä—É—é —É–º–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å –∑–∞—â–∏—â–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
                        restart_attempts += 1
                        if restart_attempts > max_restart_attempts:
                            print("‚ùå –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞. –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è.")
                            return
                        
                        # üõ°Ô∏è –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º guided attention
                        if guide_loss is not None and hasattr(guide_loss, 'activate_critical_mode'):
                            guide_loss.activate_critical_mode()
                            print("üéØ Guided Attention –ø–µ—Ä–µ–≤–µ–¥–µ–Ω –≤ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô —Ä–µ–∂–∏–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
                        old_learning_rate = hparams.learning_rate
                        old_batch_size = hparams.batch_size
                        
                        # –°–æ–∑–¥–∞–µ–º –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                        hparams = get_safe_hparams(hparams, restart_attempts)
                        print(f"[Smart Restart] –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å –£–õ–¨–¢–†–ê-–±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–ø–æ–ø—ã—Ç–∫–∞ {restart_attempts})...\n")
                        
                        # üì± TELEGRAM —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ
                        if telegram_monitor:
                            try:
                                restart_message = f"üîÑ **–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–ï–†–ï–ó–ê–ü–£–°–ö #{restart_attempts}**\n\n"
                                restart_message += f"üö® **–ü—Ä–∏—á–∏–Ω–∞:** NaN/Inf –≤ loss components\n"
                                restart_message += f"üõ°Ô∏è **–î–µ–π—Å—Ç–≤–∏—è:**\n"
                                restart_message += f"  ‚Ä¢ üî• Learning rate: {old_learning_rate:.8f} ‚Üí {hparams.learning_rate:.8f}\n"
                                restart_message += f"  ‚Ä¢ üì¶ Batch size: {old_batch_size} ‚Üí {hparams.batch_size}\n"
                                restart_message += f"  ‚Ä¢ üéØ Guided attention —É—Å–∏–ª–µ–Ω –¥–æ {getattr(hparams, 'guide_loss_initial_weight', '–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω')}\n"
                                restart_message += f"  ‚Ä¢ ‚úÇÔ∏è Grad clipping: {hparams.grad_clip_thresh:.4f} (—Å—Ç—Ä–æ–∂–µ)\n"
                                restart_message += f"  ‚Ä¢ üõ°Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º guided attention –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω\n"
                                restart_message += f"\n‚è∞ **–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 3 —Å–µ–∫—É–Ω–¥—ã...**"
                                
                                telegram_monitor._send_text_message(restart_message)
                                print("üì± –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Telegram")
                            except Exception as e:
                                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ: {e}")
                        
                        # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
                        import time
                        time.sleep(3)
                        break

                if is_main_node:
                    try:
                        duration = time.perf_counter() - start
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è duration: {e}")
                        duration = 0.0
                    try:
                        print(
                            "Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it".format(
                                iteration, reduced_loss, grad_norm, duration
                            )
                        )
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–≤–æ–¥–∞ –º–µ—Ç—Ä–∏–∫: {e}")
                        print(f"Train loss {iteration} N/A Grad Norm N/A N/A s/it")

                    # –û–±–Ω–æ–≤–ª—è–µ–º learning_rate –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ optimizer (–º–æ–≥ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –∞–≤—Ç–æ-–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–º)
                    try:
                        learning_rate = optimizer.param_groups[0]["lr"]
                    except (IndexError, KeyError) as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è learning_rate: {e}")
                        learning_rate = hparams.learning_rate

                    # --- EarlyStopController: –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ ---
                    if stop_ctrl:
                        try:
                            stop_ctrl.add_metrics(
                                {
                                    "train_loss": reduced_loss,
                                    "grad_norm": grad_norm,
                                    "learning_rate": learning_rate,
                                    "guide_loss": (
                                        reduced_guide_loss
                                        if hparams.use_guided_attn and reduced_guide_loss is not None
                                        else 0.0
                                    ),
                                    "gate_loss": reduced_gate_loss,
                                }
                            )
                        except Exception as e:
                            print(f"‚ö†Ô∏è EarlyStopController add_metrics –æ—à–∏–±–∫–∞: {e}")

                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard
                    try:
                        writer.add_scalar("training.loss", reduced_loss, iteration)
                        writer.add_scalar("training.taco_loss", reduced_taco_loss, iteration)
                        writer.add_scalar("training.atten_loss", reduced_atten_loss, iteration)
                        writer.add_scalar("training.mi_loss", reduced_mi_loss, iteration)
                        writer.add_scalar("training.guide_loss", reduced_guide_loss if reduced_guide_loss is not None else 0.0, iteration)
                        writer.add_scalar("training.gate_loss", reduced_gate_loss, iteration)
                        writer.add_scalar("training.emb_loss", reduced_emb_loss, iteration)
                        writer.add_scalar("grad.norm", grad_norm, iteration)
                        writer.add_scalar("learning.rate", learning_rate, iteration)
                        writer.add_scalar("duration", duration, iteration)
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ TensorBoard: {e}")
                    if hparams.use_guided_attn and guide_loss is not None:
                        try:
                            writer.add_scalar(
                                "training.guide_loss_weight", guide_loss.get_weight(), iteration
                            )
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è guide_loss weight: {e}")

                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
                    if MLFLOW_AVAILABLE:
                        training_metrics = {
                            "training.loss": reduced_loss,
                            "training.taco_loss": reduced_taco_loss,
                            "training.atten_loss": reduced_atten_loss,
                            "training.mi_loss": reduced_mi_loss,
                            "training.guide_loss": reduced_guide_loss if reduced_guide_loss is not None else 0.0,
                            "training.gate_loss": reduced_gate_loss,
                            "training.emb_loss": reduced_emb_loss,
                            "grad.norm": grad_norm,
                            "learning.rate": learning_rate,
                            "duration": duration,
                            "batch_size": hparams.batch_size,
                            "learning_rate": learning_rate,
                        }
                        for metric_name, metric_value in training_metrics.items():
                            try:
                                mlflow.log_metric(metric_name, metric_value, step=iteration)
                            except Exception as e:
                                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLflow –¥–ª—è {metric_name}: {e}")

                    # üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è guided attention
                    if guide_loss is not None and hasattr(guide_loss, 'check_diagonality_and_adapt') and y_pred is not None:
                        try:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º alignments –∏–∑ y_pred
                            alignments = None
                            if len(y_pred) >= 4:
                                alignments = y_pred[3] if len(y_pred) == 4 else y_pred[4]  # Alignments –æ–±—ã—á–Ω–æ 4-–π —ç–ª–µ–º–µ–Ω—Ç
                            
                            if alignments is not None:
                                guide_loss.check_diagonality_and_adapt(alignments)
                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")

                    # üì± Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
                    if is_main_node and telegram_monitor:
                        try:
                            if iteration % 100 == 0:
                                print(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –¥–ª—è —à–∞–≥–∞ {iteration}")

                                # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Telegram
                                telegram_metrics = {
                                    "loss": reduced_loss,
                                    "train_loss": reduced_loss,  # –î—É–±–ª–∏—Ä—É–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                                    "mel_loss": reduced_taco_loss,
                                    "gate_loss": reduced_gate_loss,
                                    "guide_loss": reduced_guide_loss if reduced_guide_loss is not None else 0.0,
                                    "grad_norm": grad_norm,
                                    "learning_rate": learning_rate,
                                    "epoch": epoch,
                                    "batch_size": hparams.batch_size,
                                    "guide_loss_weight": hparams.guide_loss_weight if hasattr(hparams, 'guide_loss_weight') and hparams.guide_loss_weight is not None else 1.0,
                                    "gate_threshold": hparams.gate_threshold if hasattr(hparams, 'gate_threshold') and hparams.gate_threshold is not None else 0.5,
                                }
                                # –î–æ–±–∞–≤–ª—è–µ–º validation loss –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
                                if last_validation_loss is not None:
                                    telegram_metrics["validation_loss"] = last_validation_loss
                                    telegram_metrics["val_loss"] = last_validation_loss

                                # ü§ñ –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ—à–µ–Ω–∏—è—Ö Smart Tuner
                                smart_tuner_decisions = {}

                                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç AdvancedQualityController
                                if quality_ctrl:
                                    try:
                                        quality_summary = quality_ctrl.get_quality_summary()
                                        if quality_summary:
                                            smart_tuner_decisions["quality_controller"] = {
                                                "active": True,
                                                "status": "–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞",
                                                "summary": quality_summary,
                                            }
                                    except Exception as e:
                                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è quality summary: {e}")

                                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç ParamScheduler
                                if sched_ctrl:
                                    try:
                                        sched_status = sched_ctrl.get_status()
                                        if sched_status:
                                            smart_tuner_decisions["param_scheduler"] = {
                                                "active": True,
                                                "status": sched_status.get(
                                                    "phase", "–ê–∫—Ç–∏–≤–µ–Ω"
                                                ),
                                                "current_params": sched_status.get(
                                                    "current_params", {}
                                                ),
                                            }
                                    except Exception as e:
                                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è scheduler status: {e}")

                                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç EarlyStopController
                                if stop_ctrl:
                                    try:
                                        stop_status = stop_ctrl.get_status()
                                        if stop_status:
                                            smart_tuner_decisions[
                                                "early_stop_controller"
                                            ] = {
                                                "active": True,
                                                "status": stop_status.get(
                                                    "status", "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"
                                                ),
                                                "patience_remaining": stop_status.get(
                                                    "patience_remaining", "N/A"
                                                ),
                                            }
                                    except Exception as e:
                                        print(
                                            f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è stop controller status: {e}"
                                        )

                                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç IntelligentEpochOptimizer
                                if optimizer_epochs:
                                    try:
                                        epoch_status = optimizer_epochs.get_status()
                                        if epoch_status:
                                            smart_tuner_decisions["epoch_optimizer"] = {
                                                "active": True,
                                                "status": epoch_status.get(
                                                    "status", "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
                                                ),
                                                "recommended_epochs": epoch_status.get(
                                                    "recommended_epochs", "N/A"
                                                ),
                                            }
                                    except Exception as e:
                                        print(
                                            f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è epoch optimizer status: {e}"
                                        )

                                # –°–æ–±–∏—Ä–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                                param_changes = {}
                                if hasattr(model, "last_param_changes"):
                                    param_changes = model.last_param_changes

                                if param_changes:
                                    smart_tuner_decisions["parameter_changes"] = (
                                        param_changes
                                    )

                                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç –≤—Å–µ—Ö –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–≤
                                all_recommendations = []
                                if quality_ctrl and hasattr(
                                    quality_ctrl, "get_recommendations"
                                ):
                                    try:
                                        quality_recs = quality_ctrl.get_recommendations()
                                        all_recommendations.extend(quality_recs)
                                    except Exception:
                                        pass

                                if sched_ctrl and hasattr(
                                    sched_ctrl, "get_recommendations"
                                ):
                                    try:
                                        sched_recs = sched_ctrl.get_recommendations()
                                        all_recommendations.extend(sched_recs)
                                    except Exception:
                                        pass

                                if all_recommendations:
                                    smart_tuner_decisions["recommendations"] = (
                                        all_recommendations[:3]
                                    )  # –î–æ 3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

                                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
                                warnings = []
                                if reduced_loss > 5.0:
                                    warnings.append(
                                        "–í—ã—Å–æ–∫–∏–π loss - –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º"
                                    )
                                if grad_norm > 10.0:
                                    warnings.append(
                                        "–í—ã—Å–æ–∫–∏–π grad_norm - –≤–æ–∑–º–æ–∂–µ–Ω –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"
                                    )
                                if learning_rate > 0.01:
                                    warnings.append(
                                        "–í—ã—Å–æ–∫–∏–π learning rate - –≤–æ–∑–º–æ–∂–Ω–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å"
                                    )

                                if warnings:
                                    smart_tuner_decisions["warnings"] = warnings

                                print(f"   - smart_tuner_decisions: {smart_tuner_decisions}")

                                # –ì–ê–†–ê–ù–¢–ò–†–û–í–ê–ù–ù–û –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ (send_plots=True)
                                try:
                                    result = telegram_monitor.send_training_update(
                                        step=iteration,
                                        metrics=telegram_metrics,
                                        smart_tuner_decisions=smart_tuner_decisions,
                                        send_plots=True,
                                        send_detailed=True
                                    )
                                except Exception as e:
                                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
                                    result = False
                                print(f"üì± Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ {'–£–°–ü–ï–®–ù–û' if result else '–ù–ï'} –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–ª—è —à–∞–≥–∞ {iteration}")

                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
                            import traceback
                            print(f"   Traceback: {traceback.format_exc()}")

                    # --- –û—Ç–ø—Ä–∞–≤–∫–∞ –∞—É–¥–∏–æ –∫–∞–∂–¥—ã–µ 500 —à–∞–≥–æ–≤ ---
                    if is_main_node and telegram_monitor:
                        try:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞—É–¥–∏–æ (—Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 500 —à–∞–≥–æ–≤)
                            if iteration % 500 == 0 and iteration != 0:
                                print(f"üéµ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –∞—É–¥–∏–æ –¥–ª—è —à–∞–≥–∞ {iteration}")

                                # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å —É–¥–∞—Ä–µ–Ω–∏—è–º–∏ –¥–ª—è Tacotron2
                                test_text = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? –°–µ–≥–æ–¥–Ω—è –ø—Ä–µ–∫—Ä–∞—Å–Ω–∞—è –ø–æ–≥–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≥—É–ª–∫–∏ –≤ –ø–∞—Ä–∫–µ."

                                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ
                                try:
                                    audio_result = (
                                        telegram_monitor.generate_and_send_test_audio(
                                            model=model, step=iteration, test_text=test_text
                                        )
                                    )
                                except Exception as e:
                                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞—É–¥–∏–æ: {e}")
                                    audio_result = False

                                if audio_result:
                                    print(
                                        f"‚úÖ –ê—É–¥–∏–æ—Ñ–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –¥–ª—è —à–∞–≥–∞ {iteration}"
                                    )
                                    last_audio_step = iteration
                                else:
                                    print(
                                        f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª—ã –¥–ª—è —à–∞–≥–∞ {iteration}"
                                    )

                        except Exception as e:
                            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ: {e}")
                            import traceback
                            print(f"   Traceback: {traceback.format_exc()}")

                if iteration % hparams.validation_freq == 0:
                    print(f"üîç –í—ã–ø–æ–ª–Ω—è–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}")
                    try:
                        val_loss = validate(
                            model,
                            criterion,
                            valset,
                            iteration,
                            hparams.batch_size,
                            n_gpus,
                            collate_fn,
                            writer,
                            hparams.distributed_run,
                            rank,
                        )
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                        val_loss = float('inf')
                    print(f"üìä Validation loss: {val_loss}")

                    # Auto hyper-parameter tuning (on main node)
                    if is_main_node and quality_ctrl:
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                        align_score = getattr(
                            model, "last_validation_alignment_score", None
                        )
                        metrics_dict = {
                            "val_loss": val_loss,
                            "attention_alignment_score": (
                                align_score if align_score is not None else 0.0
                            ),
                        }
                        attention_w = getattr(model, "last_validation_alignments", None)
                        gate_out = getattr(model, "last_validation_gate_outputs", None)
                        mel_out = getattr(model, "last_validation_mel_outputs", None)
                        try:
                            analysis = quality_ctrl.analyze_training_quality(
                                epoch=iteration,
                                metrics=metrics_dict,
                                attention_weights=attention_w,
                                gate_outputs=gate_out,
                                mel_outputs=mel_out,
                            )
                            for intrv in analysis.get("recommended_interventions", []):
                                new_hp = quality_ctrl.apply_quality_intervention(
                                    intrv, vars(hparams), step=iteration
                                )
                                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫ –æ–±—ä–µ–∫—Ç—É hparams –∏ –º–æ–¥–µ–ª–∏
                                for k, v in new_hp.items():
                                    if hasattr(hparams, k):
                                        old_value = getattr(hparams, k)
                                        setattr(hparams, k, v)
                                        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
                                        quality_ctrl.track_parameter_change(
                                            k,
                                            old_value,
                                            v,
                                            f"Quality intervention: {intrv.get('type', 'unknown')}",
                                            iteration,
                                        )
                                    if (
                                        k in ["guide_loss_weight", "guided_attn_weight"]
                                        and hparams.use_guided_attn
                                    ):
                                        guide_loss.alpha = v
                                        guide_loss.current_weight = v
                                    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –æ–±–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏—è –≤ hparams
                                    setattr(hparams, "guided_attn_weight", v)
                                    setattr(hparams, "guide_loss_weight", v)
                                    if k == "learning_rate":
                                        for g in optimizer.param_groups:
                                            g["lr"] = v
                                    if k == "gate_threshold" and hasattr(
                                        model.decoder, "gate_threshold"
                                    ):
                                        model.decoder.gate_threshold = v

                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏ –¥–ª—è Telegram
                                if not hasattr(model, "last_param_changes"):
                                    model.last_param_changes = {}

                                for k, v in new_hp.items():
                                    if hasattr(hparams, k):
                                        old_value = getattr(hparams, k)
                                        model.last_param_changes[k] = {
                                            "old_value": old_value,
                                            "new_value": v,
                                            "reason": f"Quality intervention: {intrv.get('type', 'unknown')}",
                                        }
                        except Exception as e:
                            print(f"‚ö†Ô∏è AdvancedQualityController –æ—à–∏–±–∫–∞: {e}")

                    # --- EarlyStopController: –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ—à–µ–Ω–∏—è ---
                    if is_main_node and stop_ctrl:
                        try:
                            # –î–æ–±–∞–≤–ª—è–µ–º validation –º–µ—Ç—Ä–∏–∫–∏
                            align_score = getattr(
                                model, "last_validation_alignment_score", None
                            )
                            stop_ctrl.add_metrics(
                                {
                                    "val_loss": val_loss,
                                    "attention_alignment_score": (
                                        align_score if align_score is not None else 0.0
                                    ),
                                }
                            )

                            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
                            decision = stop_ctrl.decide_next_step(vars(hparams))
                            if decision["action"] == "apply_patch":
                                hparams_dict = decision["new_hparams"]
                                for k, v in hparams_dict.items():
                                    if hasattr(hparams, k):
                                        setattr(hparams, k, v)
                                    if k == "learning_rate":
                                        for g in optimizer.param_groups:
                                            g["lr"] = v
                                    if (
                                        k in ["guide_loss_weight", "guided_attn_weight"]
                                        and hparams.use_guided_attn
                                    ):
                                        guide_loss.alpha = v
                                        guide_loss.current_weight = v
                                print(
                                    f"üõ†  EarlyStop/Rescue –ø—Ä–∏–º–µ–Ω–∏–ª –ø–∞—Ç—á: {decision['reason']}"
                                )

                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–Ω–Ω–∏–π –æ—Å—Ç–∞–Ω–æ–≤
                            should_stop, reason = stop_ctrl.should_stop_early(
                                {"val_loss": val_loss}
                            )
                            if should_stop:
                                print(f"üü• –†–∞–Ω–Ω–∏–π –æ—Å—Ç–∞–Ω–æ–≤: {reason}")
                                return {
                                    "validation_loss": val_loss,
                                    "iteration": iteration,
                                    "checkpoint_path": None,
                                    "early_stop_reason": reason,
                                }
                        except Exception as e:
                            print(f"‚ö†Ô∏è EarlyStopController –æ—à–∏–±–∫–∞: {e}")

                if is_main_node and (iteration % hparams.iters_per_checkpoint == 0):
                    checkpoint_path = os.path.join(
                        output_directory, "checkpoint_{}".format(iteration)
                    )
                    try:
                        save_checkpoint(
                            model, optimizer, learning_rate, iteration, checkpoint_path
                        )
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")

                iteration += 1
            else:
                continue
            break
        else:
            # –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å –±–µ–∑ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
            break

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Smart Tuner
    if is_main_node:
        print(f"üèÅ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –ø–æ—Å–ª–µ {iteration} –∏—Ç–µ—Ä–∞—Ü–∏–π")
        val_loss = validate(
            model,
            criterion,
            valset,
            iteration,
            hparams.batch_size,
            n_gpus,
            collate_fn,
            writer,
            hparams.distributed_run,
            rank,
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π checkpoint
        final_checkpoint_path = os.path.join(
            output_directory, f"checkpoint_final_{iteration}"
        )
        save_checkpoint(
            model, optimizer, learning_rate, iteration, final_checkpoint_path
        )

        final_metrics = {
            "validation_loss": val_loss,
            "iteration": iteration,
            "checkpoint_path": final_checkpoint_path,
        }
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}")

        # --- –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–≤–æ–¥–∫–∏ –æ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–≤ ---
        if is_main_node:
            try:
                if quality_ctrl:
                    quality_summary = quality_ctrl.get_quality_summary()
                    print(f"üéØ Quality Summary: {quality_summary}")

                if stop_ctrl:
                    stop_summary = stop_ctrl.get_tts_training_summary()
                    print(f"üõë EarlyStop Summary: {stop_summary}")

                if optimizer_epochs:
                    epoch_summary = optimizer_epochs.get_optimization_summary()
                    print(f"üìà Epoch Optimization Summary: {epoch_summary}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–≤–æ–¥–æ–∫: {e}")

        if writer:
            writer.close()
        return final_metrics

    return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-o", "--output_directory", type=str, help="directory to save checkpoints"
    )
    parser.add_argument(
        "-l", "--log_directory", type=str, help="directory to save tensorboard logs"
    )
    parser.add_argument(
        "-c",
        "--checkpoint_path",
        type=str,
        default=None,
        required=False,
        help="checkpoint path",
    )
    parser.add_argument(
        "--warm-start",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument(
        "--ignore-mmi-layers",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument(
        "--ignore-gst-layers",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument(
        "--ignore-tsgst-layers",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument("--no-dga", action="store_true", help="do not use DGA")
    parser.add_argument(
        "--n_gpus", type=int, default=1, required=False, help="number of gpus"
    )
    parser.add_argument(
        "--rank", type=int, default=0, required=False, help="rank of current gpu"
    )
    parser.add_argument(
        "--group_name",
        type=str,
        default="group_name",
        required=False,
        help="Distributed group name",
    )
    parser.add_argument(
        "--hparams", type=str, required=False, help="comma separated name=value pairs"
    )

    args = parser.parse_args()
    hparams = create_hparams(args.hparams)
    hparams.no_dga = True

    torch.backends.cudnn.enabled = hparams.cudnn_enabled
    torch.backends.cudnn.benchmark = hparams.cudnn_benchmark

    print("FP16 Run:", hparams.fp16_run)
    print("Dynamic Loss Scaling:", hparams.dynamic_loss_scaling)
    print("Use MMI:", hparams.use_mmi)
    print("Distributed Run:", hparams.distributed_run)
    print("cuDNN Enabled:", hparams.cudnn_enabled)
    print("cuDNN Benchmark:", hparams.cudnn_benchmark)

    train(
        args.output_directory,
        args.log_directory,
        args.checkpoint_path,
        args.warm_start,
        args.ignore_mmi_layers,
        args.ignore_gst_layers,
        args.ignore_tsgst_layers,
        args.n_gpus,
        args.rank,
        args.group_name,
        hparams,
    )
