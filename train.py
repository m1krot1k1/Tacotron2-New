import os
import time
import argparse
import math
import random
import numpy as np
import copy

import torch
from distributed import apply_gradient_allreduce
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader

import gradient_adaptive_factor
from model import Tacotron2
from data_utils import TextMelLoader, TextMelCollate
from loss_function import Tacotron2Loss
from hparams import create_hparams
from plotting_utils import (
    plot_alignment_to_numpy,
    plot_spectrogram_to_numpy,
    plot_gate_outputs_to_numpy,
)
from text import symbol_to_id
from utils import to_gpu

# from logger import Tacotron2Logger  # ĞĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ
from smart_tuner.advanced_quality_controller import AdvancedQualityController
from smart_tuner.intelligent_epoch_optimizer import IntelligentEpochOptimizer
from smart_tuner.param_scheduler import ParamScheduler
from smart_tuner.early_stop_controller import EarlyStopController
from gradient_stability_monitor import GradientStabilityMonitor
from debug_reporter import initialize_debug_reporter, get_debug_reporter

# MLflow for experiment tracking
try:
    import mlflow
    import mlflow.pytorch  # Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ PyTorch

    MLFLOW_AVAILABLE = True

    # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    try:
        from mlflow_metrics_enhancer import (
            log_enhanced_training_metrics,
            log_system_metrics,
        )

        ENHANCED_LOGGING = True
        print("âœ… Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ¾Ğµ MLflow Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾")
    except ImportError:
        ENHANCED_LOGGING = False
        # MLflow Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (ÑĞºÑ€Ñ‹Ñ‚Ğ¾ Ğ´Ğ»Ñ Ñ‡Ğ¸ÑÑ‚Ğ¾Ñ‚Ñ‹ Ğ»Ğ¾Ğ³Ğ¾Ğ²)

except ImportError:
    MLFLOW_AVAILABLE = False
    ENHANCED_LOGGING = False

# ĞŸĞ¾Ğ´Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ warning'Ğ¸
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings(
    "ignore", category=UserWarning, message=".*torch.cuda.*DtypeTensor.*"
)
warnings.filterwarnings("ignore", message=".*deprecated.*")


def reduce_tensor(tensor, n_gpus):
    rt = tensor.clone()
    dist.all_reduce(rt, op=dist.ReduceOp.SUM)
    rt /= n_gpus
    return rt


def init_distributed(hparams, n_gpus, rank, group_name):
    assert torch.cuda.is_available(), "Distributed mode requires CUDA."
    print("Initializing Distributed")

    # Set cuda device so everything is done on the right GPU.
    torch.cuda.set_device(rank % torch.cuda.device_count())

    # Initialize distributed communication
    dist.init_process_group(
        backend=hparams.dist_backend,
        init_method=hparams.dist_url,
        world_size=n_gpus,
        rank=rank,
        group_name=group_name,
    )

    print("Done initializing distributed")


def prepare_dataloaders(hparams):
    # Get data, data loaders and collate function ready
    trainset = TextMelLoader(hparams.training_files, hparams)
    valset = TextMelLoader(hparams.validation_files, hparams)
    collate_fn = TextMelCollate(hparams.n_frames_per_step)

    if hparams.distributed_run:
        train_sampler = DistributedSampler(trainset)
        shuffle = False
    else:
        train_sampler = None
        shuffle = True

    train_loader = DataLoader(
        trainset,
        num_workers=1,
        shuffle=shuffle,
        sampler=train_sampler,
        batch_size=hparams.batch_size,
        pin_memory=False,
        drop_last=True,
        collate_fn=collate_fn,
    )
    return train_loader, valset, collate_fn


def load_model(hparams):
    model = Tacotron2(hparams).cuda()
    # FP16 Ğ±ÑƒĞ´ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· AMP, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¾ÑÑ‚Ğ°ĞµÑ‚ÑÑ Ğ² FP32

    if hparams.distributed_run:
        model = apply_gradient_allreduce(model)

    return model


def warm_start_model(checkpoint_path, model, ignore_layers, exclude=None):
    assert os.path.isfile(checkpoint_path)
    print("Warm starting model from checkpoint '{}'".format(checkpoint_path))
    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ weights_only=False Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ PyTorch 2.6+
    checkpoint_dict = torch.load(
        checkpoint_path, map_location="cpu", weights_only=False
    )
    model_dict = checkpoint_dict["state_dict"]
    # model_dict['embedding.embedding.weight'] = model_dict['embedding.weight']
    # del model_dict['embedding.weight']
    print("ignoring layers:", ignore_layers)
    if len(ignore_layers) > 0 or exclude:
        model_dict = {
            k: v
            for k, v in model_dict.items()
            if k not in ignore_layers and (not exclude or exclude not in k)
        }

        dummy_dict = model.state_dict()
        dummy_dict.update(model_dict)
        model_dict = dummy_dict
    model.load_state_dict(model_dict, strict=False)
    return model


def load_checkpoint(checkpoint_path, model, optimizer):
    assert os.path.isfile(checkpoint_path)
    print("Loading checkpoint '{}'".format(checkpoint_path))
    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ weights_only=False Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ PyTorch 2.6+
    checkpoint_dict = torch.load(
        checkpoint_path, map_location="cpu", weights_only=False
    )
    model.load_state_dict(checkpoint_dict["state_dict"])
    optimizer.load_state_dict(checkpoint_dict["optimizer"])
    learning_rate = checkpoint_dict["learning_rate"]
    iteration = checkpoint_dict["iteration"]
    print("Loaded checkpoint '{}' from iteration {}".format(checkpoint_path, iteration))
    return model, optimizer, learning_rate, iteration


def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):
    print(
        "Saving model and optimizer state at iteration {} to {}".format(
            iteration, filepath
        )
    )
    torch.save(
        {
            "iteration": iteration,
            "state_dict": model.state_dict(),
            "optimizer": optimizer.state_dict(),
            "learning_rate": learning_rate,
        },
        filepath,
    )


def validate(
    model,
    criterion,
    valset,
    iteration,
    batch_size,
    n_gpus,
    collate_fn,
    writer,
    distributed_run=False,
    rank=1,
    minimize=False,
):
    """Handles all the validation scoring and printing"""
    model.eval()
    with torch.no_grad():
        val_sampler = DistributedSampler(valset) if distributed_run else None
        val_loader = DataLoader(
            valset,
            sampler=val_sampler,
            num_workers=1,
            shuffle=False,
            batch_size=batch_size,
            pin_memory=False,
            collate_fn=collate_fn,
        )
        model.decoder.p_teacher_forcing = 0.0
        val_loss = 0.0
        for i, batch in enumerate(val_loader):
            x, y = model.parse_batch(batch)
            y_pred = model(x, minimize)
            _loss = criterion(y_pred, y)
            loss = sum(_loss)
            if distributed_run:
                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()
            else:
                reduced_val_loss = loss.item()
            val_loss += reduced_val_loss
        val_loss = val_loss / (i + 1)

    # ğŸ”¥ Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ• Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ² train Ñ€ĞµĞ¶Ğ¸Ğ¼
    model.train()
    model.decoder.p_teacher_forcing = 1.0
    if rank == 0:
        print("Validation loss {}: {:9f}  ".format(iteration, val_loss))

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· writer
        writer.add_scalar("validation.loss", val_loss, iteration)

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (Ğ²Ğ·ÑÑ‚Ğ¾ Ğ¸Ğ· Tacotron2Logger)
        try:
            # ğŸ”¥ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ inference Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¼Ğ¸ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸
            with torch.no_grad():
                # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ eval Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ BatchNorm Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
                model.eval()

                # ğŸ”¥ Ğ’ĞĞ–ĞĞ: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· validation Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²
                # Ğ’Ğ¼ĞµÑÑ‚Ğ¾ inference Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ training forward pass Ğ´Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²
                try:
                    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· validation batch
                    validation_outputs = model(x, minimize=False)

                    # validation_outputs: [decoder_outputs, mel_outputs, mel_outputs_postnet, gate_outputs, alignments, ...]
                    if len(validation_outputs) >= 5:
                        (
                            decoder_outputs_val,
                            mel_outputs_val,
                            mel_outputs_postnet_val,
                            gate_outputs_val,
                            alignments_val,
                        ) = validation_outputs[:5]

                        # Ğ”Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ validation outputs (Ğ¾Ğ½Ğ¸ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°)
                        inference_outputs = [
                            None,
                            mel_outputs_val,
                            mel_outputs_postnet_val,
                            gate_outputs_val,
                            alignments_val,
                        ]
                        print(
                            f"âœ… Validation forward pass: mel={mel_outputs_postnet_val.shape if mel_outputs_postnet_val is not None else 'None'}, "
                            f"gate={gate_outputs_val.shape if gate_outputs_val is not None else 'None'}, "
                            f"align={alignments_val.shape if alignments_val is not None else 'None'}"
                        )
                    else:
                        print(
                            f"âš ï¸ Validation outputs Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ: {len(validation_outputs)} ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ²"
                        )
                        inference_outputs = None

                except Exception as val_e:
                    print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° validation forward pass: {val_e}")

                    # Fallback Ğº inference Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
                    try:
                        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Ğ±Ğ°Ñ‚Ñ‡Ğ°
                        input_text = x[0][:1] if x[0].size(0) > 0 else x[0]

                        if input_text.size(0) == 0:
                            print("âš ï¸ ĞŸÑƒÑÑ‚Ğ¾Ğ¹ Ğ±Ğ°Ñ‚Ñ‡ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹")
                            inference_outputs = None
                        else:
                            inference_outputs = model.inference(input_text)
                            print(f"ğŸ“ Fallback inference Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½")
                    except Exception as inf_e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° fallback inference: {inf_e}")
                        inference_outputs = None

            # inference Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ [None, mel_outputs, mel_outputs_postnet, gate_outputs, alignments, emb_gst]
            if inference_outputs is not None and len(inference_outputs) >= 5:
                (
                    _,
                    mel_outputs_inf,
                    mel_outputs_postnet_inf,
                    gate_outputs_inf,
                    alignments_inf,
                ) = inference_outputs[:5]
                mel_targets, gate_targets = y[0], y[1]

                print(f"ğŸ–¼ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ TensorBoard (iteration {iteration})")

                # plot distribution of parameters (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 500 Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸)
                if iteration % 500 == 0:
                    for tag, value in model.named_parameters():
                        tag = tag.replace(".", "/")
                        writer.add_histogram(tag, value.data.cpu().numpy(), iteration)

                idx = 0  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚ Ğ¸Ğ· Ğ±Ğ°Ñ‚Ñ‡Ğ°

                # ğŸ”¥ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞĞ• ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²

                # Alignment Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
                if alignments_inf is not None and alignments_inf.size(0) > idx:
                    try:
                        alignment_data = alignments_inf[idx].data.cpu().numpy()
                        if alignment_data.shape[0] > 1 and alignment_data.shape[1] > 1:
                            alignment_img = plot_alignment_to_numpy(alignment_data.T)
                            writer.add_image(
                                "alignment", alignment_img, iteration, dataformats="HWC"
                            )
                            print(
                                f"âœ… Alignment Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾: {alignment_img.shape}"
                            )
                        else:
                            print(
                                f"âš ï¸ Alignment Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ğ° ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ°Ñ: {alignment_data.shape}"
                            )
                    except Exception as e:
                        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ alignment Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {e}")

                # Mel target Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
                if mel_targets.size(0) > idx:
                    try:
                        mel_target_data = mel_targets[idx].data.cpu().numpy()
                        if (
                            mel_target_data.shape[0] > 1
                            and mel_target_data.shape[1] > 1
                        ):
                            mel_target_img = plot_spectrogram_to_numpy(mel_target_data)
                            writer.add_image(
                                "mel_target",
                                mel_target_img,
                                iteration,
                                dataformats="HWC",
                            )
                            print(
                                f"âœ… Mel target Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾: {mel_target_img.shape}"
                            )
                        else:
                            print(
                                f"âš ï¸ Mel target ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹: {mel_target_data.shape}"
                            )
                    except Exception as e:
                        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ mel target Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {e}")

                # Mel predicted Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
                if mel_outputs_inf is not None and mel_outputs_inf.size(0) > idx:
                    try:
                        mel_pred_data = mel_outputs_inf[idx].data.cpu().numpy()
                        if mel_pred_data.shape[0] > 1 and mel_pred_data.shape[1] > 1:
                            mel_pred_img = plot_spectrogram_to_numpy(mel_pred_data)
                            writer.add_image(
                                "mel_predicted",
                                mel_pred_img,
                                iteration,
                                dataformats="HWC",
                            )
                            print(
                                f"âœ… Mel predicted Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾: {mel_pred_img.shape}"
                            )
                        else:
                            print(
                                f"âš ï¸ Mel predicted ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹: {mel_pred_data.shape}"
                            )
                    except Exception as e:
                        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ mel predicted Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {e}")

                # Gate outputs Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
                if (
                    gate_outputs_inf is not None
                    and gate_outputs_inf.size(0) > idx
                    and gate_targets.size(0) > idx
                ):
                    try:
                        gate_target_data = gate_targets[idx].data.cpu().numpy()
                        gate_pred_data = (
                            torch.sigmoid(gate_outputs_inf[idx]).data.cpu().numpy()
                        )

                        if len(gate_target_data) > 1 and len(gate_pred_data) > 1:
                            gate_img = plot_gate_outputs_to_numpy(
                                gate_target_data, gate_pred_data
                            )
                            writer.add_image(
                                "gate", gate_img, iteration, dataformats="HWC"
                            )
                            print(f"âœ… Gate Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾: {gate_img.shape}")
                        else:
                            print(
                                f"âš ï¸ Gate Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ: target={len(gate_target_data)}, pred={len(gate_pred_data)}"
                            )
                    except Exception as e:
                        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ gate Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ: {e}")

                # ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² TensorBoard
                writer.flush()
                print(f"ğŸ”„ TensorBoard Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ´Ğ»Ñ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ {iteration}")

                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ¾Ğ²
                try:
                    model.last_validation_alignments = alignments_inf
                    model.last_validation_gate_outputs = gate_outputs_inf
                    model.last_validation_mel_outputs = mel_outputs_postnet_inf
                except Exception:
                    pass

            else:
                print(f"âš ï¸ Inference Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹")

            # ğŸ”¥ Ğ’ĞĞ–ĞĞ: Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² train Ñ€ĞµĞ¶Ğ¸Ğ¼
            model.train()

        except Exception as e:
            print(f"âŒ ĞĞ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: {e}")
            # Fallback - ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
            try:
                mel_targets, gate_targets = y[0], y[1]
                if mel_targets.size(0) > 0:
                    mel_target_img = plot_spectrogram_to_numpy(
                        mel_targets[0].data.cpu().numpy()
                    )
                    writer.add_image(
                        "mel_target_fallback",
                        mel_target_img,
                        iteration,
                        dataformats="HWC",
                    )
                    print(f"âœ… Fallback mel target Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾")
            except Exception as fallback_e:
                print(f"âŒ Ğ”Ğ°Ğ¶Ğµ fallback Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ: {fallback_e}")

        # ğŸ”¥ ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ: Ğ£Ğ±ĞµĞ¶Ğ´Ğ°ĞµĞ¼ÑÑ Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² train Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ
        model.train()

        if MLFLOW_AVAILABLE:
            validation_metrics = {
                "validation.loss": val_loss,
                "validation.step": iteration,
            }

            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸Ğ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            if hasattr(model, "decoder") and hasattr(
                model.decoder, "attention_weights"
            ):
                try:
                    # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ attention
                    attention_weights = model.decoder.attention_weights
                    if attention_weights is not None:
                        validation_metrics["validation.attention_entropy"] = float(
                            torch.mean(
                                torch.sum(
                                    -attention_weights
                                    * torch.log(attention_weights + 1e-8),
                                    dim=-1,
                                )
                            )
                        )
                except Exception as e:
                    print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¸ attention entropy: {e}")

            # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸Ğ· alignments
            if alignments_inf is not None:
                try:
                    # Ğ”Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ alignment Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹
                    alignment_diag = torch.diagonal(alignments_inf[0], dim1=-2, dim2=-1)
                    align_score = float(torch.mean(alignment_diag))
                    validation_metrics["validation.alignment_score"] = align_score
                    # ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ´Ğ»Ñ AutoParamController
                    try:
                        model.last_validation_alignment_score = align_score
                    except Exception:
                        pass
                    # Ğ¤Ğ¾ĞºÑƒÑĞ¸Ñ€Ğ¾Ğ²ĞºĞ° attention (ĞºĞ¾Ğ½Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»Ğ¸)
                    attention_focus = torch.max(alignments_inf[0], dim=-1)[0]
                    validation_metrics["validation.attention_focus"] = float(
                        torch.mean(attention_focus)
                    )
                except Exception as e:
                    print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¸ attention Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº: {e}")

            # Ğ£Ğ±ĞµĞ¶Ğ´Ğ°ĞµĞ¼ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ´Ğ°Ğ¶Ğµ Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ align_score
            if not hasattr(model, "last_validation_alignment_score"):
                model.last_validation_alignment_score = None

            # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ¸Ğ· gate outputs
            if gate_outputs_inf is not None:
                try:
                    gate_probs = torch.sigmoid(gate_outputs_inf[0])
                    validation_metrics["validation.gate_mean"] = float(
                        torch.mean(gate_probs)
                    )
                    validation_metrics["validation.gate_std"] = float(
                        torch.std(gate_probs)
                    )
                except Exception as e:
                    print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¸ gate Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº: {e}")

            if ENHANCED_LOGGING:
                # Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ validation Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
                log_enhanced_training_metrics(validation_metrics, iteration)
            else:
                # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ MLflow Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
                for metric_name, metric_value in validation_metrics.items():
                    mlflow.log_metric(metric_name, metric_value, step=iteration)

    return val_loss


def calculate_global_mean(data_loader, global_mean_npy):
    if global_mean_npy and os.path.exists(global_mean_npy):
        global_mean = np.load(global_mean_npy)
        return to_gpu(torch.tensor(global_mean))
    sums = []
    frames = []
    print("\nCalculating global mean...\n")
    for i, batch in enumerate(data_loader):
        (
            text_padded,
            input_lengths,
            mel_padded,
            gate_padded,
            output_lengths,
            ctc_text,
            ctc_text_lengths,
            _guide_mask,
        ) = batch
        # padded values are 0.
        sums.append(mel_padded.double().sum(dim=(0, 2)))
        frames.append(output_lengths.double().sum())
    global_mean = sum(sums) / sum(frames)
    global_mean = global_mean.float()
    if global_mean_npy:
        np.save(global_mean_npy, global_mean)
    return to_gpu(global_mean)


def train(
    output_directory,
    log_directory,
    checkpoint_path,
    warm_start,
    ignore_mmi_layers,
    ignore_gst_layers,
    ignore_tsgst_layers,
    n_gpus,
    rank,
    group_name,
    hparams,
    # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ Smart Tuner
    smart_tuner_trial=None,
    smart_tuner_logger=None,
    tensorboard_writer=None,
    telegram_monitor=None,
):
    """Training and validation logging results to tensorboard and stdout

    Params
    ------
    output_directory (string): directory to save checkpoints
    log_directory (string) directory to save tensorboard logs
    checkpoint_path(string): checkpoint path
    warm_start(bool): load model from checkpoint
    n_gpus (int): number of gpus
    rank (int): rank of current gpu
    hparams (object): comma separated list of "name=value" pairs.
    """
    if hparams.distributed_run:
        init_distributed(hparams, n_gpus, rank, group_name)

    torch.manual_seed(hparams.seed)
    torch.cuda.manual_seed(hparams.seed)
    random.seed(hparams.seed)
    np.random.seed(hparams.seed)

    model = load_model(hparams)
    learning_rate = hparams.learning_rate
    optimizer = torch.optim.Adam(
        model.parameters(), lr=learning_rate, weight_decay=hparams.weight_decay
    )

    # ---------- FP16 / Mixed Precision ---------
    apex_available = False  # Ğ¤Ğ»Ğ°Ğ³ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ NVIDIA Apex
    use_native_amp = False  # Ğ¤Ğ»Ğ°Ğ³ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ torch.cuda.amp
    scaler = None  # GradScaler Ğ´Ğ»Ñ native AMP

    if hparams.fp16_run:
        try:
            from apex import amp  # type: ignore

            model, optimizer = amp.initialize(model, optimizer, opt_level="O2")
            apex_available = True
            print("âœ… NVIDIA Apex ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ Ğ´Ğ»Ñ FP16 Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ")
        except ImportError:
            # Apex Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ â€“ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ AMP PyTorch
            try:
                from torch.amp import GradScaler, autocast

                # ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ FP16 Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ AMP
                model = model.float()  # Ğ£Ğ±ĞµĞ¶Ğ´Ğ°ĞµĞ¼ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² FP32
                scaler = GradScaler("cuda")
                use_native_amp = True
                print(
                    "âœ… NVIDIA Apex Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğ½Ğ° torch.amp (PyTorch Native AMP)"
                )
            except ImportError as e:
                # Ğ”Ğ°Ğ¶Ğµ native AMP Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ â€“ Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ FP16
                hparams.fp16_run = False
                print(f"âŒ Mixed precision Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: {e}. FP16 Ğ¾Ñ‚ĞºĞ»ÑÑ‡Ñ‘Ğ½.")
    # -------------------------------------------

    if hparams.distributed_run:
        model = apply_gradient_allreduce(model)

    criterion = Tacotron2Loss(hparams)

    train_loader, valset, collate_fn = prepare_dataloaders(hparams)

    # Load checkpoint if one exists
    iteration = 0
    epoch_offset = 0
    if checkpoint_path is not None:
        if warm_start:
            model = warm_start_model(checkpoint_path, model, hparams.ignore_layers)
        else:
            model, optimizer, _learning_rate, iteration = load_checkpoint(
                checkpoint_path, model, optimizer
            )
            if hparams.use_saved_learning_rate:
                learning_rate = _learning_rate
            iteration += 1  # next iteration is iteration + 1
            epoch_offset = max(0, int(iteration / len(train_loader)))

    model.train()
    is_main_node = rank == 0

    # ğŸ’¡ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ğ½Ñ‹Ğ¹ writer, Ğ° Ğ½Ğµ ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹
    writer = tensorboard_writer if is_main_node else None

    if is_main_node and writer is None:
        # Ğ”Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ¹ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸, ĞµÑĞ»Ğ¸ train.py Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ
        from torch.utils.tensorboard import SummaryWriter

        writer = SummaryWriter(log_directory)

    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ loss Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹
    mmi_loss = None
    guide_loss = None
    
    if hparams.use_mmi:
        from mmi_loss import MMI_loss

        mmi_loss = MMI_loss(hparams.mmi_map, hparams.mmi_weight)
        print("âœ… MMI loss Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")

    if hparams.use_guided_attn:
        from loss_function import GuidedAttentionLoss

        guide_loss = GuidedAttentionLoss(alpha=hparams.guided_attn_weight)
        print("âœ… Guided Attention Loss Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")

    # --- Auto Hyper-parameter Controller ---
    quality_ctrl = None
    if is_main_node:
        try:
            quality_ctrl = AdvancedQualityController()
            print("ğŸ¤– AdvancedQualityController Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
        except Exception as e:
            print(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ AdvancedQualityController: {e}")

    # --- ParamScheduler Ğ¸ EarlyStopController ---
    sched_ctrl = None
    stop_ctrl = None
    if is_main_node:
        try:
            sched_ctrl = ParamScheduler()
            print("ğŸ“… ParamScheduler Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
        except Exception as e:
            print(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ParamScheduler: {e}")

        try:
            stop_ctrl = EarlyStopController()
            print("ğŸ›‘ EarlyStopController Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
        except Exception as e:
            print(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ EarlyStopController: {e}")

    # --- ğŸ” Debug Reporter ---
    debug_reporter = None
    if is_main_node:
        try:
            debug_reporter = initialize_debug_reporter(telegram_monitor)
            print("ğŸ” Debug Reporter Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ - Ğ¾Ñ‚Ñ‡ĞµÑ‚Ñ‹ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 1000 ÑˆĞ°Ğ³Ğ¾Ğ²")
        except Exception as e:
            print(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Debug Reporter: {e}")

    global_mean = calculate_global_mean(train_loader, hparams.global_mean_npy)

    # ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ validation loss Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾
    last_validation_loss = None
    last_audio_step = 0

    # === EMA Ğ¸ Ğ°Ğ²Ñ‚Ğ¾-LR Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ===
    grad_norm_ema = 0.0  # ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰ĞµĞµ ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°
    ema_beta = 0.95      # ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ EMA
    lr_adjust_interval = 10  # Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ LR Ğ² ÑˆĞ°Ğ³Ğ°Ñ…

    # ================ MAIN TRAINNIG LOOP ===================
    print(
        f"ğŸš€ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ: epochs={hparams.epochs}, batch_size={hparams.batch_size}, dataset_size={len(train_loader)}"
    )

    # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² MLflow
    model_params = {
        "model.total_params": sum(p.numel() for p in model.parameters()),
        "model.trainable_params": sum(
            p.numel() for p in model.parameters() if p.requires_grad
        ),
        "hparams.epochs": hparams.epochs,
        "hparams.grad_clip_thresh": hparams.grad_clip_thresh,
        "hparams.fp16_run": hparams.fp16_run,
        "hparams.use_mmi": hparams.use_mmi,
        "hparams.use_guided_attn": hparams.use_guided_attn,
        "dataset.train_size": len(train_loader),
        "dataset.val_size": len(valset),
    }

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ nested run Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ trial
    if smart_tuner_trial is not None:
        # Ğ”Ğ»Ñ Smart Tuner ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ nested run
        trial_run_name = f"trial_{smart_tuner_trial.number}"
        with mlflow.start_run(nested=True, run_name=trial_run_name):
            try:
                mlflow.log_params(model_params)
                mlflow.log_param("hparams.batch_size_init", hparams.batch_size)
                mlflow.log_param("hparams.learning_rate_init", hparams.learning_rate)
                print(f"ğŸ“Š ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ trial {smart_tuner_trial.number} Ğ·Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ² MLflow")
            except Exception as e:
                print(f"ğŸ“Š ĞÑˆĞ¸Ğ±ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² trial: {e}")
    else:
        # Ğ”Ğ»Ñ Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
        try:
            mlflow.log_params(model_params)
            mlflow.log_param("hparams.batch_size_init", hparams.batch_size)
            mlflow.log_param("hparams.learning_rate_init", hparams.learning_rate)
            print(f"ğŸ“Š ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ·Ğ°Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ² MLflow: {model_params['model.total_params']} Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²")
        except Exception as e:
            print(f"ğŸ“Š ĞÑˆĞ¸Ğ±ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²: {e}")

    # --- Intelligent Epoch Optimizer ---
    optimizer_epochs = None
    if is_main_node:
        try:
            optimizer_epochs = IntelligentEpochOptimizer()
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ dataset_meta Ğ¸Ğ· Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸
            dataset_meta = {
                "total_duration_hours": len(train_loader)
                * hparams.batch_size
                * 0.1,  # Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°
                "quality_metrics": {
                    "background_noise_level": 0.3,
                    "voice_consistency": 0.8,
                    "speech_clarity": 0.85,
                },
                "voice_features": {
                    "has_accent": False,
                    "emotional_range": "neutral",
                    "speaking_style": "normal",
                    "pitch_range_semitones": 12,
                },
            }
            analysis = optimizer_epochs.analyze_dataset(dataset_meta)
            if "recommended_epochs" in analysis:
                hparams.epochs = analysis["recommended_epochs"]
                print(f"ğŸ”§ Epochs set to {hparams.epochs} (Ğ±Ñ‹Ğ»Ğ¾ {hparams.epochs})")
        except Exception as e:
            print(f"âš ï¸ IntelligentEpochOptimizer Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")

    gradient_monitor = GradientStabilityMonitor()
    restart_attempts = 0
    max_restart_attempts = 3
    safe_hparams_history = []
    
    def get_safe_hparams(hparams, attempt):
        """
        ğŸ›¡ï¸ Ğ£Ğ›Ğ¬Ğ¢Ğ Ğ-Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ NaN/Inf
        ĞĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾ ÑĞ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        """
        new_hparams = copy.deepcopy(hparams)
        
        # ğŸ”¥ Ğ­ĞšĞ¡Ğ¢Ğ Ğ•ĞœĞĞ›Ğ¬ĞĞĞ• ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ learning rate (ĞºĞ°Ğ¶Ğ´Ğ°Ñ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ² 10 Ñ€Ğ°Ğ· Ğ¼ĞµĞ½ÑŒÑˆĞµ)
        new_hparams.learning_rate = max(new_hparams.learning_rate * (0.1 ** (attempt + 1)), 1e-8)
        
        # ğŸ“¦ ĞĞ³Ñ€ĞµÑÑĞ¸Ğ²Ğ½Ğ¾Ğµ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ batch size Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        new_hparams.batch_size = max(1, int(new_hparams.batch_size * (0.5 ** (attempt + 1))))
        
        # ğŸ¯ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ÑƒĞ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ guided attention Ğ´Ğ»Ñ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ alignment
        if hasattr(new_hparams, 'guide_loss_initial_weight'):
            new_hparams.guide_loss_initial_weight = min(1000.0, max(10.0, new_hparams.guide_loss_initial_weight * (3.0 ** (attempt + 1))))
        else:
            new_hparams.guide_loss_initial_weight = 10.0 * (3.0 ** (attempt + 1))
        
        # âœ‚ï¸ Ğ­ĞšĞ¡Ğ¢Ğ Ğ•ĞœĞĞ›Ğ¬ĞĞ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾Ğµ ĞºĞ»Ğ¸Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
        new_hparams.grad_clip_thresh = max(0.001, new_hparams.grad_clip_thresh * (0.1 ** (attempt + 1)))
        
        # ğŸš« ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ğ²ÑĞµ "Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ" Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        if hasattr(new_hparams, 'use_mmi'):
            new_hparams.use_mmi = False
        if hasattr(new_hparams, 'use_audio_quality_enhancement'):
            new_hparams.use_audio_quality_enhancement = False
        
        # ğŸ›¡ï¸ ĞŸĞ Ğ˜ĞĞ£Ğ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¯ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ guided attention
        new_hparams.use_guided_attn = True
        
        # ğŸ›ï¸ ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ dropout Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞµĞ¹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ğ¾ÑÑ‚Ğ¸
        if hasattr(new_hparams, 'dropout_rate'):
            new_hparams.dropout_rate = min(0.005, new_hparams.dropout_rate * 0.1)
        if hasattr(new_hparams, 'encoder_dropout_rate'):
            new_hparams.encoder_dropout_rate = min(0.001, new_hparams.encoder_dropout_rate * 0.1)
        if hasattr(new_hparams, 'postnet_dropout_rate'):
            new_hparams.postnet_dropout_rate = min(0.001, new_hparams.postnet_dropout_rate * 0.1)
        if hasattr(new_hparams, 'p_attention_dropout'):
            new_hparams.p_attention_dropout = min(0.01, new_hparams.p_attention_dropout * 0.1)
        if hasattr(new_hparams, 'p_decoder_dropout'):
            new_hparams.p_decoder_dropout = min(0.01, new_hparams.p_decoder_dropout * 0.1)
        
        # ğŸšª ĞšĞ¾Ğ½ÑĞµÑ€Ğ²Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ gate threshold
        if hasattr(new_hparams, 'gate_threshold'):
            new_hparams.gate_threshold = 0.5  # Ğ¡Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ğ¾Ğ³
        
        # ğŸ”„ ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ°Ñ…
        if attempt >= 2:
            new_hparams.force_model_reinit = True
            new_hparams.xavier_init = True  # ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Xavier Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        
        # ğŸ“Š Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸
        print(f"\nğŸ›¡ï¸ [ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• Ğ’ĞĞ¡Ğ¡Ğ¢ĞĞĞĞ’Ğ›Ğ•ĞĞ˜Ğ•] ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {attempt+1}:")
        print(f"  ğŸ”¥ learning_rate: {hparams.learning_rate:.8f} â†’ {new_hparams.learning_rate:.8f} (ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¾ Ğ² {hparams.learning_rate/new_hparams.learning_rate:.1f}x)")
        print(f"  ğŸ“¦ batch_size: {hparams.batch_size} â†’ {new_hparams.batch_size}")
        print(f"  ğŸ¯ guide_loss_weight: {getattr(hparams, 'guide_loss_initial_weight', 1.0):.2f} â†’ {new_hparams.guide_loss_initial_weight:.2f}")
        print(f"  âœ‚ï¸ grad_clip_thresh: {hparams.grad_clip_thresh:.4f} â†’ {new_hparams.grad_clip_thresh:.4f}")
        print(f"  ğŸ›¡ï¸ use_guided_attn: ĞŸĞ Ğ˜ĞĞ£Ğ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½")
        print(f"  ğŸš« ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½Ñ‹: MMI, audio_enhancement")
        print(f"  ğŸ›ï¸ ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ²ÑĞµ dropout Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ")
        
        return new_hparams

    while restart_attempts <= max_restart_attempts:
        for epoch in range(epoch_offset, hparams.epochs):
            print("Epoch: {} / {}".format(epoch, hparams.epochs))
            for i, batch in enumerate(train_loader):
                # ğŸ”¥ Ğ”ĞĞŸĞĞ›ĞĞ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¯ Ğ—ĞĞ©Ğ˜Ğ¢Ğ: Ğ£Ğ±ĞµĞ¶Ğ´Ğ°ĞµĞ¼ÑÑ Ñ‡Ñ‚Ğ¾ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ² train Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ
                model.train()

                start = time.perf_counter()
                model.zero_grad()

                x, y = model.parse_batch(batch)

                # Forward pass Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ğ¾Ğ¹ ÑÑ…ĞµĞ¼Ñ‹ mixed precision
                if hparams.fp16_run and use_native_amp:
                    with autocast("cuda"):
                        try:
                            y_pred = model(x)
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° forward pass Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
                            y_pred = None

                        # total loss
                        if y_pred is not None:
                            try:
                                loss_taco, loss_gate, loss_atten, loss_emb = criterion(y_pred, y)
                            except Exception as e:
                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° criterion: {e}")
                                # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ device Ğ¸Ğ· x (tuple)
                                device = x[0].device if isinstance(x, tuple) and len(x) > 0 else 'cuda'
                                loss_taco = torch.tensor(0.0, device=device, requires_grad=True)
                                loss_gate = torch.tensor(0.0, device=device, requires_grad=True)
                                loss_atten = torch.tensor(0.0, device=device, requires_grad=True)
                                loss_emb = torch.tensor(0.0, device=device, requires_grad=True)
                        else:
                            # Ğ•ÑĞ»Ğ¸ y_pred None, ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğµ loss
                            # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ device Ğ¸Ğ· x (tuple)
                            device = x[0].device if isinstance(x, tuple) and len(x) > 0 else 'cuda'
                            loss_taco = torch.tensor(0.0, device=device, requires_grad=True)
                            loss_gate = torch.tensor(0.0, device=device, requires_grad=True)
                            loss_atten = torch.tensor(0.0, device=device, requires_grad=True)
                            loss_emb = torch.tensor(0.0, device=device, requires_grad=True)
                        
                        # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ device
                        if y_pred is not None and len(y_pred) > 1 and y_pred[1] is not None:
                            device = y_pred[1].device  # mel_outputs Ğ²ÑĞµĞ³Ğ´Ğ° Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€
                        else:
                            device = x[0].device if isinstance(x, tuple) and len(x) > 0 else 'cuda'
                        try:
                            loss_guide = (
                                guide_loss(y_pred)
                                if hparams.use_guided_attn and guide_loss is not None and y_pred is not None
                                else torch.tensor(0.0, device=device, requires_grad=True)
                            )
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° guide_loss: {e}")
                            loss_guide = torch.tensor(0.0, device=device, requires_grad=True)
                        
                        try:
                            loss_mmi = (
                                mmi_loss(y_pred[1], y[0])
                                if hparams.use_mmi and mmi_loss is not None and y_pred is not None and y_pred[1] is not None
                                else torch.tensor(0.0, device=device, requires_grad=True)
                            )
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° mmi_loss: {e}")
                            loss_mmi = torch.tensor(0.0, device=device, requires_grad=True)
                        try:
                            loss = (
                                0.4 * loss_taco +
                                0.3 * loss_atten +
                                0.3 * loss_gate +
                                loss_guide +
                                loss_mmi +
                                loss_emb
                            )
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ loss: {e}")
                            loss = torch.tensor(0.0, device=device, requires_grad=True)
                else:
                    try:
                        y_pred = model(x)
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° forward pass Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
                        y_pred = None
                    # total loss
                    if y_pred is not None:
                        try:
                            loss_taco, loss_gate, loss_atten, loss_emb = criterion(y_pred, y)
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° criterion: {e}")
                            # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ device Ğ¸Ğ· x (tuple)
                            device = x[0].device if isinstance(x, tuple) and len(x) > 0 else 'cuda'
                            loss_taco = torch.tensor(0.0, device=device, requires_grad=True)
                            loss_gate = torch.tensor(0.0, device=device, requires_grad=True)
                            loss_atten = torch.tensor(0.0, device=device, requires_grad=True)
                            loss_emb = torch.tensor(0.0, device=device, requires_grad=True)
                    else:
                        # Ğ•ÑĞ»Ğ¸ y_pred None, ÑĞ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğµ loss
                        # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ device Ğ¸Ğ· x (tuple)
                        device = x[0].device if isinstance(x, tuple) and len(x) > 0 else 'cuda'
                        loss_taco = torch.tensor(0.0, device=device, requires_grad=True)
                        loss_gate = torch.tensor(0.0, device=device, requires_grad=True)
                        loss_atten = torch.tensor(0.0, device=device, requires_grad=True)
                        loss_emb = torch.tensor(0.0, device=device, requires_grad=True)
                    # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ device
                    if y_pred is not None and len(y_pred) > 1 and y_pred[1] is not None:
                        device = y_pred[1].device  # mel_outputs Ğ²ÑĞµĞ³Ğ´Ğ° Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€
                    else:
                        device = x[0].device if isinstance(x, tuple) and len(x) > 0 else 'cuda'
                    try:
                        loss_guide = (
                            guide_loss(y_pred)
                            if hparams.use_guided_attn and guide_loss is not None and y_pred is not None
                            else torch.tensor(0.0, device=device, requires_grad=True)
                        )
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° guide_loss: {e}")
                        loss_guide = torch.tensor(0.0, device=device, requires_grad=True)
                    
                    try:
                        loss_mmi = (
                            mmi_loss(y_pred[1], y[0])
                            if hparams.use_mmi and mmi_loss is not None and y_pred is not None and y_pred[1] is not None
                            else torch.tensor(0.0, device=device, requires_grad=True)
                        )
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° mmi_loss: {e}")
                        loss_mmi = torch.tensor(0.0, device=device, requires_grad=True)
                    try:
                        loss = (
                            0.4 * loss_taco +
                            0.3 * loss_atten +
                            0.3 * loss_gate +
                            loss_guide +
                            loss_mmi +
                            loss_emb
                        )
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ loss: {e}")
                        loss = torch.tensor(0.0, device=device, requires_grad=True)

                if hparams.distributed_run:
                    reduced_loss = reduce_tensor(loss.data, n_gpus).item() if loss is not None else 0.0
                else:
                    reduced_loss = loss.item() if loss is not None else 0.0
                    reduced_taco_loss = loss_taco.item() if loss_taco is not None else 0.0
                    reduced_atten_loss = loss_atten.item() if loss_atten is not None else 0.0
                    reduced_mi_loss = loss_mmi.item() if loss_mmi is not None else 0.0
                    reduced_guide_loss = (
                        loss_guide.item() if hparams.use_guided_attn and loss_guide is not None else 0.0
                    )
                    reduced_gate_loss = loss_gate.item() if loss_gate is not None else 0.0
                    reduced_emb_loss = loss_emb.item() if loss_emb is not None else 0.0

                # Backward pass
                if hparams.fp16_run and apex_available:
                    if loss is not None:
                        with amp.scale_loss(loss, optimizer) as scaled_loss:
                            scaled_loss.backward()
                    else:
                        print("âš ï¸ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: loss is None, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ backward pass")
                elif hparams.fp16_run and use_native_amp:
                    if loss is not None:
                        scaler.scale(loss).backward()
                    else:
                        print("âš ï¸ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: loss is None, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ backward pass")
                else:
                    if loss is not None:
                        if not hparams.fp16_run:
                            # --- Dynamic Loss Scaling (FP32 Ñ€ĞµĞ¶Ğ¸Ğ¼) ---
                            scaled_loss = loss * dyn_loss_scale
                            scaled_loss.backward()
                            # Unscale Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹
                            for param in model.parameters():
                                if param.grad is not None:
                                    param.grad.data.div_(dyn_loss_scale)
                        else:
                            loss.backward()
                    else:
                        print("âš ï¸ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: loss is None, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ backward pass")

                if loss is not None:
                    grad_norm = torch.nn.utils.clip_grad_norm_(
                        model.parameters(), hparams.grad_clip_thresh
                    )
                    # --- EMA Ğ½Ğ¾Ñ€Ğ¼Ñ‹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ° Ğ¸ Ğ°Ğ²Ñ‚Ğ¾-ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ LR ---
                    grad_norm_ema = ema_beta * grad_norm_ema + (1 - ema_beta) * float(grad_norm)
                    if (iteration % lr_adjust_interval) == 0 and iteration > 0:
                        current_lr = optimizer.param_groups[0]["lr"]
                        new_lr = current_lr
                        if grad_norm_ema > 10.0:
                            new_lr = max(hparams.learning_rate_min, current_lr * 0.5)
                        elif grad_norm_ema < 0.1:
                            new_lr = min(hparams.learning_rate * 2, current_lr * 1.1)
                        if abs(new_lr - current_lr) > 1e-12:
                            for g in optimizer.param_groups:
                                g["lr"] = new_lr
                            if debug_reporter:
                                debug_reporter.add_warning(
                                    f"LR auto-adjust: grad_norm_ema={grad_norm_ema:.3f}, lr {current_lr:.2e} â†’ {new_lr:.2e}"
                                )
                            print(f"ğŸ”„ LR auto-adjust: {current_lr:.6e} â†’ {new_lr:.6e} (grad_norm_ema={grad_norm_ema:.3f})")
                    # --- Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° NaN/Inf ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 10 ÑˆĞ°Ğ³Ğ¾Ğ² ---
                    if (iteration % 10) == 0 and (torch.isnan(loss) or torch.isinf(loss)):
                        print("ğŸš¨ [Auto-Recover] NaN/Inf Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ² loss â€“ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµĞ¼ LR Ğ¸ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑˆĞ°Ğ³")
                        for g in optimizer.param_groups:
                            g["lr"] = max(hparams.learning_rate_min, g["lr"] * 0.5)
                        # Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ°ĞµĞ¼ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ± Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ loss scaling
                        dyn_loss_scale = max(1.0, dyn_loss_scale / loss_scale_factor)
                        optimizer.zero_grad(set_to_none=True)
                        if debug_reporter:
                            debug_reporter.add_warning("NaN/Inf Ğ² loss: auto LR halved Ğ¸ ÑˆĞ°Ğ³ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½")
                        continue  # Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
                else:
                    grad_norm = 0.0

                # Optimizer step Ñ ÑƒÑ‡Ñ‘Ñ‚Ğ¾Ğ¼ ÑÑ…ĞµĞ¼Ñ‹ mixed precision
                if loss is not None:
                    if hparams.fp16_run and use_native_amp:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                else:
                    print("âš ï¸ ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: loss is None, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ optimizer step")

                # ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
                if loss is not None:
                    monitor_result = gradient_monitor.check_gradient_stability(model, loss, i + epoch * len(train_loader))
                else:
                    monitor_result = {'explosion_detected': False, 'nan_detected': False, 'recommendations': ['Loss is None']}
                if monitor_result['explosion_detected'] or monitor_result['nan_detected']:
                    print("\nğŸš¨ [Smart Restart] ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ° ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑˆĞ°Ğ³Ğµ {}!".format(i + epoch * len(train_loader)))
                    print("ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°: {}".format('Ğ’Ğ·Ñ€Ñ‹Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²' if monitor_result['explosion_detected'] else 'NaN/Inf Ğ² loss'))
                    print("Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸:")
                    for rec in monitor_result['recommendations']:
                        print("  - ", rec)
                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ ÑˆĞ°Ğ³
                    safe_hparams_history.append(copy.deepcopy(hparams))
                    # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
                    restart_attempts += 1
                    if restart_attempts > max_restart_attempts:
                        print("âŒ Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°. ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.")
                        return
                    hparams = get_safe_hparams(hparams, restart_attempts)
                    
                    # ğŸ”„ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
                    if hasattr(hparams, 'force_model_reinit') and hparams.force_model_reinit:
                        print("ğŸ”„ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ Ğ•Ğ˜ĞĞ˜Ğ¦Ğ˜ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ ĞœĞĞ”Ğ•Ğ›Ğ˜...")
                        try:
                            model = load_model(hparams)
                            # ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Xavier Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
                            if hasattr(hparams, 'xavier_init') and hparams.xavier_init:
                                for name, param in model.named_parameters():
                                    if len(param.shape) > 1:
                                        torch.nn.init.xavier_uniform_(param)
                                    else:
                                        torch.nn.init.zeros_(param)
                                print("âœ… Xavier Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ°")
                            
                            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ optimizer Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ
                            optimizer = torch.optim.Adam(model.parameters(), lr=hparams.learning_rate)
                            print("âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¿ĞµÑ€ĞµĞ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹")
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ€ĞµĞ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: {e}")
                            # ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒÑ
                    
                    # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ learning rate Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğµ
                    for g in optimizer.param_groups:
                        g["lr"] = hparams.learning_rate
                    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚ Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸
                    save_checkpoint(model, optimizer, hparams.learning_rate, i + epoch * len(train_loader), os.path.join(output_directory, f"restart_checkpoint_{restart_attempts}.pt"))
                    print(f"[Smart Restart] ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ Ğ½Ğ¾Ğ²Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ (Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {restart_attempts})...\n")
                    # break Ğ¾Ğ±Ğ° Ñ†Ğ¸ĞºĞ»Ğ°
                    break

                # ğŸ›¡ï¸ ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ—ĞĞ©Ğ˜Ğ¢Ğ ĞĞ¢ NaN LOSS
                if loss is not None:
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ²ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ loss Ğ½Ğ° NaN/Inf
                    loss_components = {
                        'total_loss': reduced_loss,
                        'taco_loss': reduced_taco_loss,
                        'atten_loss': reduced_atten_loss,
                        'mi_loss': reduced_mi_loss,
                        'guide_loss': reduced_guide_loss,
                        'gate_loss': reduced_gate_loss,
                        'emb_loss': reduced_emb_loss
                    }
                    
                    nan_detected = False
                    problematic_components = []
                    
                    for component_name, component_value in loss_components.items():
                        if torch.isnan(torch.tensor(component_value)) or torch.isinf(torch.tensor(component_value)):
                            nan_detected = True
                            problematic_components.append(f"{component_name}: {component_value}")
                    
                    if nan_detected:
                        print("\nğŸš¨ [ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ ĞĞ¨Ğ˜Ğ‘ĞšĞ] NaN/Inf Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½ Ğ² loss!")
                        print(f"ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: {problematic_components}")
                        print(f"Ğ¨Ğ°Ğ³: {i + epoch * len(train_loader)}")
                        
                        # ğŸ” Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ² debug reporter
                        if debug_reporter:
                            try:
                                critical_info = f"NaN/Inf Ğ½Ğ° ÑˆĞ°Ğ³Ğµ {iteration}. ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹: {', '.join(problematic_components)}"
                                debug_reporter.add_warning(critical_info)
                                print("ğŸ” ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ° Ğ² Debug Reporter")
                            except Exception as e:
                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Debug Reporter: {e}")
                        
                        # ğŸ“± Ğ¡Ğ ĞĞ§ĞĞĞ• Telegram ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ
                        if telegram_monitor:
                            try:
                                critical_message = f"ğŸš¨ **ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ ĞĞ¨Ğ˜Ğ‘ĞšĞ!**\n\n"
                                critical_message += f"âŒ **Loss ÑÑ‚Ğ°Ğ» NaN Ğ½Ğ° ÑˆĞ°Ğ³Ğµ {iteration}**\n"
                                critical_message += f"ğŸ”¥ **ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ğ¼Ğ¸:**\n"
                                for comp in problematic_components:
                                    critical_message += f"  â€¢ {comp}\n"
                                critical_message += f"\nğŸ”„ **ĞĞ’Ğ¢ĞĞœĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞŸĞ•Ğ Ğ•Ğ—ĞĞŸĞ£Ğ¡Ğš Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ...**\n"
                                critical_message += f"âš™ï¸ Ğ¡Ğ½Ğ¸Ğ¶Ğ°ĞµĞ¼ learning_rate Ğ¸ ÑƒĞºÑ€ĞµĞ¿Ğ»ÑĞµĞ¼ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
                                
                                telegram_monitor._send_text_message(critical_message)
                                print("ğŸ“± ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ² Telegram")
                            except Exception as e:
                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ: {e}")
                        
                        # ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ¾Ğ¹ Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸
                        print("ğŸ”„ Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ¸Ñ€ÑƒÑ ÑƒĞ¼Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸...")
                        restart_attempts += 1
                        if restart_attempts > max_restart_attempts:
                            print("âŒ Ğ”Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°. ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ.")
                            return
                        
                        # ğŸ›¡ï¸ ĞĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞµĞ¼ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ guided attention
                        if guide_loss is not None and hasattr(guide_loss, 'activate_critical_mode'):
                            guide_loss.activate_critical_mode()
                            print("ğŸ¯ Guided Attention Ğ¿ĞµÑ€ĞµĞ²ĞµĞ´ĞµĞ½ Ğ² ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ²Ğ¾ÑÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ")
                        
                        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ
                        old_learning_rate = hparams.learning_rate
                        old_batch_size = hparams.batch_size
                        
                        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
                        hparams = get_safe_hparams(hparams, restart_attempts)
                        print(f"[Smart Restart] ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ñ Ğ£Ğ›Ğ¬Ğ¢Ğ Ğ-Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸ (Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ° {restart_attempts})...\n")
                        
                        # ğŸ” Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ Ğ² debug reporter
                        if debug_reporter:
                            try:
                                restart_info = f"ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº #{restart_attempts}: learning_rate {old_learning_rate:.8f}â†’{hparams.learning_rate:.8f}, batch_size {old_batch_size}â†’{hparams.batch_size}, Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°: NaN/Inf"
                                debug_reporter.add_restart_info(restart_info)
                                print("ğŸ” Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ° Ğ² Debug Reporter")
                            except Exception as e:
                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ° Ğ² Debug Reporter: {e}")
                        
                        # ğŸ“± TELEGRAM ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¼ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ
                        if telegram_monitor:
                            try:
                                restart_message = f"ğŸ”„ **ĞĞ’Ğ¢ĞĞœĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ ĞŸĞ•Ğ Ğ•Ğ—ĞĞŸĞ£Ğ¡Ğš #{restart_attempts}**\n\n"
                                restart_message += f"ğŸš¨ **ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ğ°:** NaN/Inf Ğ² loss components\n"
                                restart_message += f"ğŸ›¡ï¸ **Ğ”ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ:**\n"
                                restart_message += f"  â€¢ ğŸ”¥ Learning rate: {old_learning_rate:.8f} â†’ {hparams.learning_rate:.8f}\n"
                                restart_message += f"  â€¢ ğŸ“¦ Batch size: {old_batch_size} â†’ {hparams.batch_size}\n"
                                restart_message += f"  â€¢ ğŸ¯ Guided attention ÑƒÑĞ¸Ğ»ĞµĞ½ Ğ´Ğ¾ {getattr(hparams, 'guide_loss_initial_weight', 'Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½')}\n"
                                restart_message += f"  â€¢ âœ‚ï¸ Grad clipping: {hparams.grad_clip_thresh:.4f} (ÑÑ‚Ñ€Ğ¾Ğ¶Ğµ)\n"
                                restart_message += f"  â€¢ ğŸ›¡ï¸ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ guided attention Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½\n"
                                restart_message += f"\nâ° **ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº Ñ‡ĞµÑ€ĞµĞ· 3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹...**"
                                
                                telegram_monitor._send_text_message(restart_message)
                                print("ğŸ“± Ğ£Ğ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ² Telegram")
                            except Exception as e:
                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞµ: {e}")
                        
                        # Ğ”Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ
                        time.sleep(3)
                        break

                if is_main_node:
                    try:
                        duration = time.perf_counter() - start
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ duration: {e}")
                        duration = 0.0
                    try:
                        print(
                            "Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it".format(
                                iteration, reduced_loss, grad_norm, duration
                            )
                        )
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº: {e}")
                        print(f"Train loss {iteration} N/A Grad Norm N/A N/A s/it")

                    # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ learning_rate Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¸Ğ· optimizer (Ğ¼Ğ¾Ğ³ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒÑÑ Ğ°Ğ²Ñ‚Ğ¾-ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ¾Ğ¼)
                    try:
                        learning_rate = optimizer.param_groups[0]["lr"]
                    except (IndexError, KeyError) as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ learning_rate: {e}")
                        learning_rate = hparams.learning_rate

                    # --- EarlyStopController: Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ---
                    if stop_ctrl:
                        try:
                            stop_ctrl.add_metrics(
                                {
                                    "train_loss": reduced_loss,
                                    "grad_norm": grad_norm,
                                    "learning_rate": learning_rate,
                                    "guide_loss": (
                                        reduced_guide_loss
                                        if hparams.use_guided_attn and reduced_guide_loss is not None
                                        else 0.0
                                    ),
                                    "gate_loss": reduced_gate_loss,
                                }
                            )
                        except Exception as e:
                            print(f"âš ï¸ EarlyStopController add_metrics Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")

                    # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² TensorBoard
                    try:
                        writer.add_scalar("training.loss", reduced_loss, iteration)
                        writer.add_scalar("training.taco_loss", reduced_taco_loss, iteration)
                        writer.add_scalar("training.atten_loss", reduced_atten_loss, iteration)
                        writer.add_scalar("training.mi_loss", reduced_mi_loss, iteration)
                        writer.add_scalar("training.guide_loss", reduced_guide_loss if reduced_guide_loss is not None else 0.0, iteration)
                        writer.add_scalar("training.gate_loss", reduced_gate_loss, iteration)
                        writer.add_scalar("training.emb_loss", reduced_emb_loss, iteration)
                        writer.add_scalar("grad.norm", grad_norm, iteration)
                        writer.add_scalar("learning.rate", learning_rate, iteration)
                        writer.add_scalar("duration", duration, iteration)
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² TensorBoard: {e}")
                    if hparams.use_guided_attn and guide_loss is not None:
                        try:
                            writer.add_scalar(
                                "training.guide_loss_weight", guide_loss.get_weight(), iteration
                            )
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ guide_loss weight: {e}")

                    # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² MLflow
                    if MLFLOW_AVAILABLE:
                        training_metrics = {
                            "training.loss": reduced_loss,
                            "training.taco_loss": reduced_taco_loss,
                            "training.atten_loss": reduced_atten_loss,
                            "training.mi_loss": reduced_mi_loss,
                            "training.guide_loss": reduced_guide_loss if reduced_guide_loss is not None else 0.0,
                            "training.gate_loss": reduced_gate_loss,
                            "training.emb_loss": reduced_emb_loss,
                            "grad.norm": grad_norm,
                            "learning.rate": learning_rate,
                            "duration": duration,
                            "batch_size": hparams.batch_size,
                            "learning_rate": learning_rate,
                        }
                        for metric_name, metric_value in training_metrics.items():
                            try:
                                mlflow.log_metric(metric_name, metric_value, step=iteration)
                            except Exception as e:
                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² MLflow Ğ´Ğ»Ñ {metric_name}: {e}")

                    # ğŸ¯ ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ guided attention
                    if guide_loss is not None and hasattr(guide_loss, 'check_diagonality_and_adapt') and y_pred is not None:
                        try:
                            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ alignments Ğ¸Ğ· y_pred
                            alignments = None
                            if len(y_pred) >= 4:
                                alignments = y_pred[3] if len(y_pred) == 4 else y_pred[4]  # Alignments Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ 4-Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚
                            
                            if alignments is not None:
                                guide_loss.check_diagonality_and_adapt(alignments)
                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ´Ğ¸Ğ°Ğ³Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸: {e}")

                    # ğŸ“± Telegram ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 100 ÑˆĞ°Ğ³Ğ¾Ğ²
                    if is_main_node and telegram_monitor:
                        try:
                            if iteration % 100 == 0:
                                print(f"ğŸš€ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Telegram ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ÑˆĞ°Ğ³Ğ° {iteration}")

                                # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Telegram
                                telegram_metrics = {
                                    "loss": reduced_loss,
                                    "train_loss": reduced_loss,  # Ğ”ÑƒĞ±Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
                                    "mel_loss": reduced_taco_loss,
                                    "gate_loss": reduced_gate_loss,
                                    "guide_loss": reduced_guide_loss if reduced_guide_loss is not None else 0.0,
                                    "grad_norm": grad_norm,
                                    "learning_rate": learning_rate,
                                    "epoch": epoch,
                                    "batch_size": hparams.batch_size,
                                    "guide_loss_weight": hparams.guide_loss_weight if hasattr(hparams, 'guide_loss_weight') and hparams.guide_loss_weight is not None else 1.0,
                                    "gate_threshold": hparams.gate_threshold if hasattr(hparams, 'gate_threshold') and hparams.gate_threshold is not None else 0.5,
                                }
                                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ validation loss ĞµÑĞ»Ğ¸ Ğ¾Ğ½ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
                                if last_validation_loss is not None:
                                    telegram_metrics["validation_loss"] = last_validation_loss
                                    telegram_metrics["val_loss"] = last_validation_loss

                                # ğŸ¤– Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ñ€ĞµÑˆĞµĞ½Ğ¸ÑÑ… Smart Tuner
                                smart_tuner_decisions = {}

                                # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ AdvancedQualityController
                                if quality_ctrl:
                                    try:
                                        quality_summary = quality_ctrl.get_quality_summary()
                                        if quality_summary:
                                            smart_tuner_decisions["quality_controller"] = {
                                                "active": True,
                                                "status": "ĞĞ½Ğ°Ğ»Ğ¸Ğ· ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°",
                                                "summary": quality_summary,
                                            }
                                    except Exception as e:
                                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ quality summary: {e}")

                                # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ ParamScheduler
                                if sched_ctrl:
                                    try:
                                        sched_status = sched_ctrl.get_status()
                                        if sched_status:
                                            smart_tuner_decisions["param_scheduler"] = {
                                                "active": True,
                                                "status": sched_status.get(
                                                    "phase", "ĞĞºÑ‚Ğ¸Ğ²ĞµĞ½"
                                                ),
                                                "current_params": sched_status.get(
                                                    "current_params", {}
                                                ),
                                            }
                                    except Exception as e:
                                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ scheduler status: {e}")

                                # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ EarlyStopController
                                if stop_ctrl:
                                    try:
                                        stop_status = stop_ctrl.get_status()
                                        if stop_status:
                                            smart_tuner_decisions[
                                                "early_stop_controller"
                                            ] = {
                                                "active": True,
                                                "status": stop_status.get(
                                                    "status", "ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³"
                                                ),
                                                "patience_remaining": stop_status.get(
                                                    "patience_remaining", "N/A"
                                                ),
                                            }
                                    except Exception as e:
                                        print(
                                            f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ stop controller status: {e}"
                                        )

                                # Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ñ‚ IntelligentEpochOptimizer
                                if optimizer_epochs:
                                    try:
                                        epoch_status = optimizer_epochs.get_status()
                                        if epoch_status:
                                            smart_tuner_decisions["epoch_optimizer"] = {
                                                "active": True,
                                                "status": epoch_status.get(
                                                    "status", "ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"
                                                ),
                                                "recommended_epochs": epoch_status.get(
                                                    "recommended_epochs", "N/A"
                                                ),
                                            }
                                    except Exception as e:
                                        print(
                                            f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ epoch optimizer status: {e}"
                                        )

                                # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
                                param_changes = {}
                                if hasattr(model, "last_param_changes"):
                                    param_changes = model.last_param_changes

                                if param_changes:
                                    smart_tuner_decisions["parameter_changes"] = (
                                        param_changes
                                    )

                                # Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚ Ğ²ÑĞµÑ… ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ¾Ğ²
                                all_recommendations = []
                                if quality_ctrl and hasattr(
                                    quality_ctrl, "get_recommendations"
                                ):
                                    try:
                                        quality_recs = quality_ctrl.get_recommendations()
                                        all_recommendations.extend(quality_recs)
                                    except Exception:
                                        pass

                                if sched_ctrl and hasattr(
                                    sched_ctrl, "get_recommendations"
                                ):
                                    try:
                                        sched_recs = sched_ctrl.get_recommendations()
                                        all_recommendations.extend(sched_recs)
                                    except Exception:
                                        pass

                                if all_recommendations:
                                    smart_tuner_decisions["recommendations"] = (
                                        all_recommendations[:3]
                                    )  # Ğ”Ğ¾ 3 Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¹

                                # ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ
                                warnings = []
                                if reduced_loss > 5.0:
                                    warnings.append(
                                        "Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ loss - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸ĞµĞ¼"
                                    )
                                if grad_norm > 10.0:
                                    warnings.append(
                                        "Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ grad_norm - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶ĞµĞ½ Ğ²Ğ·Ñ€Ñ‹Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²"
                                    )
                                if learning_rate > 0.01:
                                    warnings.append(
                                        "Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ learning rate - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ° Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"
                                    )

                                if warnings:
                                    smart_tuner_decisions["warnings"] = warnings

                                print(f"   - smart_tuner_decisions: {smart_tuner_decisions}")

                                # ğŸ” Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ DEBUG REPORTER
                                if debug_reporter:
                                    try:
                                        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ loss ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸
                                        loss_components = {
                                            'total_loss': reduced_loss,
                                            'mel_loss': reduced_taco_loss if 'reduced_taco_loss' in locals() else 0.0,
                                            'gate_loss': reduced_gate_loss if 'reduced_gate_loss' in locals() else 0.0,
                                        }
                                        
                                        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ guided attention loss ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
                                        if guide_loss is not None and y_pred is not None:
                                            try:
                                                if len(y_pred) >= 4:
                                                    alignments = y_pred[3] if len(y_pred) == 4 else y_pred[4]
                                                    if alignments is not None:
                                                        # ğŸ”¥ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ guided loss
                                                        guided_loss_result = guide_loss(y_pred)  # ĞŸĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ Ğ²ĞµÑÑŒ y_pred
                                                        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
                                                        if isinstance(guided_loss_result, tuple):
                                                            guided_loss_val = guided_loss_result[0]
                                                        else:
                                                            guided_loss_val = guided_loss_result
                                                        loss_components['guided_loss'] = guided_loss_val.item()
                                            except Exception as e:
                                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ guided loss Ğ´Ğ»Ñ debug: {e}")
                                        
                                        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ MMI loss ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
                                        if mmi_loss is not None and y_pred is not None:
                                            try:
                                                # Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° y_pred Ğ´Ğ»Ñ MMI
                                                if isinstance(y_pred, (list, tuple)) and len(y_pred) > 1:
                                                    mel_outputs = y_pred[1]
                                                    if hasattr(mel_outputs, 'shape'):  # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€
                                                        # ğŸ”¥ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñƒ
                                                        mel_outputs = mel_outputs.float()  # ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº float32
                                                        mel_target = y[0].float()  # ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº float32
                                                        mmi_loss_val = mmi_loss(mel_outputs, mel_target)
                                                        loss_components['mmi_loss'] = mmi_loss_val.item()
                                            except Exception as e:
                                                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ MMI loss Ğ´Ğ»Ñ debug: {e}")
                                        
                                        # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
                                        debug_metrics = {
                                            'loss': reduced_loss,
                                            'grad_norm': grad_norm,
                                            'learning_rate': learning_rate,
                                            'batch_size': hparams.batch_size,
                                            'iteration': iteration,
                                            'epoch': epoch,
                                            'diagonality': 0.0,  # Ğ‘ÑƒĞ´ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¾ Ğ² debug_reporter
                                            'quality': 0.0,      # Ğ‘ÑƒĞ´ĞµÑ‚ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¾ Ğ² debug_reporter
                                        }
                                        
                                        # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² debug reporter
                                        debug_reporter.collect_step_data(
                                            step=iteration,
                                            metrics=debug_metrics,
                                            model=model,
                                            y_pred=y_pred,
                                            loss_components=loss_components,
                                            hparams=hparams,
                                            smart_tuner_decisions=smart_tuner_decisions
                                        )
                                        
                                    except Exception as e:
                                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ±Ğ¾Ñ€Ğ° debug Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {e}")

                                # Ğ“ĞĞ ĞĞĞ¢Ğ˜Ğ ĞĞ’ĞĞĞĞ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ³Ñ€Ğ°Ñ„Ğ¸ĞºĞ¸ (send_plots=True)
                                try:
                                    result = telegram_monitor.send_training_update(
                                        step=iteration,
                                        metrics=telegram_metrics,
                                        smart_tuner_decisions=smart_tuner_decisions
                                    )
                                except Exception as e:
                                    print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Telegram ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ: {e}")
                                    result = False
                                print(f"ğŸ“± Telegram ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ {'Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞ' if result else 'ĞĞ•'} Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ´Ğ»Ñ ÑˆĞ°Ğ³Ğ° {iteration}")

                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Telegram ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ: {e}")
                            import traceback
                            print(f"   Traceback: {traceback.format_exc()}")

                    # --- ĞÑ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ° Ğ°ÑƒĞ´Ğ¸Ğ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 500 ÑˆĞ°Ğ³Ğ¾Ğ² ---
                    if is_main_node and telegram_monitor:
                        try:
                            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ»Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 500 ÑˆĞ°Ğ³Ğ¾Ğ²)
                            if iteration % 500 == 0 and iteration != 0:
                                print(f"ğŸµ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ğ´Ğ»Ñ ÑˆĞ°Ğ³Ğ° {iteration}")

                                # Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñ ÑƒĞ´Ğ°Ñ€ĞµĞ½Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ Tacotron2
                                test_text = "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞšĞ°Ğº Ğ´ĞµĞ»Ğ°? Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½Ğ°Ñ Ğ¿Ğ¾Ğ³Ğ¾Ğ´Ğ° Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ³ÑƒĞ»ĞºĞ¸ Ğ² Ğ¿Ğ°Ñ€ĞºĞµ."

                                # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾
                                try:
                                    audio_result = (
                                        telegram_monitor.generate_and_send_test_audio(
                                            model=model, step=iteration, test_text=test_text
                                        )
                                    )
                                except Exception as e:
                                    print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾: {e}")
                                    audio_result = False

                                if audio_result:
                                    print(
                                        f"âœ… ĞÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ´Ğ»Ñ ÑˆĞ°Ğ³Ğ° {iteration}"
                                    )
                                    last_audio_step = iteration
                                else:
                                    print(
                                        f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ´Ğ»Ñ ÑˆĞ°Ğ³Ğ° {iteration}"
                                    )

                        except Exception as e:
                            print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾: {e}")
                            import traceback
                            print(f"   Traceback: {traceback.format_exc()}")

                if iteration % hparams.validation_freq == 0:
                    print(f"ğŸ” Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ {iteration}")
                    try:
                        val_loss = validate(
                            model,
                            criterion,
                            valset,
                            iteration,
                            hparams.batch_size,
                            n_gpus,
                            collate_fn,
                            writer,
                            hparams.distributed_run,
                            rank,
                        )
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸: {e}")
                        val_loss = float('inf')
                    print(f"ğŸ“Š Validation loss: {val_loss}")

                    # Auto hyper-parameter tuning (on main node)
                    if is_main_node and quality_ctrl:
                        # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
                        align_score = getattr(
                            model, "last_validation_alignment_score", None
                        )
                        metrics_dict = {
                            "val_loss": val_loss,
                            "attention_alignment_score": (
                                align_score if align_score is not None else 0.0
                            ),
                        }
                        attention_w = getattr(model, "last_validation_alignments", None)
                        gate_out = getattr(model, "last_validation_gate_outputs", None)
                        mel_out = getattr(model, "last_validation_mel_outputs", None)
                        try:
                            analysis = quality_ctrl.analyze_training_quality(
                                epoch=iteration,
                                metrics=metrics_dict,
                                attention_weights=attention_w,
                                gate_outputs=gate_out,
                                mel_outputs=mel_out,
                            )
                            for intrv in analysis.get("recommended_interventions", []):
                                new_hp = quality_ctrl.apply_quality_intervention(
                                    intrv, vars(hparams), step=iteration
                                )
                                # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñƒ hparams Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
                                for k, v in new_hp.items():
                                    if hasattr(hparams, k):
                                        old_value = getattr(hparams, k)
                                        setattr(hparams, k, v)
                                        # ĞÑ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°
                                        quality_ctrl.track_parameter_change(
                                            k,
                                            old_value,
                                            v,
                                            f"Quality intervention: {intrv.get('type', 'unknown')}",
                                            iteration,
                                        )
                                    if (
                                        k in ["guide_loss_weight", "guided_attn_weight"]
                                        and hparams.use_guided_attn
                                    ):
                                        guide_loss.alpha = v
                                        guide_loss.current_weight = v
                                    # Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ±Ğ° Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² hparams
                                    setattr(hparams, "guided_attn_weight", v)
                                    setattr(hparams, "guide_loss_weight", v)
                                    if k == "learning_rate":
                                        for g in optimizer.param_groups:
                                            g["lr"] = v
                                    if k == "gate_threshold" and hasattr(
                                        model.decoder, "gate_threshold"
                                    ):
                                        model.decoder.gate_threshold = v

                                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Telegram
                                if not hasattr(model, "last_param_changes"):
                                    model.last_param_changes = {}

                                for k, v in new_hp.items():
                                    if hasattr(hparams, k):
                                        old_value = getattr(hparams, k)
                                        model.last_param_changes[k] = {
                                            "old_value": old_value,
                                            "new_value": v,
                                            "reason": f"Quality intervention: {intrv.get('type', 'unknown')}",
                                        }
                        except Exception as e:
                            print(f"âš ï¸ AdvancedQualityController Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")

                    # --- EarlyStopController: Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ---
                    if is_main_node and stop_ctrl:
                        try:
                            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ validation Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
                            align_score = getattr(
                                model, "last_validation_alignment_score", None
                            )
                            stop_ctrl.add_metrics(
                                {
                                    "val_loss": val_loss,
                                    "attention_alignment_score": (
                                        align_score if align_score is not None else 0.0
                                    ),
                                }
                            )

                            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ°
                            decision = stop_ctrl.decide_next_step(vars(hparams))
                            if decision["action"] == "apply_patch":
                                hparams_dict = decision["new_hparams"]
                                for k, v in hparams_dict.items():
                                    if hasattr(hparams, k):
                                        setattr(hparams, k, v)
                                    if k == "learning_rate":
                                        for g in optimizer.param_groups:
                                            g["lr"] = v
                                    if (
                                        k in ["guide_loss_weight", "guided_attn_weight"]
                                        and hparams.use_guided_attn
                                    ):
                                        guide_loss.alpha = v
                                        guide_loss.current_weight = v
                                print(
                                    f"ğŸ›   EarlyStop/Rescue Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ğ» Ğ¿Ğ°Ñ‚Ñ‡: {decision['reason']}"
                                )

                            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€Ğ°Ğ½Ğ½Ğ¸Ğ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²
                            should_stop, reason = stop_ctrl.should_stop_early(
                                {"val_loss": val_loss}
                            )
                            if should_stop:
                                print(f"ğŸŸ¥ Ğ Ğ°Ğ½Ğ½Ğ¸Ğ¹ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²: {reason}")
                                return {
                                    "validation_loss": val_loss,
                                    "iteration": iteration,
                                    "checkpoint_path": None,
                                    "early_stop_reason": reason,
                                }
                        except Exception as e:
                            print(f"âš ï¸ EarlyStopController Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}")

                if is_main_node and (iteration % hparams.iters_per_checkpoint == 0):
                    checkpoint_path = os.path.join(
                        output_directory, "checkpoint_{}".format(iteration)
                    )
                    try:
                        save_checkpoint(
                            model, optimizer, learning_rate, iteration, checkpoint_path
                        )
                    except Exception as e:
                        print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°: {e}")

                iteration += 1
            else:
                continue
            break
        else:
            # Ğ•ÑĞ»Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ¾ÑÑŒ Ğ±ĞµĞ· ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
            break

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Smart Tuner
    if is_main_node:
        print(f"ğŸ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ Ğ¿Ğ¾ÑĞ»Ğµ {iteration} Ğ¸Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹")
        val_loss = validate(
            model,
            criterion,
            valset,
            iteration,
            hparams.batch_size,
            n_gpus,
            collate_fn,
            writer,
            hparams.distributed_run,
            rank,
        )

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ checkpoint
        final_checkpoint_path = os.path.join(
            output_directory, f"checkpoint_final_{iteration}"
        )
        save_checkpoint(
            model, optimizer, learning_rate, iteration, final_checkpoint_path
        )

        final_metrics = {
            "validation_loss": val_loss,
            "iteration": iteration,
            "checkpoint_path": final_checkpoint_path,
        }
        print(f"ğŸ“Š Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸: {final_metrics}")

        # --- Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ²Ğ¾Ğ´ĞºĞ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ»ĞµÑ€Ğ¾Ğ² ---
        if is_main_node:
            try:
                if quality_ctrl:
                    quality_summary = quality_ctrl.get_quality_summary()
                    print(f"ğŸ¯ Quality Summary: {quality_summary}")

                if stop_ctrl:
                    stop_summary = stop_ctrl.get_tts_training_summary()
                    print(f"ğŸ›‘ EarlyStop Summary: {stop_summary}")

                if optimizer_epochs:
                    epoch_summary = optimizer_epochs.get_optimization_summary()
                    print(f"ğŸ“ˆ Epoch Optimization Summary: {epoch_summary}")
            except Exception as e:
                print(f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ²Ğ¾Ğ´Ğ¾Ğº: {e}")

        if writer:
            writer.close()
        return final_metrics

    return None


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-o", "--output_directory", type=str, help="directory to save checkpoints"
    )
    parser.add_argument(
        "-l", "--log_directory", type=str, help="directory to save tensorboard logs"
    )
    parser.add_argument(
        "-c",
        "--checkpoint_path",
        type=str,
        default=None,
        required=False,
        help="checkpoint path",
    )
    parser.add_argument(
        "--warm-start",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument(
        "--ignore-mmi-layers",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument(
        "--ignore-gst-layers",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument(
        "--ignore-tsgst-layers",
        action="store_true",
        help="load model weights only, ignore specified layers",
    )
    parser.add_argument("--no-dga", action="store_true", help="do not use DGA")
    parser.add_argument(
        "--n_gpus", type=int, default=1, required=False, help="number of gpus"
    )
    parser.add_argument(
        "--rank", type=int, default=0, required=False, help="rank of current gpu"
    )
    parser.add_argument(
        "--group_name",
        type=str,
        default="group_name",
        required=False,
        help="Distributed group name",
    )
    parser.add_argument(
        "--hparams", type=str, required=False, help="comma separated name=value pairs"
    )

    args = parser.parse_args()
    hparams = create_hparams(args.hparams)
    hparams.no_dga = True

    torch.backends.cudnn.enabled = hparams.cudnn_enabled
    torch.backends.cudnn.benchmark = hparams.cudnn_benchmark

    print("FP16 Run:", hparams.fp16_run)
    print("Dynamic Loss Scaling:", hparams.dynamic_loss_scaling)
    print("Use MMI:", hparams.use_mmi)
    print("Distributed Run:", hparams.distributed_run)
    print("cuDNN Enabled:", hparams.cudnn_enabled)
    print("cuDNN Benchmark:", hparams.cudnn_benchmark)

    train(
        args.output_directory,
        args.log_directory,
        args.checkpoint_path,
        args.warm_start,
        args.ignore_mmi_layers,
        args.ignore_gst_layers,
        args.ignore_tsgst_layers,
        args.n_gpus,
        args.rank,
        args.group_name,
        hparams,
    )
