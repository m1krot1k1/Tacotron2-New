#!/usr/bin/env python3
"""
üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –î–õ–Ø TACOTRON2-NEW
–ê–≤—Ç–æ—Ä: AI Assistant –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ Intelligent TTS Training Pipeline

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º:
1. –í–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (400k+ ‚Üí <10)
2. –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ guided attention loss
3. –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è alignment diagnostics
4. –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π gradient clipping
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# 1. –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï GRADIENT CLIPPING (–ö–†–ò–¢–ò–ß–ù–û)
# ============================================================================

class AdaptiveGradientClipper:
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π gradient clipper –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
    –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è Tacotron2.
    """
    
    def __init__(self, max_norm: float = 1.0):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ gradient clipper.
        
        Args:
            max_norm: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (1.0 –¥–ª—è Tacotron2)
        """
        self.max_norm = max_norm
        
    def clip_gradients(self, model: nn.Module) -> float:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–ª–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤."""
        return torch.nn.utils.clip_grad_norm_(model.parameters(), self.max_norm)

# ============================================================================
# 2. GUIDED ATTENTION LOSS (–ö–†–ò–¢–ò–ß–ù–û)
# ============================================================================

class GuidedAttentionLoss(nn.Module):
    """
    Guided Attention Loss –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è.
    –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º —Å attention mechanism.
    """
    
    def __init__(self, sigma: float = 0.4):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Guided Attention Loss.
        
        Args:
            sigma: –®–∏—Ä–∏–Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–π –º–∞—Å–∫–∏ (0.4 –¥–ª—è Tacotron2)
        """
        super().__init__()
        self.sigma = sigma
        
    def forward(self, attention_weights, input_lengths, output_lengths):
        batch_size, max_time, encoder_steps = attention_weights.size()
        guided_mask = torch.zeros_like(attention_weights)
        
        for b in range(batch_size):
            in_len = input_lengths[b].item()
            out_len = output_lengths[b].item()
            
            for i in range(min(out_len, max_time)):
                for j in range(min(in_len, encoder_steps)):
                    ideal_j = int((i / out_len) * in_len)
                    distance = abs(j - ideal_j)
                    guided_mask[b, i, j] = torch.exp(-(distance ** 2) / (2 * self.sigma ** 2))
        
        return torch.mean(attention_weights * guided_mask)

# ============================================================================
# 3. ALIGNMENT DIAGNOSTICS –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø (–ö–†–ò–¢–ò–ß–ù–û)
# ============================================================================

class AlignmentDiagnostics:
    """
    –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ alignment –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ training loop.
    –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ attention –∫–∞—á–µ—Å—Ç–≤–∞.
    """
    
    def __init__(self, log_interval: int = 100):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Alignment Diagnostics.
        
        Args:
            log_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫
        """
        self.log_interval = log_interval
        self.diagnostics_history = []
        
    def compute_alignment_metrics(self, attention_weights: torch.Tensor,
                                input_lengths: torch.Tensor,
                                output_lengths: torch.Tensor) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ alignment.
        
        Args:
            attention_weights: [batch_size, max_time, encoder_steps]
            input_lengths: [batch_size]
            output_lengths: [batch_size]
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ alignment
        """
        batch_size = attention_weights.size(0)
        metrics = {
            'diagonality': 0.0,
            'coverage': 0.0,
            'focus': 0.0,
            'monotonicity': 0.0
        }
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞
            batch_metrics = []
            for b in range(batch_size):
                in_len = input_lengths[b].item()
                out_len = output_lengths[b].item()
                
                # –ë–µ—Ä–µ–º attention –¥–ª—è —ç—Ç–æ–≥–æ –±–∞—Ç—á–∞
                attn = attention_weights[b, :out_len, :in_len].detach().cpu().numpy()
                
                # 1. –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
                diagonality = self._compute_diagonality(attn)
                
                # 2. –ü–æ–∫—Ä—ã—Ç–∏–µ
                coverage = self._compute_coverage(attn)
                
                # 3. –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞
                focus = self._compute_focus(attn)
                
                # 4. –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å
                monotonicity = self._compute_monotonicity(attn)
                
                batch_metrics.append({
                    'diagonality': diagonality,
                    'coverage': coverage,
                    'focus': focus,
                    'monotonicity': monotonicity
                })
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –±–∞—Ç—á—É
            for key in metrics:
                metrics[key] = np.mean([bm[key] for bm in batch_metrics])
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è alignment –º–µ—Ç—Ä–∏–∫: {e}")
            
        return metrics
    
    def _compute_diagonality(self, attention: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–µ–ø–µ–Ω—å –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ attention."""
        H, W = attention.shape
        if H == 0 or W == 0:
            return 0.0
            
        diagonal_sum = 0.0
        total_sum = np.sum(attention)
        
        if total_sum == 0:
            return 0.0
        
        # –°—É–º–º–∏—Ä—É–µ–º –≤–µ—Å–∞ –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
        for i in range(H):
            diag_pos = int((i / H) * W)
            for j in range(max(0, diag_pos-2), min(W, diag_pos+3)):
                diagonal_sum += attention[i, j]
        
        return diagonal_sum / total_sum
    
    def _compute_coverage(self, attention: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        H, W = attention.shape
        if W == 0:
            return 0.0
            
        # –°—É–º–º–∏—Ä—É–µ–º attention –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        coverage = np.sum(attention, axis=0)
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        coverage = coverage / np.max(coverage) if np.max(coverage) > 0 else coverage
        return np.mean(coverage)
    
    def _compute_focus(self, attention: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–µ–ø–µ–Ω—å —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ attention."""
        H, W = attention.shape
        if H == 0:
            return 0.0
            
        focus_scores = []
        for i in range(H):
            row = attention[i]
            if np.sum(row) > 0:
                # –≠–Ω—Ç—Ä–æ–ø–∏—è —Å—Ç—Ä–æ–∫–∏ (–Ω–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è = –≤—ã—Å–æ–∫–∞—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞)
                row_normalized = row / np.sum(row)
                entropy = -np.sum(row_normalized * np.log(row_normalized + 1e-8))
                max_entropy = np.log(len(row))
                focus_score = 1.0 - (entropy / max_entropy)
                focus_scores.append(focus_score)
        
        return np.mean(focus_scores) if focus_scores else 0.0
    
    def _compute_monotonicity(self, attention: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–µ–ø–µ–Ω—å –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏ attention."""
        H, W = attention.shape
        if H <= 1:
            return 1.0
            
        monotonic_violations = 0
        total_transitions = 0
        
        prev_peak = 0
        for i in range(1, H):
            current_peak = np.argmax(attention[i])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å
            if current_peak < prev_peak:
                monotonic_violations += 1
            
            prev_peak = current_peak
            total_transitions += 1
        
        if total_transitions == 0:
            return 1.0
            
        return 1.0 - (monotonic_violations / total_transitions)
    
    def log_metrics(self, metrics: Dict[str, float], step: int, mlflow_logger=None):
        """
        –õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ alignment.
        
        Args:
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ alignment
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞
            mlflow_logger: MLflow logger –¥–ª—è –∑–∞–ø–∏—Å–∏ –º–µ—Ç—Ä–∏–∫
        """
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.diagnostics_history.append({
            'step': step,
            'metrics': metrics
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.diagnostics_history) > 1000:
            self.diagnostics_history.pop(0)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤ MLflow
        if mlflow_logger:
            try:
                mlflow_logger.log_metrics({
                    'alignment/diagonality': metrics['diagonality'],
                    'alignment/coverage': metrics['coverage'],
                    'alignment/focus': metrics['focus'],
                    'alignment/monotonicity': metrics['monotonicity']
                }, step=step)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLflow: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        if metrics['diagonality'] < 0.3:
            logger.warning(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention: {metrics['diagonality']:.3f}")
        if metrics['monotonicity'] < 0.5:
            logger.warning(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å attention: {metrics['monotonicity']:.3f}")

# ============================================================================
# 4. –ü–†–ê–í–ò–õ–¨–ù–´–ï –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–´ (–ö–†–ò–¢–ò–ß–ù–û)
# ============================================================================

class Tacotron2Hyperparams:
    """
    –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Tacotron2.
    –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.
    """
    
    @staticmethod
    def get_stable_hyperparams() -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Tacotron2.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        return {
            'learning_rate': 1e-4,
            'gradient_clip_threshold': 1.0,
            'batch_size': 16,
            'guided_attention_weight': 1.0,
            'use_guided_attn': True
        }
    
    @staticmethod
    def get_emergency_hyperparams() -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        return {
            'learning_rate': 1e-5,
            'gradient_clip_threshold': 0.1,
            'batch_size': 1,
            'guided_attention_weight': 10.0,
            'use_guided_attn': True
        }

# ============================================================================
# 5. –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –í TRAINING LOOP (–ö–†–ò–¢–ò–ß–ù–û)
# ============================================================================

def integrate_critical_fixes_in_training_loop():
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ training loop.
    –≠—Ç–æ—Ç –∫–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω –≤ train.py.
    """
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    gradient_clipper = AdaptiveGradientClipper(max_norm=1.0)
    guided_attention_loss = GuidedAttentionLoss(sigma=0.4)
    alignment_diagnostics = AlignmentDiagnostics(log_interval=100)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    hyperparams = Tacotron2Hyperparams.get_stable_hyperparams()
    
    # –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ training loop:
    """
    # –í training loop (train.py):
    
    for epoch in range(epochs):
        for i, batch in enumerate(train_loader):
            # Forward pass
            y_pred = model(x)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
            loss_taco, loss_gate, loss_atten, loss_emb = criterion(y_pred, y)
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Guided Attention Loss
            if hyperparams['use_guided_attn']:
                loss_guide = guided_attention_loss(
                    y_pred[3],  # attention weights
                    input_lengths,
                    output_lengths
                )
            else:
                loss_guide = torch.tensor(0.0, device=device)
            
            # –û–±—â–∏–π loss
            total_loss = (
                hyperparams['mel_loss_weight'] * loss_taco +
                hyperparams['gate_loss_weight'] * loss_gate +
                hyperparams['guided_attention_weight'] * loss_guide +
                loss_atten + loss_emb
            )
            
            # Backward pass
            total_loss.backward()
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Gradient Clipping
            grad_norm = gradient_clipper.clip_gradients(model)
            
            # Optimizer step
            optimizer.step()
            optimizer.zero_grad()
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Alignment Diagnostics
            if i % alignment_diagnostics.log_interval == 0:
                alignment_metrics = alignment_diagnostics.compute_alignment_metrics(
                    y_pred[3], input_lengths, output_lengths
                )
                alignment_diagnostics.log_metrics(alignment_metrics, i, mlflow_logger)
                
                # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–µ—Ä—Ç—ã
                if alignment_metrics['diagonality'] < 0.3:
                    logger.warning("üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention!")
    """

# ============================================================================
# 6. TELEGRAM –£–í–ï–î–û–ú–õ–ï–ù–ò–Ø –° –ö–û–ù–ö–†–ï–¢–ù–´–ú–ò –î–ï–ô–°–¢–í–ò–Ø–ú–ò
# ============================================================================

def send_detailed_telegram_alert(step: int, metrics: Dict[str, float], 
                               grad_norm: float, actions_taken: List[str]):
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–µ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–µ–π—Å—Ç–≤–∏—è–º–∏.
    
    Args:
        step: –ù–æ–º–µ—Ä —à–∞–≥–∞
        metrics: –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        grad_norm: –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        actions_taken: –°–ø–∏—Å–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
    """
    message = f"ü§ñ **Smart Tuner V2 - –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç**\n\n"
    message += f"üìä **–®–∞–≥:** {step}\n"
    message += f"üî• **Gradient Norm:** {grad_norm:.2f}\n"
    message += f"üìà **Loss:** {metrics.get('loss', 'N/A'):.4f}\n"
    message += f"üéØ **Attention Diagonality:** {metrics.get('diagonality', 'N/A'):.3f}\n"
    message += f"üéØ **Attention Coverage:** {metrics.get('coverage', 'N/A'):.3f}\n"
    message += f"üéØ **Gate Accuracy:** {metrics.get('gate_accuracy', 'N/A'):.3f}\n\n"
    
    message += f"üõ†Ô∏è **–í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:**\n"
    for action in actions_taken:
        message += f"  ‚Ä¢ {action}\n"
    
    message += f"\nüìã **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n"
    
    if grad_norm > 10.0:
        message += f"  ‚Ä¢ üî• –°–Ω–∏–∑–∏—Ç—å learning rate –Ω–∞ 50%\n"
        message += f"  ‚Ä¢ ‚úÇÔ∏è –£—Å–∏–ª–∏—Ç—å gradient clipping –¥–æ 0.5\n"
    
    if metrics.get('diagonality', 1.0) < 0.3:
        message += f"  ‚Ä¢ üéØ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º guided attention\n"
        message += f"  ‚Ä¢ üìä –£–≤–µ–ª–∏—á–∏—Ç—å guided attention weight –¥–æ 10.0\n"
    
    if metrics.get('loss', 0.0) > 50.0:
        message += f"  ‚Ä¢ üì¶ –£–º–µ–Ω—å—à–∏—Ç—å batch size\n"
        message += f"  ‚Ä¢ üéõÔ∏è –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö\n"
    
    # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–¥ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram
    print(f"üì± TELEGRAM ALERT:\n{message}")

# ============================================================================
# 7. –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô
# ============================================================================

def apply_critical_fixes():
    """
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω–∞ –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è.
    """
    logger.info("üö® –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è Tacotron2...")
    
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    gradient_clipper = AdaptiveGradientClipper(max_norm=1.0)
    guided_attention_loss = GuidedAttentionLoss(sigma=0.4)
    alignment_diagnostics = AlignmentDiagnostics(log_interval=100)
    
    # 2. –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    hyperparams = Tacotron2Hyperparams.get_stable_hyperparams()
    
    # 3. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    logger.info("‚úÖ Gradient Clipper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (max_norm=1.0)")
    logger.info("‚úÖ Guided Attention Loss –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    logger.info("‚úÖ Alignment Diagnostics –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    logger.info(f"‚úÖ –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω—ã: lr={hyperparams['learning_rate']:.2e}")
    
    return {
        'gradient_clipper': gradient_clipper,
        'guided_attention_loss': guided_attention_loss,
        'alignment_diagnostics': alignment_diagnostics,
        'hyperparams': hyperparams
    }

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    fixes = apply_critical_fixes()
    print("‚úÖ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã!")
    print(f"üìä –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {fixes['hyperparams']}") 