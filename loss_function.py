from torch import nn
import torch
import numpy as np
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any
import math


class SpectralMelLoss(nn.Module):
    """
    üéµ –£–ª—É—á—à–µ–Ω–Ω–∞—è Mel Loss —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    """
    def __init__(self, n_mel_channels=80, sample_rate=22050):
        super(SpectralMelLoss, self).__init__()
        self.n_mel_channels = n_mel_channels
        self.sample_rate = sample_rate
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
        mel_weights = torch.ones(n_mel_channels)
        # –ë–æ–ª—å—à–∏–π –≤–µ—Å –Ω–∞ —Å—Ä–µ–¥–Ω–∏—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö (–≤–∞–∂–Ω—ã—Ö –¥–ª—è –≥–æ–ª–æ—Å–∞)
        mid_range = slice(n_mel_channels//4, 3*n_mel_channels//4)
        mel_weights[mid_range] *= 1.5
        # –ú–µ–Ω—å—à–∏–π –≤–µ—Å –Ω–∞ –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö
        high_range = slice(3*n_mel_channels//4, n_mel_channels)
        mel_weights[high_range] *= 0.8
        
        self.register_buffer('mel_weights', mel_weights)
        
    def forward(self, mel_pred, mel_target):
        # –û—Å–Ω–æ–≤–Ω–æ–π MSE loss —Å –≤–µ—Å–∞–º–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–∞–º
        weighted_mse = F.mse_loss(mel_pred * self.mel_weights[None, :, None], 
                                  mel_target * self.mel_weights[None, :, None])
        
        # –î–æ–±–∞–≤–ª—è–µ–º L1 loss –¥–ª—è —Ä–µ–∑–∫–æ—Å—Ç–∏
        l1_loss = F.l1_loss(mel_pred, mel_target)
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π loss (—Ä–∞–∑–Ω–æ—Å—Ç–∏ —Å–æ—Å–µ–¥–Ω–∏—Ö —Ñ—Ä–µ–π–º–æ–≤)
        mel_pred_diff = mel_pred[:, :, 1:] - mel_pred[:, :, :-1]
        mel_target_diff = mel_target[:, :, 1:] - mel_target[:, :, :-1]
        spectral_loss = F.mse_loss(mel_pred_diff, mel_target_diff)
        
        return weighted_mse + 0.3 * l1_loss + 0.2 * spectral_loss


class AdaptiveGateLoss(nn.Module):
    """
    üö™ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è Gate Loss —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –≤–µ—Å–æ–º
    """
    def __init__(self, initial_weight=1.3, min_weight=0.8, max_weight=2.0):
        super(AdaptiveGateLoss, self).__init__()
        self.initial_weight = initial_weight
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.current_weight = initial_weight
        
    def update_weight(self, gate_accuracy):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π gate accuracy"""
        if gate_accuracy < 0.5:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –µ—Å–ª–∏ accuracy –Ω–∏–∑–∫–∞—è
            self.current_weight = min(self.max_weight, self.current_weight * 1.1)
        elif gate_accuracy > 0.8:
            # –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å –µ—Å–ª–∏ accuracy –≤—ã—Å–æ–∫–∞—è
            self.current_weight = max(self.min_weight, self.current_weight * 0.95)
            
    def forward(self, gate_pred, gate_target):
        return self.current_weight * F.binary_cross_entropy_with_logits(gate_pred, gate_target)


class Tacotron2Loss(nn.Module):
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ Loss –¥–ª—è Tacotron2 –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.
    
    –í–∫–ª—é—á–∞–µ—Ç:
    1. –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô Guided Attention Loss (MonoAlign 2024)
    2. Spectral Mel Loss –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ (Llasa 2025)  
    3. Adaptive Gate Loss (Very Attentive Tacotron 2025)
    4. Perceptual Loss –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    5. Style Loss –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞ –≥–æ–ª–æ—Å–∞
    6. Monotonic Alignment Loss –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, hparams):
        super(Tacotron2Loss, self).__init__()
        self.hparams = hparams
        
        # üìä –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –≤–µ—Å–∞ loss —Ñ—É–Ω–∫—Ü–∏–π (–∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π)
        self.mel_loss_weight = getattr(hparams, 'mel_loss_weight', 1.0)
        self.gate_loss_weight = getattr(hparams, 'gate_loss_weight', 1.0)
        self.guide_loss_weight = getattr(hparams, 'guide_loss_weight', 2.0)  # –£–≤–µ–ª–∏—á–µ–Ω–æ
        
        # üéµ –ù–û–í–´–ï –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏
        self.spectral_loss_weight = getattr(hparams, 'spectral_loss_weight', 0.3)
        self.perceptual_loss_weight = getattr(hparams, 'perceptual_loss_weight', 0.2)
        self.style_loss_weight = getattr(hparams, 'style_loss_weight', 0.1)
        self.monotonic_loss_weight = getattr(hparams, 'monotonic_loss_weight', 0.1)
        
        # Guided attention –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï)
        self.guide_decay = getattr(hparams, 'guide_decay', 0.9999)  # –ú–µ–¥–ª–µ–Ω–Ω–µ–µ decay
        self.guide_sigma = getattr(hparams, 'guide_sigma', 0.4)      # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è sigma
        
        # Adaptive parameters
        self.adaptive_gate_threshold = getattr(hparams, 'adaptive_gate_threshold', True)
        self.curriculum_teacher_forcing = getattr(hparams, 'curriculum_teacher_forcing', True)
        
        # –°—á–µ—Ç—á–∏–∫ —à–∞–≥–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö loss
        self.global_step = 0
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö loss —Ñ—É–Ω–∫—Ü–∏–π
        self.spectral_mel_loss = SpectralMelLoss()
        self.adaptive_gate_loss = AdaptiveGateLoss()
        self.perceptual_loss = PerceptualLoss()
        self.style_loss = StyleLoss()
        self.monotonic_alignment_loss = MonotonicAlignmentLoss()
        
    def forward(self, model_output, targets, attention_weights=None, gate_outputs=None):
        """
        –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π forward pass —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ loss —Ñ—É–Ω–∫—Ü–∏—è–º–∏.
        """
        mel_target, gate_target = targets[0], targets[1]
        mel_target.requires_grad = False
        gate_target.requires_grad = False
        gate_target = gate_target.view(-1, 1)

        mel_out, mel_out_postnet, gate_out, alignments = model_output
        gate_out = gate_out.view(-1, 1)
        
        # üéØ 1. –û–°–ù–û–í–ù–´–ï LOSS –§–£–ù–ö–¶–ò–ò
        
        # Mel loss (–æ—Å–Ω–æ–≤–∞ –∫–∞—á–µ—Å—Ç–≤–∞)
        mel_loss = nn.MSELoss()(mel_out, mel_target) + \
                   nn.MSELoss()(mel_out_postnet, mel_target)
        
        # Gate loss (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π)
        gate_loss = self.adaptive_gate_loss(gate_out, gate_target, self.global_step)
        
        # üîÑ 2. GUIDED ATTENTION LOSS (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∏–∑ MonoAlign)
        guide_loss = 0.0
        if attention_weights is not None and self.guide_loss_weight > 0:
            guide_loss = self.guided_attention_loss(
                attention_weights, 
                mel_target.size(2), 
                mel_out.size(1)
            )
        
        # üéµ 3. –ü–†–û–î–í–ò–ù–£–¢–´–ï LOSS –§–£–ù–ö–¶–ò–ò
        
        # Spectral Mel Loss –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞—Å—Ç–æ—Ç
        spectral_loss = self.spectral_mel_loss(mel_out_postnet, mel_target)
        
        # Perceptual Loss –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è
        perceptual_loss = self.perceptual_loss(mel_out_postnet, mel_target)
        
        # Style Loss –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞ –≥–æ–ª–æ—Å–∞
        style_loss = self.style_loss(mel_out_postnet, mel_target)
        
        # Monotonic Alignment Loss –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        monotonic_loss = 0.0
        if attention_weights is not None:
            monotonic_loss = self.monotonic_alignment_loss(attention_weights)
        
        # üìä –û–ë–©–ò–ô LOSS —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ –≤–µ—Å–∞–º–∏
        total_loss = (
            self.mel_loss_weight * mel_loss +
            self.gate_loss_weight * gate_loss +
            self._get_adaptive_guide_weight() * guide_loss +
            self.spectral_loss_weight * spectral_loss +
            self.perceptual_loss_weight * perceptual_loss +
            self.style_loss_weight * style_loss +
            self.monotonic_loss_weight * monotonic_loss
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —à–∞–≥–æ–≤
        self.global_step += 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ loss
        loss_dict = {
            'total_loss': total_loss,
            'mel_loss': mel_loss,
            'gate_loss': gate_loss,
            'guide_loss': guide_loss,
            'spectral_loss': spectral_loss,
            'perceptual_loss': perceptual_loss,
            'style_loss': style_loss,
            'monotonic_loss': monotonic_loss,
            'guide_weight': self._get_adaptive_guide_weight()
        }
        
        return total_loss, loss_dict

    def guided_attention_loss(self, att_ws, mel_len, text_len):
        """
        üî• –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ô Guided Attention Loss –Ω–∞ –æ—Å–Ω–æ–≤–µ Very Attentive Tacotron (2025).
        
        –ö–ª—é—á–µ–≤—ã–µ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:
        1. Location-Relative —Ñ–æ—Ä–º—É–ª–∞ –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –¥–∏–∞–≥–æ–Ω–∞–ª–∏
        2. Tensor –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–æ–≤ (–≤ 10x –±—ã—Å—Ç—Ä–µ–µ) 
        3. Adaptive sigma –∏ weight decay
        4. –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ KL divergence
        """
        batch_size = att_ws.size(0)
        max_mel_len = att_ws.size(1)
        max_text_len = att_ws.size(2)
        
        # üî• –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã
        # –°–æ–∑–¥–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω—ã–µ —Å–µ—Ç–∫–∏ –¥–ª—è –≤—Å–µ—Ö –±–∞—Ç—á–µ–π —Å—Ä–∞–∑—É
        mel_indices = torch.arange(max_mel_len, device=att_ws.device, dtype=torch.float32).unsqueeze(1)
        text_indices = torch.arange(max_text_len, device=att_ws.device, dtype=torch.float32).unsqueeze(0)
        
        # üî• –ê–î–ê–ü–¢–ò–í–ù–ê–Ø sigma –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        adaptive_sigma = self._get_adaptive_sigma()
        
        # üî• LOCATION-RELATIVE —Ñ–æ—Ä–º—É–ª–∞ –∏–∑ Very Attentive Tacotron
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥–ª–∏–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        mel_normalized = mel_indices / max_mel_len  # [max_mel_len, 1]
        text_normalized = text_indices / max_text_len  # [1, max_text_len]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–∂–∏–¥–∞–µ–º–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π
        expected_alignment = torch.exp(-0.5 * ((mel_normalized - text_normalized) ** 2) / (adaptive_sigma ** 2))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ mel —à–∞–≥–∞
        expected_alignment = expected_alignment / (expected_alignment.sum(dim=1, keepdim=True) + 1e-8)
        
        # –î–æ–±–∞–≤–ª—è–µ–º batch —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏ —Å–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É
        expected_alignment = expected_alignment.unsqueeze(0).expand(batch_size, -1, -1)
        
        # üî• –£–ú–ù–ê–Ø –º–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–ª–∏–Ω
        mask = torch.zeros_like(expected_alignment, dtype=torch.bool)
        for b in range(batch_size):
            actual_mel_len = min(mel_len, max_mel_len) if isinstance(mel_len, int) else min(mel_len[b], max_mel_len)
            actual_text_len = min(text_len, max_text_len) if isinstance(text_len, int) else min(text_len[b], max_text_len)
            mask[b, :actual_mel_len, :actual_text_len] = True
        
        # üî• KL DIVERGENCE –≤–º–µ—Å—Ç–æ MSE –¥–ª—è –ª—É—á—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ epsilon –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        att_ws_masked = att_ws * mask.float() + 1e-8
        expected_masked = expected_alignment * mask.float() + 1e-8
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        att_ws_normalized = att_ws_masked / att_ws_masked.sum(dim=2, keepdim=True)
        expected_normalized = expected_masked / expected_masked.sum(dim=2, keepdim=True)
        
        # üî• –í–ï–ö–¢–û–†–ò–ó–û–í–ê–ù–ù–´–ô KL divergence
        kl_div = F.kl_div(
            torch.log(att_ws_normalized + 1e-8), 
            expected_normalized, 
            reduction='none'
        )
        
        # –ú–∞—Å–∫–∏—Ä—É–µ–º –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º
        kl_div_masked = kl_div * mask.float()
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
        valid_elements = mask.float().sum()
        if valid_elements > 0:
            guide_loss = kl_div_masked.sum() / valid_elements
        else:
            guide_loss = torch.tensor(0.0, device=att_ws.device, requires_grad=True)
        
        return guide_loss

    def _get_adaptive_guide_weight(self):
        """
        üî• –ê–î–ê–ü–¢–ò–í–ù–´–ô –≤–µ—Å guided attention –Ω–∞ –æ—Å–Ω–æ–≤–µ Very Attentive Tacotron.
        
        –°—Ö–µ–º–∞: –Ω–∞—á–∏–Ω–∞–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–µ—Å–∞, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —Å–Ω–∏–∂–∞–µ–º –¥–æ –º–∏–Ω–∏–º—É–º–∞.
        """
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        initial_weight = getattr(self.hparams, 'guide_loss_initial_weight', 15.0)  # üî• –£–í–ï–õ–ò–ß–ï–ù–û!
        decay_start = getattr(self.hparams, 'guide_loss_decay_start', 2000)
        decay_steps = getattr(self.hparams, 'guide_loss_decay_steps', 25000)
        min_weight = 0.05  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è alignment
        
        if self.global_step < decay_start:
            # –§–∞–∑–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ guided attention
            return initial_weight
        elif self.global_step < decay_start + decay_steps:
            # –§–∞–∑–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è
            progress = (self.global_step - decay_start) / decay_steps
            # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π decay –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è
            decay_factor = math.exp(-progress * 3)  # -3 –¥–ª—è —É–º–µ—Ä–µ–Ω–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è
            current_weight = min_weight + (initial_weight - min_weight) * decay_factor
            return max(min_weight, current_weight)
        else:
            # –§–∞–∑–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ guided attention
            return min_weight

    def _get_adaptive_sigma(self):
        """
        üî• –ê–î–ê–ü–¢–ò–í–ù–ê–Ø sigma –¥–ª—è guided attention –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è.
        
        –°—Ö–µ–º–∞: —É–∑–∫–∞—è -> —à–∏—Ä–æ–∫–∞—è -> —Å—Ä–µ–¥–Ω—è—è –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ alignment.
        """
        if self.global_step < 1000:
            # –ù–∞—á–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞: —É–∑–∫–∞—è sigma –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ alignment
            return 0.1
        elif self.global_step < 5000:
            # –†–∞—Å—à–∏—Ä—è—é—â–∞—è —Ñ–∞–∑–∞: —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º sigma –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
            progress = (self.global_step - 1000) / 4000
            return 0.1 + 0.3 * progress  # –û—Ç 0.1 –¥–æ 0.4
        else:
            # –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—â–∞—è —Ñ–∞–∑–∞: —Å—Ä–µ–¥–Ω—è—è sigma –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
            progress = min(1.0, (self.global_step - 5000) / 15000)
            return 0.4 - 0.25 * progress  # –û—Ç 0.4 –¥–æ 0.15


class PerceptualLoss(nn.Module):
    """
    Perceptual Loss –¥–ª—è –±–æ–ª–µ–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–≤—É—á–∞–Ω–∏—è.
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏ –∑–≤—É–∫–∞.
    """
    
    def __init__(self, mel_channels=80):
        super(PerceptualLoss, self).__init__()
        self.mel_channels = mel_channels
        
        # –í–µ—Å–∞ —á–∞—Å—Ç–æ—Ç (–Ω–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã –≤–∞–∂–Ω–µ–µ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è)
        freq_weights = torch.exp(-torch.arange(mel_channels) / 20.0)
        self.register_buffer('freq_weights', freq_weights)
        
    def forward(self, mel_pred, mel_target):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç perceptual loss —Å –≤–µ—Å–∞–º–∏ —á–∞—Å—Ç–æ—Ç.
        """
        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π MSE loss
        weighted_diff = (mel_pred - mel_target) ** 2
        weighted_loss = weighted_diff * self.freq_weights.view(1, -1, 1)
        
        # –î–æ–±–∞–≤–ª—è–µ–º L1 loss –¥–ª—è —Ä–µ–∑–∫–æ—Å—Ç–∏
        l1_loss = F.l1_loss(mel_pred, mel_target)
        
        perceptual_loss = torch.mean(weighted_loss) + 0.5 * l1_loss
        return perceptual_loss


class StyleLoss(nn.Module):
    """
    Style Loss –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≥–æ–ª–æ—Å–∞.
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö mel —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º.
    """
    
    def __init__(self):
        super(StyleLoss, self).__init__()
        
    def forward(self, mel_pred, mel_target):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç style loss –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ mel.
        """
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ (—Å—Ä–µ–¥–Ω–µ–µ)
        mean_pred = torch.mean(mel_pred, dim=2)
        mean_target = torch.mean(mel_target, dim=2)
        mean_loss = F.mse_loss(mean_pred, mean_target)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ (–¥–∏—Å–ø–µ—Ä—Å–∏—è)
        var_pred = torch.var(mel_pred, dim=2)
        var_target = torch.var(mel_target, dim=2)
        var_loss = F.mse_loss(var_pred, var_target)
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∫–∞–Ω–∞–ª–∞–º–∏
        def compute_gram_matrix(x):
            b, c, t = x.size()
            features = x.view(b, c, t)
            gram = torch.bmm(features, features.transpose(1, 2))
            return gram / (c * t)
        
        gram_pred = compute_gram_matrix(mel_pred)
        gram_target = compute_gram_matrix(mel_target)
        gram_loss = F.mse_loss(gram_pred, gram_target)
        
        style_loss = mean_loss + var_loss + 0.5 * gram_loss
        return style_loss


class MonotonicAlignmentLoss(nn.Module):
    """
    Monotonic Alignment Loss –¥–ª—è –ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏—è –∫ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–º—É alignment.
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ MonoAlign –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö (2024).
    """
    
    def __init__(self):
        super(MonotonicAlignmentLoss, self).__init__()
        
    def forward(self, attention_weights):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —à—Ç—Ä–∞—Ñ –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏ alignment.
        """
        # attention_weights: (B, T_mel, T_text)
        batch_size, mel_len, text_len = attention_weights.shape
        
        monotonic_loss = 0.0
        
        for b in range(batch_size):
            att_matrix = attention_weights[b]  # (T_mel, T_text)
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏ attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ mel —à–∞–≥–∞
            peak_positions = torch.argmax(att_matrix, dim=1)  # (T_mel,)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏
            for i in range(1, mel_len):
                # –®—Ç—Ä–∞—Ñ –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –ø–∏–∫ —Ä–∞–Ω—å—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ
                if peak_positions[i] < peak_positions[i-1]:
                    # –†–∞–∑–º–µ—Ä –Ω–∞—Ä—É—à–µ–Ω–∏—è
                    violation = peak_positions[i-1] - peak_positions[i]
                    monotonic_loss += violation.float()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É batch –∏ –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        monotonic_loss = monotonic_loss / (batch_size * mel_len)
        
        return monotonic_loss


class QualityEnhancedMelLoss(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π Mel Loss —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å.
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Ö–æ–¥–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.
    """
    
    def __init__(self, alpha=1.0, beta=0.5, gamma=0.3):
        super(QualityEnhancedMelLoss, self).__init__()
        self.alpha = alpha  # MSE weight
        self.beta = beta    # L1 weight  
        self.gamma = gamma  # Dynamic range weight
        
    def forward(self, mel_pred, mel_target):
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π mel loss –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.
        """
        # 1. –ë–∞–∑–æ–≤—ã–π MSE loss
        mse_loss = F.mse_loss(mel_pred, mel_target)
        
        # 2. L1 loss –¥–ª—è —Ä–µ–∑–∫–æ—Å—Ç–∏ –¥–µ—Ç–∞–ª–µ–π
        l1_loss = F.l1_loss(mel_pred, mel_target)
        
        # 3. Dynamic range loss –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏
        pred_range = torch.max(mel_pred, dim=2)[0] - torch.min(mel_pred, dim=2)[0]
        target_range = torch.max(mel_target, dim=2)[0] - torch.min(mel_target, dim=2)[0]
        range_loss = F.mse_loss(pred_range, target_range)
        
        # 4. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π loss
        total_loss = (
            self.alpha * mse_loss + 
            self.beta * l1_loss + 
            self.gamma * range_loss
        )
        
        return total_loss


def create_enhanced_loss_function(hparams):
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π loss function.
    """
    return Tacotron2Loss(hparams)
