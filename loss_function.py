from torch import nn
import torch
import numpy as np
import torch.nn.functional as F


class SpectralMelLoss(nn.Module):
    """
    üéµ –£–ª—É—á—à–µ–Ω–Ω–∞—è Mel Loss —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    """
    def __init__(self, n_mel_channels=80, sample_rate=22050):
        super(SpectralMelLoss, self).__init__()
        self.n_mel_channels = n_mel_channels
        self.sample_rate = sample_rate
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
        mel_weights = torch.ones(n_mel_channels)
        # –ë–æ–ª—å—à–∏–π –≤–µ—Å –Ω–∞ —Å—Ä–µ–¥–Ω–∏—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö (–≤–∞–∂–Ω—ã—Ö –¥–ª—è –≥–æ–ª–æ—Å–∞)
        mid_range = slice(n_mel_channels//4, 3*n_mel_channels//4)
        mel_weights[mid_range] *= 1.5
        # –ú–µ–Ω—å—à–∏–π –≤–µ—Å –Ω–∞ –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö
        high_range = slice(3*n_mel_channels//4, n_mel_channels)
        mel_weights[high_range] *= 0.8
        
        self.register_buffer('mel_weights', mel_weights)
        
    def forward(self, mel_pred, mel_target):
        # –û—Å–Ω–æ–≤–Ω–æ–π MSE loss —Å –≤–µ—Å–∞–º–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–∞–º
        weighted_mse = F.mse_loss(mel_pred * self.mel_weights[None, :, None], 
                                  mel_target * self.mel_weights[None, :, None])
        
        # –î–æ–±–∞–≤–ª—è–µ–º L1 loss –¥–ª—è —Ä–µ–∑–∫–æ—Å—Ç–∏
        l1_loss = F.l1_loss(mel_pred, mel_target)
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π loss (—Ä–∞–∑–Ω–æ—Å—Ç–∏ —Å–æ—Å–µ–¥–Ω–∏—Ö —Ñ—Ä–µ–π–º–æ–≤)
        mel_pred_diff = mel_pred[:, :, 1:] - mel_pred[:, :, :-1]
        mel_target_diff = mel_target[:, :, 1:] - mel_target[:, :, :-1]
        spectral_loss = F.mse_loss(mel_pred_diff, mel_target_diff)
        
        return weighted_mse + 0.3 * l1_loss + 0.2 * spectral_loss


class AdaptiveGateLoss(nn.Module):
    """
    üö™ –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è Gate Loss —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –≤–µ—Å–æ–º
    """
    def __init__(self, initial_weight=1.3, min_weight=0.8, max_weight=2.0):
        super(AdaptiveGateLoss, self).__init__()
        self.initial_weight = initial_weight
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.current_weight = initial_weight
        
    def update_weight(self, gate_accuracy):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π gate accuracy"""
        if gate_accuracy < 0.5:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –µ—Å–ª–∏ accuracy –Ω–∏–∑–∫–∞—è
            self.current_weight = min(self.max_weight, self.current_weight * 1.1)
        elif gate_accuracy > 0.8:
            # –£–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å –µ—Å–ª–∏ accuracy –≤—ã—Å–æ–∫–∞—è
            self.current_weight = max(self.min_weight, self.current_weight * 0.95)
            
    def forward(self, gate_pred, gate_target):
        return self.current_weight * F.binary_cross_entropy_with_logits(gate_pred, gate_target)


class Tacotron2Loss(nn.Module):
    def __init__(self, hparams, iteration=0):
        super(Tacotron2Loss, self).__init__()
        self.hparams = hparams
        self.guide_decay = 0.9999
        self.scale = 10.0 * (self.guide_decay**iteration)
        # Guide scale —Å–∫—Ä—ã—Ç –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã –ª–æ–≥–æ–≤
        self.guide_lowbound = 0.1
        self.criterion_attention = nn.L1Loss()
        
        # üîß –ù–û–í–´–ï —É–ª—É—á—à–µ–Ω–Ω—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏
        self.spectral_mel_loss = SpectralMelLoss(hparams.n_mel_channels)
        self.adaptive_gate_loss = AdaptiveGateLoss()
        
        self.guided_attention = GuidedAttentionLoss(sigma=0.4, alpha=1.0)

    def forward(self, model_output, targets):
        _, mel_out, mel_out_postnet, gate_out, alignments_out, tpse_gst_pred, gst_target = model_output
        mel_target, gate_target, guide_target = targets[0], targets[1], targets[2]

        mel_target.requires_grad = False
        gate_target.requires_grad = False
        gate_target = gate_target.view(-1, 1)
        guide_target = guide_target.transpose(2,1)
        _, w, h = alignments_out.shape
        guide_target = guide_target[:,:w,:h]

        gate_out = gate_out.view(-1, 1)
        emb_loss = torch.tensor(0, device=mel_target.device)
        if tpse_gst_pred is not None:
            emb_loss = nn.L1Loss()(tpse_gst_pred, gst_target.detach())
        
        # üîß –£–õ–£–ß–®–ï–ù–ù–´–ô mel loss —Å —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º
        mel_loss = (self.spectral_mel_loss(mel_out, mel_target) + 
                   self.spectral_mel_loss(mel_out_postnet, mel_target))
        
        # üîß –ê–î–ê–ü–¢–ò–í–ù–´–ô gate loss
        gate_loss = self.adaptive_gate_loss(gate_out, gate_target)

        attention_masks = torch.ones_like(alignments_out)
        loss_atten = torch.mean(alignments_out * guide_target) * self.scale
        
        self.scale *= self.guide_decay
        if self.scale < self.guide_lowbound:
            self.scale = self.guide_lowbound

        return mel_loss, gate_loss, loss_atten, emb_loss


class GuidedAttentionLoss(nn.Module):
    """
    Guided Attention Loss –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏ attention alignment –≤ Tacotron2.
    
    –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Å—Ç–∞—Ç—å–µ "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional 
    Networks with Guided Attention" (https://arxiv.org/abs/1710.08969)
    """
    
    def __init__(self, sigma=0.4, alpha=1.0, reset_always=True):
        """
        Args:
            sigma (float): –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –≥–∞—É—Å—Å–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–µ–π –º–∞—Ç—Ä–∏—Ü—ã
            alpha (float): –ù–∞—á–∞–ª—å–Ω—ã–π –≤–µ—Å guided attention loss
            reset_always (bool): –°–±—Ä–∞—Å—ã–≤–∞—Ç—å –ª–∏ –≤–µ—Å –Ω–∞ –∫–∞–∂–¥–æ–º forward pass
        """
        super(GuidedAttentionLoss, self).__init__()
        self.sigma = sigma
        self.alpha = alpha
        self.reset_always = reset_always
        self.guided_attn_masks = {}
        
    def _make_guided_attention_mask(self, ilen, olen):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–µ–π –º–∞—Å–∫–∏ attention."""
        grid_x, grid_y = torch.meshgrid(
            torch.arange(olen).to(torch.float32),
            torch.arange(ilen).to(torch.float32),
            indexing='ij'
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        grid_x = grid_x / (olen - 1) if olen > 1 else grid_x
        grid_y = grid_y / (ilen - 1) if ilen > 1 else grid_y
        
        # –ì–∞—É—Å—Å–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–π –Ω–∞–ø—Ä–∞–≤–ª—è—é—â–µ–π –º–∞—Å–∫–∏
        return 1.0 - torch.exp(-((grid_x - grid_y) ** 2) / (2 * (self.sigma ** 2)))
    
    def forward(self, model_output, input_lengths=None, output_lengths=None):
        """
        Args:
            model_output: –í—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π attention weights
            input_lengths: –î–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            output_lengths: –î–ª–∏–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            
        Returns:
            guided_attn_loss: Guided attention loss
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º attention weights –∏–∑ model_output
        if isinstance(model_output, (list, tuple)):
            # Tacotron2 –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂: (mel_outputs, mel_outputs_postnet, gate_outputs, alignments)
            if len(model_output) >= 4:
                attention_weights = model_output[4]  # alignments
            else:
                # –ï—Å–ª–∏ alignments –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤–æ–π loss
                return torch.tensor(0.0, device=model_output[0].device, requires_grad=True)
        else:
            attention_weights = model_output
            
        if attention_weights is None:
            return torch.tensor(0.0, requires_grad=True)
            
        batch_size, max_target_len, max_input_len = attention_weights.size()
        
        # –ï—Å–ª–∏ –¥–ª–∏–Ω—ã –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–µ
        if input_lengths is None:
            input_lengths = [max_input_len] * batch_size
        if output_lengths is None:
            output_lengths = [max_target_len] * batch_size
            
        guided_attn_loss = 0.0
        
        for b in range(batch_size):
            ilen = input_lengths[b]
            olen = output_lengths[b]
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Å–∫–∏
            mask_key = (ilen, olen)
            
            if mask_key not in self.guided_attn_masks:
                # –°–æ–∑–¥–∞–µ–º –Ω–∞–ø—Ä–∞–≤–ª—è—é—â—É—é –º–∞—Å–∫—É
                mask = self._make_guided_attention_mask(ilen, olen)
                self.guided_attn_masks[mask_key] = mask
            else:
                mask = self.guided_attn_masks[mask_key]
                
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–∞—Å–∫—É –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            mask = mask.to(attention_weights.device)
            
            # –û–±—Ä–µ–∑–∞–µ–º attention weights –∏ –º–∞—Å–∫—É –¥–æ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–ª–∏–Ω
            attn = attention_weights[b, :olen, :ilen]
            mask = mask[:olen, :ilen]
            
            # –í—ã—á–∏—Å–ª—è–µ–º guided attention loss –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            guided_attn_loss += torch.mean(attn * mask)
            
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –±–∞—Ç—á—É
        guided_attn_loss /= batch_size
        
        return self.alpha * guided_attn_loss
    
    def get_weight(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –≤–µ—Å guided attention loss."""
        return self.alpha
    
    def set_weight(self, alpha):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–π –≤–µ—Å guided attention loss."""
        self.alpha = alpha
        
    def decay_weight(self, decay_factor=0.99):
        """–£–º–µ–Ω—å—à–∞–µ—Ç –≤–µ—Å guided attention loss."""
        self.alpha *= decay_factor
        
    def reset_weight(self, alpha=1.0):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –≤–µ—Å guided attention loss."""
        self.alpha = alpha
