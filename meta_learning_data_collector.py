#!/usr/bin/env python3
"""
üìö META-LEARNING DATA COLLECTOR
–°–∏—Å—Ç–µ–º–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Meta-Learning Engine

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
‚úÖ –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –æ—Ç–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
‚úÖ –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–¥–∫–∏—Ö —Å–ª—É—á–∞–µ–≤
‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
"""

import os
import json
import time
import pickle
import numpy as np
import sqlite3
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import logging

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Meta-Learning Engine
try:
    from meta_learning_engine import MetaLearningEngine, TrainingEpisode, LearningStrategy, TrainingPhase
    META_LEARNING_AVAILABLE = True
except ImportError:
    META_LEARNING_AVAILABLE = False
    # Fallback enum
    class TrainingPhase:
        PRE_ALIGNMENT = "pre_alignment"
        ALIGNMENT_LEARNING = "alignment"
        REFINEMENT = "refinement"
        CONVERGENCE = "convergence"

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–∞–º–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
try:
    from unified_logging_system import UnifiedLoggingSystem
    UNIFIED_LOGGING_AVAILABLE = True
except ImportError:
    UNIFIED_LOGGING_AVAILABLE = False

@dataclass
class DataCollectionConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    min_episodes_target: int = 500  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–ª—å –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    max_episodes_storage: int = 2000  # –ú–∞–∫—Å–∏–º—É–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    quality_threshold: float = 0.7  # –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    diversity_weight: float = 0.3  # –í–µ—Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –ø—Ä–∏ –æ—Ç–±–æ—Ä–µ
    synthetic_data_ratio: float = 0.4  # –î–æ–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    data_augmentation_enabled: bool = True
    real_time_collection: bool = True
    export_formats: List[str] = None
    
    def __post_init__(self):
        if self.export_formats is None:
            self.export_formats = ['json', 'pickle', 'sqlite']

@dataclass
class DataQualityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    total_episodes: int
    successful_episodes: int
    failed_episodes: int
    phase_distribution: Dict[str, int]
    strategy_distribution: Dict[str, int]
    quality_score: float
    diversity_score: float
    completeness_score: float
    data_freshness: float  # –ù–∞—Å–∫–æ–ª—å–∫–æ —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ (0-1)

class SyntheticDataGenerator:
    """ü§ñ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, config: DataCollectionConfig):
        self.config = config
        self.logger = logging.getLogger(__name__ + ".SyntheticGenerator")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
        self.scenario_templates = {
            'normal_training': {
                'loss_range': (2.0, 8.0),
                'attention_range': (0.4, 0.8),
                'gradient_range': (0.5, 3.0),
                'success_probability': 0.7
            },
            'difficult_convergence': {
                'loss_range': (5.0, 15.0),
                'attention_range': (0.1, 0.4),
                'gradient_range': (1.0, 8.0),
                'success_probability': 0.3
            },
            'fast_learning': {
                'loss_range': (1.0, 4.0),
                'attention_range': (0.6, 0.9),
                'gradient_range': (0.3, 1.5),
                'success_probability': 0.9
            },
            'instability_recovery': {
                'loss_range': (10.0, 50.0),
                'attention_range': (0.05, 0.2),
                'gradient_range': (5.0, 20.0),
                'success_probability': 0.2
            }
        }
    
    def generate_episodes(self, n_episodes: int, scenario_mix: Optional[Dict[str, float]] = None) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        if scenario_mix is None:
            scenario_mix = {
                'normal_training': 0.6,
                'difficult_convergence': 0.2,
                'fast_learning': 0.15,
                'instability_recovery': 0.05
            }
        
        episodes = []
        
        for i in range(n_episodes):
            # –í—ã–±–æ—Ä —Å—Ü–µ–Ω–∞—Ä–∏—è
            scenario = np.random.choice(
                list(scenario_mix.keys()),
                p=list(scenario_mix.values())
            )
            
            episode = self._generate_single_episode(scenario, i)
            episodes.append(episode)
        
        self.logger.info(f"ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {n_episodes} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤")
        return episodes
    
    def _generate_single_episode(self, scenario: str, episode_id: int) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞"""
        template = self.scenario_templates[scenario]
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        initial_loss = np.random.uniform(*template['loss_range'])
        initial_attention = np.random.uniform(*template['attention_range'])
        
        # –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        success = np.random.random() < template['success_probability']
        
        if success:
            # –£–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            final_loss = initial_loss * np.random.uniform(0.3, 0.8)
            final_attention = min(0.95, initial_attention * np.random.uniform(1.5, 3.0))
            improvement_score = np.random.uniform(0.2, 0.8)
        else:
            # –£—Ö—É–¥—à–µ–Ω–∏–µ –∏–ª–∏ —Å—Ç–∞–≥–Ω–∞—Ü–∏—è
            final_loss = initial_loss * np.random.uniform(0.9, 2.0)
            final_attention = initial_attention * np.random.uniform(0.5, 1.1)
            improvement_score = np.random.uniform(-0.3, 0.1)
        
        # –í—ã–±–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        strategies = ['AGGRESSIVE', 'CONSERVATIVE', 'BALANCED', 'ADAPTIVE']
        strategy = np.random.choice(strategies)
        
        # –í—ã–±–æ—Ä —Ñ–∞–∑—ã
        phases = ['PRE_ALIGNMENT', 'ALIGNMENT_LEARNING', 'REFINEMENT', 'CONVERGENCE']
        initial_phase = np.random.choice(phases[:3])  # –§–∏–Ω–∞–ª—å–Ω—É—é —Ñ–∞–∑—É –Ω–µ –≤—ã–±–∏—Ä–∞–µ–º –∫–∞–∫ –Ω–∞—á–∞–ª—å–Ω—É—é
        
        if success:
            final_phase_idx = min(len(phases) - 1, phases.index(initial_phase) + np.random.randint(1, 3))
            final_phase = phases[final_phase_idx]
        else:
            final_phase = initial_phase  # –ù–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏–π
        decisions = self._generate_decisions(scenario, strategy, success)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–µ–Ω—è–ª–∏—Å—å
        parameters_changed = self._generate_parameter_changes(strategy, success)
        
        episode = {
            'episode_id': f"synthetic_{scenario}_{episode_id}",
            'start_time': time.time() - np.random.uniform(3600, 86400),  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
            'end_time': time.time() - np.random.uniform(0, 3600),
            'initial_phase': initial_phase,
            'final_phase': final_phase,
            'initial_loss': initial_loss,
            'initial_attention_quality': initial_attention,
            'final_loss': final_loss,
            'final_attention_quality': final_attention,
            'strategy_used': strategy,
            'decisions_made': decisions,
            'parameters_changed': parameters_changed,
            'success': success,
            'improvement_score': improvement_score,
            'convergence_achieved': success and final_phase == 'CONVERGENCE',
            'model_architecture': 'tacotron2',
            'dataset_size': np.random.randint(1000, 10000),
            'total_steps': np.random.randint(500, 5000),
            'scenario_type': scenario,  # –ú–µ—Ç–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            'synthetic': True
        }
        
        return episode
    
    def _generate_decisions(self, scenario: str, strategy: str, success: bool) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è —ç–ø–∏–∑–æ–¥–∞"""
        decisions = []
        
        num_decisions = np.random.randint(3, 12)
        
        for i in range(num_decisions):
            decision_types = ['lr_adjustment', 'batch_size_change', 'attention_weight_change', 'optimization_change']
            decision_type = np.random.choice(decision_types)
            
            if decision_type == 'lr_adjustment':
                old_lr = np.random.uniform(1e-5, 1e-3)
                if strategy == 'AGGRESSIVE':
                    new_lr = old_lr * np.random.uniform(1.2, 2.0)
                elif strategy == 'CONSERVATIVE':
                    new_lr = old_lr * np.random.uniform(0.5, 0.9)
                else:
                    new_lr = old_lr * np.random.uniform(0.8, 1.3)
                
                decision = {
                    'timestamp': time.time() - np.random.uniform(0, 3600),
                    'type': decision_type,
                    'old_value': old_lr,
                    'new_value': new_lr,
                    'reason': f'{strategy.lower()}_strategy'
                }
            
            elif decision_type == 'attention_weight_change':
                old_weight = np.random.uniform(1.0, 10.0)
                new_weight = old_weight * np.random.uniform(0.7, 1.5)
                
                decision = {
                    'timestamp': time.time() - np.random.uniform(0, 3600),
                    'type': decision_type,
                    'old_value': old_weight,
                    'new_value': new_weight,
                    'reason': 'attention_quality_improvement'
                }
            
            else:
                decision = {
                    'timestamp': time.time() - np.random.uniform(0, 3600),
                    'type': decision_type,
                    'reason': f'{strategy.lower()}_optimization'
                }
            
            decisions.append(decision)
        
        return decisions
    
    def _generate_parameter_changes(self, strategy: str, success: bool) -> Dict[str, float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        changes = {}
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏—è learning rate
        if np.random.random() < 0.8:
            base_lr = np.random.uniform(1e-5, 1e-3)
            if strategy == 'AGGRESSIVE':
                changes['learning_rate'] = base_lr * np.random.uniform(1.5, 3.0)
            else:
                changes['learning_rate'] = base_lr * np.random.uniform(0.3, 1.2)
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏—è guided attention weight
        if np.random.random() < 0.6:
            changes['guided_attention_weight'] = np.random.uniform(2.0, 12.0)
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏—è batch size
        if np.random.random() < 0.4:
            changes['batch_size'] = np.random.choice([16, 32, 64, 128])
        
        return changes

class DataQualityAnalyzer:
    """üìä –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__ + ".QualityAnalyzer")
    
    def analyze_data_quality(self, episodes: List[Dict[str, Any]]) -> DataQualityMetrics:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not episodes:
            return DataQualityMetrics(0, 0, 0, {}, {}, 0.0, 0.0, 0.0, 0.0)
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_episodes = len(episodes)
        successful_episodes = sum(1 for ep in episodes if ep.get('success', False))
        failed_episodes = total_episodes - successful_episodes
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ñ–∞–∑–∞–º
        phase_distribution = Counter(ep.get('initial_phase', 'unknown') for ep in episodes)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
        strategy_distribution = Counter(ep.get('strategy_used', 'unknown') for ep in episodes)
        
        # –†–∞—Å—á–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score = self._calculate_quality_score(episodes)
        diversity_score = self._calculate_diversity_score(episodes)
        completeness_score = self._calculate_completeness_score(episodes)
        data_freshness = self._calculate_data_freshness(episodes)
        
        metrics = DataQualityMetrics(
            total_episodes=total_episodes,
            successful_episodes=successful_episodes,
            failed_episodes=failed_episodes,
            phase_distribution=dict(phase_distribution),
            strategy_distribution=dict(strategy_distribution),
            quality_score=quality_score,
            diversity_score=diversity_score,
            completeness_score=completeness_score,
            data_freshness=data_freshness
        )
        
        self.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {total_episodes} —ç–ø–∏–∑–æ–¥–æ–≤, –∫–∞—á–µ—Å—Ç–≤–æ={quality_score:.2f}")
        return metrics
    
    def _calculate_quality_score(self, episodes: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if not episodes:
            return 0.0
        
        quality_factors = []
        
        # –§–∞–∫—Ç–æ—Ä 1: –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö –∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
        success_rate = sum(ep.get('success', False) for ep in episodes) / len(episodes)
        balanced_success = 1.0 - abs(success_rate - 0.6)  # –û–ø—Ç–∏–º—É–º 60% —É—Å–ø–µ—Ö–∞
        quality_factors.append(balanced_success)
        
        # –§–∞–∫—Ç–æ—Ä 2: –ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
        complete_episodes = sum(
            1 for ep in episodes 
            if all(key in ep for key in ['initial_loss', 'final_loss', 'strategy_used', 'decisions_made'])
        )
        completeness = complete_episodes / len(episodes)
        quality_factors.append(completeness)
        
        # –§–∞–∫—Ç–æ—Ä 3: –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        strategies = set(ep.get('strategy_used', '') for ep in episodes)
        strategy_diversity = min(1.0, len(strategies) / 4)  # 4 –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        quality_factors.append(strategy_diversity)
        
        # –§–∞–∫—Ç–æ—Ä 4: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫
        realistic_episodes = sum(
            1 for ep in episodes
            if (0.1 <= ep.get('initial_loss', 0) <= 100 and 
                0.0 <= ep.get('initial_attention_quality', 0) <= 1.0)
        )
        realism = realistic_episodes / len(episodes)
        quality_factors.append(realism)
        
        return np.mean(quality_factors)
    
    def _calculate_diversity_score(self, episodes: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–∞–Ω–Ω—ã—Ö"""
        if not episodes:
            return 0.0
        
        diversity_factors = []
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ñ–∞–∑
        phases = [ep.get('initial_phase', '') for ep in episodes]
        phase_entropy = self._calculate_entropy(phases)
        diversity_factors.append(phase_entropy)
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        strategies = [ep.get('strategy_used', '') for ep in episodes]
        strategy_entropy = self._calculate_entropy(strategies)
        diversity_factors.append(strategy_entropy)
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        improvement_scores = [ep.get('improvement_score', 0) for ep in episodes]
        score_variance = np.var(improvement_scores) if improvement_scores else 0
        normalized_variance = min(1.0, score_variance / 0.25)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        diversity_factors.append(normalized_variance)
        
        return np.mean(diversity_factors)
    
    def _calculate_completeness_score(self, episodes: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö"""
        if not episodes:
            return 0.0
        
        required_fields = [
            'episode_id', 'initial_loss', 'final_loss', 'initial_attention_quality',
            'final_attention_quality', 'strategy_used', 'success', 'improvement_score'
        ]
        
        complete_episodes = 0
        for episode in episodes:
            if all(field in episode and episode[field] is not None for field in required_fields):
                complete_episodes += 1
        
        return complete_episodes / len(episodes)
    
    def _calculate_data_freshness(self, episodes: List[Dict[str, Any]]) -> float:
        """–†–∞—Å—á–µ—Ç —Å–≤–µ–∂–µ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        if not episodes:
            return 0.0
        
        current_time = time.time()
        episode_ages = []
        
        for episode in episodes:
            episode_time = episode.get('end_time', current_time)
            age_hours = (current_time - episode_time) / 3600
            freshness = max(0, 1.0 - age_hours / (24 * 7))  # –ú–∞–∫—Å–∏–º—É–º –Ω–µ–¥–µ–ª—è
            episode_ages.append(freshness)
        
        return np.mean(episode_ages)
    
    def _calculate_entropy(self, values: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
        if not values:
            return 0.0
        
        counts = Counter(values)
        total = len(values)
        
        entropy = 0
        for count in counts.values():
            p = count / total
            if p > 0:
                entropy -= p * np.log2(p)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏
        max_entropy = np.log2(len(counts)) if counts else 1
        return entropy / max_entropy if max_entropy > 0 else 0

class MetaLearningDataCollector:
    """üìö –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Meta-Learning Engine"""
    
    def __init__(self, config: Optional[DataCollectionConfig] = None, data_dir: str = "meta_learning_data"):
        self.config = config or DataCollectionConfig()
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        self.synthetic_generator = SyntheticDataGenerator(self.config)
        self.quality_analyzer = DataQualityAnalyzer()
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö
        self.episodes_storage = []
        self.real_episodes_count = 0
        self.synthetic_episodes_count = 0
        
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è persistence
        self.db_path = self.data_dir / "meta_learning_data.db"
        self._init_database()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if UNIFIED_LOGGING_AVAILABLE:
            self.logger = UnifiedLoggingSystem().register_component("MetaLearningDataCollector")
        else:
            self.logger = logging.getLogger(__name__)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self._load_existing_data()
        
        self.logger.info(f"üìö Meta-Learning Data Collector –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {len(self.episodes_storage)} —ç–ø–∏–∑–æ–¥–æ–≤")
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS episodes (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    episode_id TEXT UNIQUE,
                    data BLOB,
                    success INTEGER,
                    improvement_score REAL,
                    strategy_used TEXT,
                    synthetic INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_success ON episodes(success)
            ''')
            conn.execute('''
                CREATE INDEX IF NOT EXISTS idx_synthetic ON episodes(synthetic)
            ''')
    
    def _load_existing_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute('SELECT data, synthetic FROM episodes ORDER BY created_at')
                
                for data_blob, is_synthetic in cursor.fetchall():
                    try:
                        episode = pickle.loads(data_blob)
                        self.episodes_storage.append(episode)
                        
                        if is_synthetic:
                            self.synthetic_episodes_count += 1
                        else:
                            self.real_episodes_count += 1
                            
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —ç–ø–∏–∑–æ–¥–∞: {e}")
                        
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def add_real_episode(self, episode_data: Dict[str, Any]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —ç–ø–∏–∑–æ–¥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        if not self._validate_episode_data(episode_data):
            self.logger.warning("‚ö†Ô∏è –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —ç–ø–∏–∑–æ–¥–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            return
        
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        enriched_episode = self._enrich_episode_data(episode_data)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ storage
        self.episodes_storage.append(enriched_episode)
        self.real_episodes_count += 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        self._save_episode_to_db(enriched_episode, synthetic=False)
        
        # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º storage
        self._manage_storage_size()
        
        self.logger.info(f"üìù –î–æ–±–∞–≤–ª–µ–Ω —Ä–µ–∞–ª—å–Ω—ã–π —ç–ø–∏–∑–æ–¥: {enriched_episode.get('episode_id', 'unknown')}")
    
    def generate_synthetic_data(self, target_episodes: Optional[int] = None):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if target_episodes is None:
            current_total = len(self.episodes_storage)
            target_episodes = max(0, self.config.min_episodes_target - current_total)
        
        if target_episodes <= 0:
            self.logger.info("üìö –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            return
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω—É–∂–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
        current_quality = self.quality_analyzer.analyze_data_quality(self.episodes_storage)
        scenario_mix = self._determine_scenario_mix(current_quality)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–ø–∏–∑–æ–¥–æ–≤
        synthetic_episodes = self.synthetic_generator.generate_episodes(target_episodes, scenario_mix)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ storage
        for episode in synthetic_episodes:
            self.episodes_storage.append(episode)
            self.synthetic_episodes_count += 1
            self._save_episode_to_db(episode, synthetic=True)
        
        self.logger.info(f"ü§ñ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(synthetic_episodes)} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —ç–ø–∏–∑–æ–¥–æ–≤")
    
    def get_training_dataset(self, format_type: str = 'episodes') -> Any:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Meta-Learning Engine"""
        if format_type == 'episodes':
            return self.episodes_storage.copy()
        elif format_type == 'processed':
            return self._process_episodes_for_training()
        elif format_type == 'quality_metrics':
            return self.quality_analyzer.analyze_data_quality(self.episodes_storage)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {format_type}")
    
    def enhance_data_quality(self):
        """–£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üîß –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        current_quality = self.quality_analyzer.analyze_data_quality(self.episodes_storage)
        
        improvements_needed = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        if current_quality.total_episodes < self.config.min_episodes_target:
            needed = self.config.min_episodes_target - current_quality.total_episodes
            improvements_needed.append(f"–ù—É–∂–Ω–æ –µ—â–µ {needed} —ç–ø–∏–∑–æ–¥–æ–≤")
            self.generate_synthetic_data(needed)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å–∞ —É—Å–ø–µ—Ö–∞/–Ω–µ—É–¥–∞—á–∏
        success_rate = current_quality.successful_episodes / max(1, current_quality.total_episodes)
        if success_rate < 0.3 or success_rate > 0.8:
            improvements_needed.append("–ù—É–∂–µ–Ω –ª—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —É—Å–ø–µ—Ö–∞/–Ω–µ—É–¥–∞—á–∏")
            self._balance_success_ratio()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ñ–∞–∑
        phase_counts = current_quality.phase_distribution
        min_phase_count = max(1, current_quality.total_episodes // 10)
        for phase in ['PRE_ALIGNMENT', 'ALIGNMENT_LEARNING', 'REFINEMENT', 'CONVERGENCE']:
            if phase_counts.get(phase, 0) < min_phase_count:
                improvements_needed.append(f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∞–∑—ã {phase}")
                self._generate_phase_specific_data(phase, min_phase_count)
        
        if improvements_needed:
            self.logger.info(f"üîß –ü—Ä–∏–º–µ–Ω–µ–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è: {', '.join(improvements_needed)}")
        else:
            self.logger.info("‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ")
    
    def export_data(self, export_format: str = 'json', file_path: Optional[str] = None) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
        if file_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            file_path = self.data_dir / f"meta_learning_data_{timestamp}.{export_format}"
        else:
            file_path = Path(file_path)
        
        try:
            if export_format == 'json':
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        'episodes': self.episodes_storage,
                        'metadata': {
                            'total_episodes': len(self.episodes_storage),
                            'real_episodes': self.real_episodes_count,
                            'synthetic_episodes': self.synthetic_episodes_count,
                            'export_timestamp': datetime.now().isoformat()
                        }
                    }, f, indent=2, default=str, ensure_ascii=False)
                    
            elif export_format == 'pickle':
                with open(file_path, 'wb') as f:
                    pickle.dump(self.episodes_storage, f)
                    
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞: {export_format}")
            
            self.logger.info(f"üì§ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {file_path}")
            return str(file_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}")
            raise
    
    def get_data_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–∞–Ω–Ω—ã–º"""
        quality_metrics = self.quality_analyzer.analyze_data_quality(self.episodes_storage)
        
        return {
            'data_counts': {
                'total_episodes': len(self.episodes_storage),
                'real_episodes': self.real_episodes_count,
                'synthetic_episodes': self.synthetic_episodes_count,
                'target_episodes': self.config.min_episodes_target
            },
            'quality_metrics': asdict(quality_metrics),
            'progress_to_target': len(self.episodes_storage) / self.config.min_episodes_target,
            'recommendations': self._get_data_recommendations(quality_metrics)
        }
    
    def _validate_episode_data(self, episode_data: Dict[str, Any]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–∞"""
        required_fields = ['episode_id', 'success']
        
        for field in required_fields:
            if field not in episode_data:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤ –∑–Ω–∞—á–µ–Ω–∏–π
        if 'initial_loss' in episode_data:
            if not (0.01 <= episode_data['initial_loss'] <= 1000):
                return False
        
        if 'initial_attention_quality' in episode_data:
            if not (0.0 <= episode_data['initial_attention_quality'] <= 1.0):
                return False
        
        return True
    
    def _enrich_episode_data(self, episode_data: Dict[str, Any]) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–∞"""
        enriched = episode_data.copy()
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ timestamp –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        if 'end_time' not in enriched:
            enriched['end_time'] = time.time()
        
        # –†–∞—Å—á–µ—Ç improvement_score –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        if 'improvement_score' not in enriched and 'initial_loss' in enriched and 'final_loss' in enriched:
            loss_improvement = (enriched['initial_loss'] - enriched['final_loss']) / enriched['initial_loss']
            attention_improvement = enriched.get('final_attention_quality', 0) - enriched.get('initial_attention_quality', 0)
            enriched['improvement_score'] = loss_improvement * 0.7 + attention_improvement * 0.3
        
        # –ú–µ—Ç–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        enriched['synthetic'] = False
        
        return enriched
    
    def _save_episode_to_db(self, episode: Dict[str, Any], synthetic: bool):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–ø–∏–∑–æ–¥–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            episode_blob = pickle.dumps(episode)
            
            with sqlite3.connect(self.db_path) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO episodes 
                    (episode_id, data, success, improvement_score, strategy_used, synthetic)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    episode.get('episode_id', ''),
                    episode_blob,
                    int(episode.get('success', False)),
                    episode.get('improvement_score', 0.0),
                    episode.get('strategy_used', ''),
                    int(synthetic)
                ))
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î: {e}")
    
    def _manage_storage_size(self):
        """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º storage"""
        if len(self.episodes_storage) > self.config.max_episodes_storage:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —ç–ø–∏–∑–æ–¥—ã, –æ—Ç–¥–∞–≤–∞—è –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º
            episodes_to_remove = len(self.episodes_storage) - self.config.max_episodes_storage
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ç–∏–ø—É (—Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–≤—ã–º–∏)
            sorted_episodes = sorted(
                enumerate(self.episodes_storage), 
                key=lambda x: (not x[1].get('synthetic', False), x[1].get('end_time', 0))
            )
            
            for i in range(episodes_to_remove):
                idx, episode = sorted_episodes[i]
                if episode.get('synthetic', False):
                    self.synthetic_episodes_count -= 1
                else:
                    self.real_episodes_count -= 1
            
            # –£–¥–∞–ª—è–µ–º —ç–ø–∏–∑–æ–¥—ã
            indices_to_remove = [sorted_episodes[i][0] for i in range(episodes_to_remove)]
            self.episodes_storage = [
                ep for i, ep in enumerate(self.episodes_storage) 
                if i not in indices_to_remove
            ]
    
    def _determine_scenario_mix(self, quality_metrics: DataQualityMetrics) -> Dict[str, float]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∏–∫—Å–∞ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        base_mix = {
            'normal_training': 0.6,
            'difficult_convergence': 0.2,
            'fast_learning': 0.15,
            'instability_recovery': 0.05
        }
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        if quality_metrics.successful_episodes / max(1, quality_metrics.total_episodes) < 0.4:
            # –ú–∞–ª–æ —É—Å–ø–µ—à–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤ - –±–æ–ª—å—à–µ fast_learning
            base_mix['fast_learning'] += 0.2
            base_mix['normal_training'] -= 0.1
            base_mix['difficult_convergence'] -= 0.1
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        total = sum(base_mix.values())
        return {k: v / total for k, v in base_mix.items()}
    
    def _balance_success_ratio(self):
        """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —É—Å–ø–µ—Ö–∞/–Ω–µ—É–¥–∞—á–∏"""
        current_success_rate = self.real_episodes_count and sum(
            1 for ep in self.episodes_storage if ep.get('success', False)
        ) / len(self.episodes_storage)
        
        target_success_rate = 0.6
        
        if current_success_rate < target_success_rate:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–µ —É—Å–ø–µ—à–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
            scenario_mix = {
                'normal_training': 0.7,
                'fast_learning': 0.3,
                'difficult_convergence': 0.0,
                'instability_recovery': 0.0
            }
        else:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª—å—à–µ —Å–ª–æ–∂–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
            scenario_mix = {
                'normal_training': 0.3,
                'fast_learning': 0.1,
                'difficult_convergence': 0.4,
                'instability_recovery': 0.2
            }
        
        balance_episodes = min(50, self.config.min_episodes_target // 10)
        synthetic_episodes = self.synthetic_generator.generate_episodes(balance_episodes, scenario_mix)
        
        for episode in synthetic_episodes:
            self.episodes_storage.append(episode)
            self.synthetic_episodes_count += 1
    
    def _generate_phase_specific_data(self, phase: str, count: int):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ–∞–∑—ã"""
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Ñ–∞–∑—É
        phase_scenarios = {
            'PRE_ALIGNMENT': ['instability_recovery', 'difficult_convergence'],
            'ALIGNMENT_LEARNING': ['normal_training', 'difficult_convergence'],
            'REFINEMENT': ['normal_training', 'fast_learning'],
            'CONVERGENCE': ['fast_learning', 'normal_training']
        }
        
        scenarios = phase_scenarios.get(phase, ['normal_training'])
        scenario_mix = {scenario: 1.0 / len(scenarios) for scenario in scenarios}
        
        episodes = self.synthetic_generator.generate_episodes(count, scenario_mix)
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω—É–∂–Ω—É—é —Ñ–∞–∑—É
        for episode in episodes:
            episode['initial_phase'] = phase
            self.episodes_storage.append(episode)
            self.synthetic_episodes_count += 1
    
    def _get_data_recommendations(self, quality_metrics: DataQualityMetrics) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö"""
        recommendations = []
        
        if quality_metrics.total_episodes < self.config.min_episodes_target:
            recommendations.append(f"–ù–∞–∫–æ–ø–∏—Ç–µ –µ—â–µ {self.config.min_episodes_target - quality_metrics.total_episodes} —ç–ø–∏–∑–æ–¥–æ–≤")
        
        if quality_metrics.quality_score < 0.7:
            recommendations.append("–£–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏—é –∏ –æ—á–∏—Å—Ç–∫—É")
        
        if quality_metrics.diversity_score < 0.6:
            recommendations.append("–£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –æ–±—É—á–µ–Ω–∏—è")
        
        if quality_metrics.data_freshness < 0.5:
            recommendations.append("–û–±–Ω–æ–≤–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –±–æ–ª–µ–µ —Å–≤–µ–∂–∏–º–∏ —ç–ø–∏–∑–æ–¥–∞–º–∏")
        
        return recommendations


def run_data_collection_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("üìö META-LEARNING DATA COLLECTOR DEMO")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ç–æ—Ä–∞
    config = DataCollectionConfig(min_episodes_target=100, max_episodes_storage=200)
    collector = MetaLearningDataCollector(config)
    
    print(f"üìä –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {len(collector.episodes_storage)} —ç–ø–∏–∑–æ–¥–æ–≤")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    print("\nü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    collector.generate_synthetic_data(50)
    
    # –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞
    print("\nüîß –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
    collector.enhance_data_quality()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
    stats = collector.get_data_statistics()
    
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤: {stats['data_counts']['total_episodes']}")
    print(f"   ‚Ä¢ –†–µ–∞–ª—å–Ω—ã—Ö: {stats['data_counts']['real_episodes']}")
    print(f"   ‚Ä¢ –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö: {stats['data_counts']['synthetic_episodes']}")
    print(f"   ‚Ä¢ –ü—Ä–æ–≥—Ä–µ—Å—Å –∫ —Ü–µ–ª–∏: {stats['progress_to_target']*100:.1f}%")
    print(f"   ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {stats['quality_metrics']['quality_score']:.2f}")
    print(f"   ‚Ä¢ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {stats['quality_metrics']['diversity_score']:.2f}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if stats['recommendations']:
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for rec in stats['recommendations']:
            print(f"   ‚Ä¢ {rec}")
    
    # –≠–∫—Å–ø–æ—Ä—Ç
    print("\nüì§ –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö...")
    export_path = collector.export_data('json')
    print(f"   ‚Ä¢ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤: {export_path}")
    
    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    return collector


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    run_data_collection_demo() 