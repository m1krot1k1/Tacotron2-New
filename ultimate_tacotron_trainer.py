#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üèÜ ULTIMATE ENHANCED TACOTRON TRAINER
–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –í–°–ï–• –ª—É—á—à–∏—Ö —Ä–µ—à–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –æ–±—É—á–µ–Ω–∏—è

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ª—É—á—à–µ–µ –∏–∑:
- EnhancedTacotronTrainer (—Ñ–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
- train.py (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
- smart_tuner_main.py (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
- train_with_auto_fixes.py (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è)

–†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
- 'simple': –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- 'enhanced': –ü–æ–ª–Ω–æ–µ —Ñ–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º  
- 'auto_optimized': –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è + –æ–±—É—á–µ–Ω–∏–µ
- 'ultimate': –í—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ + –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.utils.data
from torch.utils.data import DataLoader
import numpy as np
import logging
import copy
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import yaml
import argparse

# –ò–º–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from model import Tacotron2
from loss_function import Tacotron2Loss
from hparams import create_hparams
from audio_quality_enhancer import AudioQualityEnhancer

# üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ò–ó TRAIN.PY
try:
    from smart_tuner.gradient_clipper import AdaptiveGradientClipper, get_global_clipper, set_global_clipper
    GRADIENT_CLIPPER_AVAILABLE = True
except ImportError:
    GRADIENT_CLIPPER_AVAILABLE = False
    logging.warning("AdaptiveGradientClipper –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

try:
    from smart_tuner.smart_lr_adapter import SmartLRAdapter, get_global_lr_adapter, set_global_lr_adapter  
    SMART_LR_AVAILABLE = True
except ImportError:
    SMART_LR_AVAILABLE = False
    logging.warning("SmartLRAdapter –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# ü§ñ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø
try:
    from smart_tuner.auto_fix_manager import AutoFixManager
    AUTO_FIX_AVAILABLE = True
except ImportError:
    AUTO_FIX_AVAILABLE = False
    logging.warning("AutoFixManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
try:
    from smart_tuner.optimization_engine import OptimizationEngine
    from smart_tuner.smart_tuner_integration import SmartTunerIntegration
    OPTIMIZATION_AVAILABLE = True
except ImportError:
    OPTIMIZATION_AVAILABLE = False
    logging.warning("OptimizationEngine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# üì± –ú–û–ù–ò–¢–û–†–ò–ù–ì –ò –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
try:
    from smart_tuner.telegram_monitor_enhanced import TelegramMonitorEnhanced
    from debug_reporter import initialize_debug_reporter, get_debug_reporter
    from alignment_diagnostics import AlignmentDiagnostics
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    logging.warning("–°–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

# üìä –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
try:
    import mlflow
    import mlflow.pytorch
    from torch.utils.tensorboard import SummaryWriter
    LOGGING_AVAILABLE = True
except ImportError:
    LOGGING_AVAILABLE = False
    logging.warning("MLflow/TensorBoard –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

# –ò–º–ø–æ—Ä—Ç –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
try:
    from data_utils import TextMelLoader, TextMelCollate
    DATA_UTILS_AVAILABLE = True
except ImportError:
    DATA_UTILS_AVAILABLE = False
    logging.warning("data_utils –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Å–æ–∑–¥–∞–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é")

class UltimateEnhancedTacotronTrainer:
    """
    üèÜ ULTIMATE Enhanced Tacotron Trainer
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–ï –ª—É—á—à–∏–µ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º:
    - –§–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ (4 —Ñ–∞–∑—ã —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π)
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (AutoFixManager)
    - –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ (AdaptiveGradientClipper)
    - Smart LR –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (15+ –º–µ—Ç—Ä–∏–∫)
    - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    """
    
    def __init__(self, hparams, mode: str = 'enhanced', dataset_info: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ultimate Enhanced Trainer.
        
        Args:
            hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            mode: –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã ('simple', 'enhanced', 'auto_optimized', 'ultimate')
            dataset_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        self.hparams = hparams
        self.mode = mode
        self.dataset_info = dataset_info or {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = self._setup_logger()
        self.logger.info(f"üèÜ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ultimate Enhanced Tacotron Trainer (—Ä–µ–∂–∏–º: {mode})")
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        
        # üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ò–ó TRAIN.PY
        self.adaptive_gradient_clipper = None
        self.smart_lr_adapter = None
        
        # ü§ñ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø
        self.auto_fix_manager = None
        
        # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
        self.optimization_engine = None
        self.smart_tuner = None
        
        # üì± –ú–û–ù–ò–¢–û–†–ò–ù–ì –ò –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
        self.telegram_monitor = None
        self.debug_reporter = None
        self.alignment_diagnostics = None
        
        # üìä –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
        self.tensorboard_writer = None
        self.mlflow_run = None
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.global_step = 0
        self.best_validation_loss = float('inf')
        self.training_metrics_history = []
        self.last_attention_diagonality = 0.0
        
        # üéØ –§–ê–ó–û–í–û–ï –û–ë–£–ß–ï–ù–ò–ï
        self.training_phases = {
            'pre_alignment': {'max_epoch': 500, 'focus': 'attention_learning'},
            'alignment_learning': {'max_epoch': 2000, 'focus': 'attention_stabilization'}, 
            'quality_optimization': {'max_epoch': 3000, 'focus': 'quality_improvement'},
            'fine_tuning': {'max_epoch': 3500, 'focus': 'final_polishing'}
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_components()
        
        self.logger.info("‚úÖ Ultimate Enhanced Tacotron Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Ultimate Trainer."""
        logger = logging.getLogger('UltimateEnhancedTacotronTrainer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π handler
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                '%(asctime)s - [üèÜ Ultimate Trainer] - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            
            # –§–∞–π–ª–æ–≤—ã–π handler
            file_handler = logging.FileHandler('ultimate_training.log')
            file_handler.setFormatter(console_formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def _initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞."""
        self.logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ–∂–∏–º–∞ '{self.mode}'...")
        
        # üìä –í–°–ï–ì–î–ê: –ë–∞–∑–æ–≤–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self._initialize_logging()
        
        # üì± –í–°–ï–ì–î–ê: Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥  
        self._initialize_telegram_monitoring()
        
        if self.mode in ['enhanced', 'auto_optimized', 'ultimate']:
            # üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´
            self._initialize_critical_components()
            
        if self.mode in ['auto_optimized', 'ultimate']:
            # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
            self._initialize_optimization()
            
        if self.mode == 'ultimate':
            # ü§ñ –í–°–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò
            self._initialize_ultimate_features()
    
    def _initialize_logging(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        if not LOGGING_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è –°–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            return
            
        try:
            # TensorBoard
            self.tensorboard_writer = SummaryWriter('logs')
            self.logger.info("‚úÖ TensorBoard writer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # MLflow  
            mlflow.start_run(run_name=f"ultimate_training_{int(time.time())}")
            self.mlflow_run = mlflow.active_run()
            self.logger.info("‚úÖ MLflow run –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    def _initialize_telegram_monitoring(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞."""
        if not MONITORING_AVAILABLE:
            return
            
        try:
            config_path = "smart_tuner/config.yaml"
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                telegram_config = config.get('telegram', {})
                if telegram_config.get('enabled', False):
                    self.telegram_monitor = TelegramMonitorEnhanced(
                        bot_token=telegram_config.get('bot_token'),
                        chat_id=telegram_config.get('chat_id'),
                        enabled=True
                    )
                    self.logger.info("‚úÖ Telegram Monitor Enhanced –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                    
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Telegram: {e}")
    
    def _initialize_critical_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–∑ train.py."""
        self.logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # üîß AdaptiveGradientClipper
        if GRADIENT_CLIPPER_AVAILABLE:
            try:
                self.adaptive_gradient_clipper = AdaptiveGradientClipper(
                    max_norm=1.0,
                    adaptive=True,
                    emergency_threshold=100.0,
                    history_size=1000,
                    percentile=95
                )
                set_global_clipper(self.adaptive_gradient_clipper)
                self.logger.info("‚úÖ AdaptiveGradientClipper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ AdaptiveGradientClipper: {e}")
        
        # ü§ñ AutoFixManager  
        if AUTO_FIX_AVAILABLE:
            try:
                # AutoFixManager –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
                self.logger.info("ü§ñ AutoFixManager –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ—Å–ª–µ –º–æ–¥–µ–ª–∏")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ AutoFixManager: {e}")
        
        # üìä Alignment Diagnostics
        if MONITORING_AVAILABLE:
            try:
                self.alignment_diagnostics = AlignmentDiagnostics()
                self.logger.info("‚úÖ AlignmentDiagnostics –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ AlignmentDiagnostics: {e}")
                
        # üîç Debug Reporter
        if MONITORING_AVAILABLE:
            try:
                self.debug_reporter = initialize_debug_reporter(self.telegram_monitor)
                self.logger.info("‚úÖ Debug Reporter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Debug Reporter: {e}")
    
    def _initialize_optimization(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        if not OPTIMIZATION_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è OptimizationEngine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return
            
        try:
            self.optimization_engine = OptimizationEngine("smart_tuner/config.yaml")
            self.smart_tuner = SmartTunerIntegration()
            self.logger.info("‚úÖ OptimizationEngine –∏ SmartTuner –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
    
    def _initialize_ultimate_features(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π Ultimate —Ä–µ–∂–∏–º–∞."""
        self.logger.info("üèÜ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ultimate –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π...")
        
        # –ó–¥–µ—Å—å –±—É–¥—É—Ç —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Ultimate —Ä–µ–∂–∏–º–∞
        # - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        # - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        # - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã
        # - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑
        
        self.logger.info("‚úÖ Ultimate –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã") 

    def initialize_training(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        self.logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
        # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í (–¥–ª—è —Ä–µ–∂–∏–º–æ–≤ auto_optimized –∏ ultimate)
        if self.mode in ['auto_optimized', 'ultimate'] and self.smart_tuner:
            try:
                original_hparams = vars(self.hparams).copy()
                optimized_hparams = self.smart_tuner.on_training_start(
                    original_hparams, self.dataset_info
                )
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                for key, value in optimized_hparams.items():
                    if hasattr(self.hparams, key):
                        setattr(self.hparams, key, value)
                        
                self.logger.info("‚ú® –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ Smart Tuner")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = Tacotron2(self.hparams).cuda()
        self.logger.info(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {sum(p.numel() for p in self.model.parameters())} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è loss —Ñ—É–Ω–∫—Ü–∏–∏
        self.criterion = Tacotron2Loss(self.hparams)
        self.logger.info("üéØ Enhanced loss function –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.hparams.learning_rate,
            weight_decay=getattr(self.hparams, 'weight_decay', 1e-6)
        )
        self.logger.info("‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # üîß Smart LR Adapter (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if SMART_LR_AVAILABLE:
            try:
                self.smart_lr_adapter = SmartLRAdapter(
                    optimizer=self.optimizer,
                    patience=10,
                    factor=0.5,
                    min_lr=getattr(self.hparams, 'learning_rate_min', 1e-8),
                    max_lr=self.hparams.learning_rate * 2,
                    emergency_factor=0.1,
                    grad_norm_threshold=1000.0,
                    loss_nan_threshold=1e6
                )
                set_global_lr_adapter(self.smart_lr_adapter)
                self.logger.info("‚úÖ Smart LR Adapter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart LR Adapter: {e}")
        
        # ü§ñ AutoFixManager (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞)
        if AUTO_FIX_AVAILABLE and self.mode in ['enhanced', 'auto_optimized', 'ultimate']:
            try:
                self.auto_fix_manager = AutoFixManager(
                    model=self.model,
                    optimizer=self.optimizer,
                    hparams=self.hparams,
                    telegram_monitor=self.telegram_monitor
                )
                self.logger.info("‚úÖ AutoFixManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ AutoFixManager: {e}")
    
    def get_current_training_phase(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–ø–æ—Ö–∏."""
        for phase, config in self.training_phases.items():
            if self.current_epoch < config['max_epoch']:
                return phase
        return 'fine_tuning'  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Ñ–∞–∑–∞
    
    def adjust_hyperparams_for_phase(self, phase: str):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è."""
        if self.mode not in ['enhanced', 'auto_optimized', 'ultimate']:
            return  # –§–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ä–µ–∂–∏–º–æ–≤
            
        phase_configs = {
            'pre_alignment': {
                'learning_rate_multiplier': 1.2,
                'guide_loss_weight_multiplier': 3.0,
                'batch_size_multiplier': 0.8
            },
            'alignment_learning': {
                'learning_rate_multiplier': 1.0, 
                'guide_loss_weight_multiplier': 2.0,
                'batch_size_multiplier': 1.0
            },
            'quality_optimization': {
                'learning_rate_multiplier': 0.8,
                'guide_loss_weight_multiplier': 1.5,
                'batch_size_multiplier': 1.2
            },
            'fine_tuning': {
                'learning_rate_multiplier': 0.5,
                'guide_loss_weight_multiplier': 1.0,
                'batch_size_multiplier': 1.0
            }
        }
        
        config = phase_configs.get(phase, phase_configs['alignment_learning'])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        if hasattr(self.criterion, 'guide_loss_weight'):
            new_weight = getattr(self.hparams, 'guide_loss_weight', 2.5) * config['guide_loss_weight_multiplier']
            self.criterion.guide_loss_weight = new_weight
            
        self.logger.info(f"üìä –§–∞–∑–∞ '{phase}': –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã "
                        f"lr_mult={config['learning_rate_multiplier']}")
    
    def train_step(self, batch):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏."""
        self.model.train()
        self.optimizer.zero_grad()
        
        # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ batch (TextMelCollate –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
        text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths, ctc_text, ctc_text_lengths, guide_mask = batch
        
        # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
        text_inputs = text_inputs.cuda()
        mel_targets = mel_targets.cuda() 
        gate_targets = gate_targets.cuda()
        
        # üîß –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê MODEL_OUTPUTS (–∏–∑ train.py)
        try:
            x, y = self.model.parse_batch(batch)
            model_outputs = self.model(x)
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ 1-7+ –∑–Ω–∞—á–µ–Ω–∏–π)
            if len(model_outputs) >= 4:
                mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model_outputs[:4]
            elif len(model_outputs) == 3:
                mel_outputs, mel_outputs_postnet, gate_outputs = model_outputs
                alignments = None
            elif len(model_outputs) == 2:
                mel_outputs, mel_outputs_postnet = model_outputs
                gate_outputs = None
                alignments = None
            else:
                mel_outputs = model_outputs[0] if len(model_outputs) > 0 else None
                mel_outputs_postnet = None
                gate_outputs = None
                alignments = None
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ forward pass: {e}")
            return {'total_loss': torch.tensor(float('inf'))}
        
        # üîç –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê
        attention_diagonality = 0.0
        gate_accuracy = 0.0
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º attention_diagonality
            if alignments is not None:
                attention_matrix = alignments.detach().cpu().numpy()
                if attention_matrix.ndim == 3:
                    batch_diagonalities = []
                    for b in range(attention_matrix.shape[0]):
                        attn = attention_matrix[b]
                        if attn.sum() > 0:
                            attn = attn / attn.sum(axis=1, keepdims=True)
                        
                        min_dim = min(attn.shape[0], attn.shape[1])
                        diagonal_elements = [attn[i, i] for i in range(min_dim)]
                        batch_diagonalities.append(np.mean(diagonal_elements) if diagonal_elements else 0.0)
                    
                    attention_diagonality = np.mean(batch_diagonalities) if batch_diagonalities else 0.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º gate_accuracy
            if gate_outputs is not None:
                gate_pred = (gate_outputs > 0.5).float()
                gate_targets_binary = (gate_targets > 0.5).float()
                correct = (gate_pred == gate_targets_binary).float().mean()
                gate_accuracy = correct.item()
                
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        self.last_attention_diagonality = attention_diagonality
        
        # üéØ –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê GUIDED ATTENTION
        if hasattr(self.criterion, 'guide_loss_weight') and self.global_step > 0:
            current_weight = self.criterion.guide_loss_weight
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ù–ï –î–û–ü–£–°–ö–ê–ï–ú –í–ó–†–´–í–ê –î–û 100.0!
            if attention_diagonality < 0.05:
                # –û—á–µ–Ω—å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ - –ù–ï –ë–û–õ–ï–ï 15.0!
                new_weight = min(current_weight * 1.5, 15.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.warning(f"üö® –û—Å—Ç–æ—Ä–æ–∂–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
            elif attention_diagonality < 0.1:
                # –£–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ - –ù–ï –ë–û–õ–ï–ï 12.0!
                new_weight = min(current_weight * 1.3, 12.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.warning(f"üö® –£–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
            elif attention_diagonality > 0.7:
                # –°–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–≥–¥–∞ attention —É–∂–µ —Ö–æ—Ä–æ—à–µ–µ
                new_weight = max(current_weight * 0.9, 1.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.info(f"üìâ –°–Ω–∏–∂–µ–Ω–∏–µ guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
            
            # –ê–í–ê–†–ò–ô–ù–ê–Ø –ó–ê–©–ò–¢–ê: –µ—Å–ª–∏ –∫–∞–∫-—Ç–æ –¥–æ—à–ª–æ –¥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            if self.criterion.guide_loss_weight > 20.0:
                self.criterion.guide_loss_weight = 10.0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –¥–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ
                self.logger.error(f"üö® –ê–í–ê–†–ò–ô–ù–´–ô –°–ë–†–û–° guided attention weight –¥–æ 10.0!")
        
        # üéØ –í–´–ß–ò–°–õ–ï–ù–ò–ï LOSS –° –ë–ï–ó–û–ü–ê–°–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–û–ô
        try:
            loss_components = self.criterion(
                model_outputs, 
                (mel_targets, gate_targets),
                attention_weights=alignments,
                gate_outputs=gate_outputs
            )
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            if isinstance(loss_components, (list, tuple)):
                if len(loss_components) == 4:
                    mel_loss, gate_loss, guide_loss, emb_loss = loss_components
                    loss = mel_loss + gate_loss + guide_loss + emb_loss
                    loss_dict = {
                        'mel_loss': mel_loss.item(),
                        'gate_loss': gate_loss.item(), 
                        'guide_loss': guide_loss.item(),
                        'emb_loss': emb_loss.item(),
                        'total_loss': loss.item()
                    }
                else:
                    loss = loss_components[0]
                    loss_dict = {'total_loss': loss.item()}
            else:
                loss = loss_components
                loss_dict = {'total_loss': loss.item()}
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss: {e}")
            return {'total_loss': torch.tensor(float('inf'))}
        
        # Backward pass
        try:
            loss.backward()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ backward pass: {e}")
            return {'total_loss': torch.tensor(float('inf'))}
        
        # üîß –ü–†–û–î–í–ò–ù–£–¢–û–ï –£–ü–†–ê–í–õ–ï–ù–ò–ï –ì–†–ê–î–ò–ï–ù–¢–ê–ú–ò
        if self.adaptive_gradient_clipper:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º AdaptiveGradientClipper
            was_clipped, grad_norm, clip_threshold = self.adaptive_gradient_clipper.clip_gradients(
                self.model, self.global_step
            )
            
            if was_clipped:
                self.logger.info(f"üîß –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ–±—Ä–µ–∑–∞–Ω—ã: {grad_norm:.2f} ‚Üí {clip_threshold:.2f}")
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∫–ª–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ –∞–ª–µ—Ä—Ç–∞–º–∏
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                getattr(self.hparams, 'grad_clip_thresh', 1.0)
            )
            
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–µ—Ä—Ç—ã –¥–ª—è –≤—ã—Å–æ–∫–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            if grad_norm > 10.0:
                self.logger.warning(f"üö® –í–´–°–û–ö–ê–Ø –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {grad_norm:.2f}")
            if grad_norm > 100.0:
                self.logger.error(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {grad_norm:.2f}")
        
        # ü§ñ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø
        if self.auto_fix_manager:
            try:
                fix_metrics = {
                    'grad_norm': float(grad_norm),
                    'attention_diagonality': attention_diagonality,
                    'gate_accuracy': gate_accuracy,
                    'loss': float(loss.item()),
                    'mel_loss': loss_dict.get('mel_loss', 0),
                    'gate_loss': loss_dict.get('gate_loss', 0),
                    'guide_loss': loss_dict.get('guide_loss', 0)
                }
                
                applied_fixes = self.auto_fix_manager.analyze_and_fix(
                    step=self.global_step,
                    metrics=fix_metrics,
                    loss=loss
                )
                
                if applied_fixes:
                    self.logger.info(f"üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–æ {len(applied_fixes)} –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π")
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ AutoFixManager: {e}")
        
        # üîß Smart LR Adapter
        if self.smart_lr_adapter:
            try:
                lr_changed = self.smart_lr_adapter.step(
                    loss=float(loss.item()),
                    grad_norm=float(grad_norm),
                    step=self.global_step
                )
                if lr_changed:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ LR –Ω–µ —Å—Ç–∞–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–º
                    if current_lr < 1e-7:
                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–π LR
                        for param_group in self.optimizer.param_groups:
                            param_group['lr'] = 1e-6
                        current_lr = 1e-6
                        self.logger.info(f"üîÑ LR –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è: {current_lr:.2e}")
                    else:
                        self.logger.info(f"üîÑ Smart LR –∞–¥–∞–ø—Ç–∞—Ü–∏—è: LR –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ {current_lr:.2e}")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart LR Adapter: {e}")
        
        # Optimizer step
        try:
            self.optimizer.step()
            self.global_step += 1
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ optimizer step: {e}")
            return {'total_loss': torch.tensor(float('inf'))}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        loss_dict.update({
            'grad_norm': grad_norm,
            'attention_diagonality': attention_diagonality,
            'gate_accuracy': gate_accuracy,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        })
        
        return loss_dict
    
    def validate_step(self, val_loader):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º."""
        self.model.eval()
        
        val_losses = []
        attention_scores = []
        gate_accuracies = []
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∫ –≤ train_step, –Ω–æ –±–µ–∑ backward
                    text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths, ctc_text, ctc_text_lengths, guide_mask = batch
                    
                    text_inputs = text_inputs.cuda()
                    mel_targets = mel_targets.cuda()
                    gate_targets = gate_targets.cuda()
                    
                    x, y = self.model.parse_batch(batch)
                    model_outputs = self.model(x)
                    
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
                    loss_components = self.criterion(model_outputs, (mel_targets, gate_targets))
                    
                    if isinstance(loss_components, (list, tuple)):
                        val_loss = sum(loss_components)
                    else:
                        val_loss = loss_components
                        
                    val_losses.append(val_loss.item())
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
                    if len(model_outputs) >= 4:
                        alignments = model_outputs[3]
                        gate_outputs = model_outputs[2]
                        
                        # Attention diagonality
                        if alignments is not None:
                            attention_matrix = alignments.detach().cpu().numpy()
                            if attention_matrix.ndim == 3:
                                diag_score = np.mean([np.trace(attention_matrix[b]) / min(attention_matrix[b].shape) 
                                                    for b in range(attention_matrix.shape[0])])
                                attention_scores.append(diag_score)
                        
                        # Gate accuracy
                        if gate_outputs is not None:
                            gate_pred = (gate_outputs > 0.5).float()
                            gate_targets_binary = (gate_targets > 0.5).float()
                            accuracy = (gate_pred == gate_targets_binary).float().mean()
                            gate_accuracies.append(accuracy.item())
                            
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                    continue
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation_metrics = {
            'val_loss': np.mean(val_losses) if val_losses else float('inf'),
            'val_attention_diagonality': np.mean(attention_scores) if attention_scores else 0.0,
            'val_gate_accuracy': np.mean(gate_accuracies) if gate_accuracies else 0.0
        }
        
        return validation_metrics 

    def train(self, train_loader, val_loader, num_epochs: int = 3500, max_steps: Optional[int] = None):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º.
        
        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            max_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
        """
        self.logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º Ultimate Enhanced Training (—Ä–µ–∂–∏–º: {self.mode})")
        if max_steps:
            self.logger.info(f"üî¨ –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ {max_steps} —à–∞–≥–æ–≤")
        else:
            self.logger.info(f"üìä –≠–ø–æ—Ö: {num_epochs}, –ë–∞—Ç—á–µ–π: {len(train_loader)}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.initialize_training()
        
        # üì± –û—Ç–ø—Ä–∞–≤–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        if self.telegram_monitor:
            try:
                if hasattr(self.telegram_monitor, 'send_training_start_notification'):
                    self.telegram_monitor.send_training_start_notification(
                        hparams=self.hparams,
                        dataset_info=self.dataset_info
                    )
                elif hasattr(self.telegram_monitor, 'send_message'):
                    self.telegram_monitor.send_message("üöÄ –ù–∞—á–∏–Ω–∞—é Ultimate Enhanced Training!")
                elif hasattr(self.telegram_monitor, 'send_training_notification'):
                    self.telegram_monitor.send_training_notification("üöÄ –ù–∞—á–∏–Ω–∞—é Ultimate Enhanced Training!")
                else:
                    self.logger.debug("Telegram monitor –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É —Å–æ–æ–±—â–µ–Ω–∏–π")
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        epoch_start_time = time.time()
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = getattr(self.hparams, 'early_stopping_patience', 10)
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        training_history = []
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            epoch_start = time.time()
            
            # üéØ –§–ê–ó–û–í–û–ï –û–ë–£–ß–ï–ù–ò–ï (–¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ä–µ–∂–∏–º–æ–≤)
            current_phase = self.get_current_training_phase()
            if epoch % 100 == 0 or epoch < 5:  # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
                self.adjust_hyperparams_for_phase(current_phase)
            
            # üìä –û–ë–£–ß–ï–ù–ò–ï –ó–ê –≠–ü–û–•–£
            epoch_losses = []
            epoch_grad_norms = []
            epoch_attention_scores = []
            epoch_gate_accuracies = []
            
            for batch_idx, batch in enumerate(train_loader):
                try:
                    # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                    step_metrics = self.train_step(batch)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    epoch_losses.append(step_metrics['total_loss'])
                    epoch_grad_norms.append(step_metrics.get('grad_norm', 0))
                    epoch_attention_scores.append(step_metrics.get('attention_diagonality', 0))
                    epoch_gate_accuracies.append(step_metrics.get('gate_accuracy', 0))
                    
                    # üìä –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ö–ê–ñ–î–´–ï 100 –®–ê–ì–û–í (–∏–ª–∏ –∫–∞–∂–¥—ã–µ 10 –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ)
                    log_frequency = 10 if max_steps else 100
                    if self.global_step % log_frequency == 0:
                        self._log_training_step(step_metrics, epoch, batch_idx)
                    
                    # üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –ú–û–ù–ò–¢–û–†–ò–ù–ì
                    if step_metrics['total_loss'] > 100 or step_metrics.get('grad_norm', 0) > 1000:
                        self.logger.error(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–û–ö–ê–ó–ê–¢–ï–õ–ò –Ω–∞ —à–∞–≥–µ {self.global_step}!")
                        self.logger.error(f"Loss: {step_metrics['total_loss']:.2f}, Grad Norm: {step_metrics.get('grad_norm', 0):.2f}")
                        
                        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞–≤–∞—Ä–∏–π–Ω—ã–µ –º–µ—Ä—ã
                        if self.mode in ['auto_optimized', 'ultimate'] and self.auto_fix_manager:
                            emergency_fixes = self.auto_fix_manager.emergency_intervention(
                                step=self.global_step,
                                critical_metrics=step_metrics
                            )
                            if emergency_fixes:
                                self.logger.info(f"üîß –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –∞–≤–∞—Ä–∏–π–Ω—ã–µ –º–µ—Ä—ã: {emergency_fixes}")
                    
                    # üî¨ –ü–†–û–í–ï–†–ö–ê –õ–ò–ú–ò–¢–ê –®–ê–ì–û–í –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
                    if max_steps and self.global_step >= max_steps:
                        self.logger.info(f"üî¨ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {max_steps} —à–∞–≥–æ–≤ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
                        break
                        
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —à–∞–≥–µ –æ–±—É—á–µ–Ω–∏—è {batch_idx}: {e}")
                    continue
            
            # üìä –ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ò–ï –ú–ï–¢–†–ò–ö –≠–ü–û–•–ò
            epoch_metrics = {
                'epoch': epoch,
                'train_loss': np.mean(epoch_losses) if epoch_losses else float('inf'),
                'train_grad_norm': np.mean(epoch_grad_norms) if epoch_grad_norms else 0,
                'train_attention_diagonality': np.mean(epoch_attention_scores) if epoch_attention_scores else 0,
                'train_gate_accuracy': np.mean(epoch_gate_accuracies) if epoch_gate_accuracies else 0,
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'phase': current_phase,
                'epoch_time': time.time() - epoch_start
            }
            
            # üìä –í–ê–õ–ò–î–ê–¶–ò–Ø –ö–ê–ñ–î–´–ï 10 –≠–ü–û–• (–∏–ª–∏ —Å—Ä–∞–∑—É –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ)
            validation_frequency = 1 if max_steps else 10
            if epoch % validation_frequency == 0 or epoch < 5:
                try:
                    val_metrics = self.validate_step(val_loader)
                    epoch_metrics.update(val_metrics)
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è
                    if val_metrics['val_loss'] < best_val_loss:
                        best_val_loss = val_metrics['val_loss']
                        patience_counter = 0
                        self._save_checkpoint(epoch, is_best=True)
                        self.logger.info(f"üéâ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: val_loss = {best_val_loss:.4f}")
                    else:
                        patience_counter += 1
                        
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                    val_metrics = {'val_loss': float('inf')}
                    epoch_metrics.update(val_metrics)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            training_history.append(epoch_metrics)
            
            # üìä –ü–û–î–†–û–ë–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –≠–ü–û–•–ò
            self._log_epoch_summary(epoch_metrics)
            
            # üì± TELEGRAM –£–í–ï–î–û–ú–õ–ï–ù–ò–Ø (–∫–∞–∂–¥—ã–µ 5 —à–∞–≥–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ, –∫–∞–∂–¥—ã–µ 50 —ç–ø–æ—Ö –≤ –æ–±—ã—á–Ω–æ–º)
            telegram_frequency = 5 if max_steps else 50
            if epoch % telegram_frequency == 0 or epoch < 5 or patience_counter > max_patience // 2:
                self._send_epoch_telegram_update(epoch_metrics)
            
            # üîÑ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø (–¥–ª—è ultimate —Ä–µ–∂–∏–º–∞)
            if self.mode == 'ultimate' and epoch > 10 and len(training_history) >= 10:
                self._perform_intelligent_adjustments(training_history[-10:])  # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 10 —ç–ø–æ—Ö
            
            # üõë EARLY STOPPING
            if patience_counter >= max_patience:
                self.logger.info(f"üõë Early stopping –ø–æ—Å–ª–µ {patience_counter} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π")
                break
                
            # üî¨ –í–´–•–û–î –ò–ó –¶–ò–ö–õ–ê –ü–†–ò –î–û–°–¢–ò–ñ–ï–ù–ò–ò –õ–ò–ú–ò–¢–ê –®–ê–ì–û–í
            if max_steps and self.global_step >= max_steps:
                self.logger.info(f"üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –Ω–∞ {self.global_step} —à–∞–≥–∞—Ö")
                break
            
            # üîß –ü–ï–†–ò–û–î–ò–ß–ï–°–ö–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï
            save_frequency = 20 if max_steps else 100
            if epoch % save_frequency == 0:
                self._save_checkpoint(epoch, is_best=False)
                
        # üéâ –ó–ê–í–ï–†–®–ï–ù–ò–ï –û–ë–£–ß–ï–ù–ò–Ø
        total_time = time.time() - epoch_start_time
        self._finalize_training(training_history, total_time)
        
        return training_history
    
    def _log_training_step(self, metrics: Dict, epoch: int, batch_idx: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è."""
        # Console logging
        self.logger.info(
            f"–≠–ø–æ—Ö–∞ {epoch}, –ë–∞—Ç—á {batch_idx}, –®–∞–≥ {self.global_step}: "
            f"Loss: {metrics['total_loss']:.4f}, "
            f"Grad: {metrics.get('grad_norm', 0):.2f}, "
            f"Attn: {metrics.get('attention_diagonality', 0):.3f}, "
            f"Gate: {metrics.get('gate_accuracy', 0):.3f}"
        )
        
        # TensorBoard logging
        if self.tensorboard_writer:
            try:
                self.tensorboard_writer.add_scalar('Train/Loss', metrics['total_loss'], self.global_step)
                self.tensorboard_writer.add_scalar('Train/GradNorm', metrics.get('grad_norm', 0), self.global_step)
                self.tensorboard_writer.add_scalar('Train/AttentionDiagonality', metrics.get('attention_diagonality', 0), self.global_step)
                self.tensorboard_writer.add_scalar('Train/GateAccuracy', metrics.get('gate_accuracy', 0), self.global_step)
                self.tensorboard_writer.add_scalar('Train/LearningRate', metrics.get('learning_rate', 0), self.global_step)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ TensorBoard: {e}")
        
        # MLflow logging
        if self.mlflow_run:
            try:
                mlflow.log_metrics({
                    'train_loss': metrics['total_loss'],
                    'grad_norm': metrics.get('grad_norm', 0),
                    'attention_diagonality': metrics.get('attention_diagonality', 0),
                    'gate_accuracy': metrics.get('gate_accuracy', 0)
                }, step=self.global_step)
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ MLflow: {e}")
    
    def _log_epoch_summary(self, metrics: Dict):
        """–ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–æ—Ö–∏."""
        epoch = metrics['epoch']
        
        self.logger.info("=" * 80)
        self.logger.info(f"üìä –≠–ü–û–•–ê {epoch} –ó–ê–í–ï–†–®–ï–ù–ê")
        self.logger.info(f"üéØ –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è: {metrics.get('phase', 'unknown')}")
        self.logger.info(f"üìà Train Loss: {metrics['train_loss']:.4f}")
        
        if 'val_loss' in metrics:
            self.logger.info(f"üìâ Val Loss: {metrics['val_loss']:.4f}")
            self.logger.info(f"üéØ Val Attention: {metrics.get('val_attention_diagonality', 0):.3f}")
            self.logger.info(f"üéØ Val Gate Acc: {metrics.get('val_gate_accuracy', 0):.3f}")
        
        self.logger.info(f"üîß Grad Norm: {metrics['train_grad_norm']:.2f}")
        self.logger.info(f"üìä Attention Diag: {metrics['train_attention_diagonality']:.3f}")
        self.logger.info(f"üéØ Gate Accuracy: {metrics['train_gate_accuracy']:.3f}")
        self.logger.info(f"‚öôÔ∏è Learning Rate: {metrics['learning_rate']:.2e}")
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {metrics['epoch_time']:.1f}s")
        self.logger.info("=" * 80)
    
    def _send_epoch_telegram_update(self, metrics: Dict):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ Telegram."""
        if not self.telegram_monitor:
            return
            
        try:
            message = f"""üèÜ Ultimate Tacotron Training

üìä –≠–ø–æ—Ö–∞: {metrics['epoch']} | –§–∞–∑–∞: {metrics.get('phase', 'unknown')}
üìà Train Loss: {metrics['train_loss']:.4f}
üìâ Val Loss: {metrics.get('val_loss', 'N/A')}
üéØ Attention: {metrics['train_attention_diagonality']:.3f}
üîß Grad Norm: {metrics['train_grad_norm']:.2f}
‚öôÔ∏è LR: {metrics['learning_rate']:.2e}
‚è±Ô∏è –í—Ä–µ–º—è: {metrics['epoch_time']:.1f}s

üöÄ –†–µ–∂–∏–º: {self.mode}"""
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
            if hasattr(self.telegram_monitor, 'send_message'):
                self.telegram_monitor.send_message(message)
            elif hasattr(self.telegram_monitor, 'send_epoch_update'):
                self.telegram_monitor.send_epoch_update(metrics)
            else:
                self.logger.debug("Telegram monitor –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É —Å–æ–æ–±—â–µ–Ω–∏–π")
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
    
    def _perform_intelligent_adjustments(self, recent_history: List[Dict]):
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è Ultimate —Ä–µ–∂–∏–º–∞."""
        if not recent_history:
            return
            
        self.logger.info("üß† –í—ã–ø–æ–ª–Ω—è—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        losses = [h['train_loss'] for h in recent_history]
        attention_scores = [h['train_attention_diagonality'] for h in recent_history]
        grad_norms = [h['train_grad_norm'] for h in recent_history]
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫
        adjustments_made = []
        
        # –ü—Ä–æ–±–ª–µ–º–∞ 1: –°—Ç–∞–≥–Ω–∞—Ü–∏—è loss
        if len(losses) > 10:
            recent_loss_trend = np.mean(losses[-5:]) - np.mean(losses[-10:-5])
            if abs(recent_loss_trend) < 0.001:  # –°—Ç–∞–≥–Ω–∞—Ü–∏—è
                if hasattr(self.criterion, 'guide_loss_weight'):
                    old_weight = self.criterion.guide_loss_weight
                    self.criterion.guide_loss_weight *= 1.5
                    adjustments_made.append(f"–£–≤–µ–ª–∏—á–µ–Ω guide_loss_weight: {old_weight:.1f} ‚Üí {self.criterion.guide_loss_weight:.1f}")
        
        # –ü—Ä–æ–±–ª–µ–º–∞ 2: –ù–∏–∑–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
        avg_attention = np.mean(attention_scores[-10:]) if len(attention_scores) >= 10 else 0
        if avg_attention < 0.3:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ñ–æ–∫—É—Å –Ω–∞ attention –ë–ï–ó–û–ü–ê–°–ù–û
            if hasattr(self.criterion, 'guide_loss_weight'):
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ - –ù–ï –ë–û–õ–ï–ï 20.0!
                old_weight = self.criterion.guide_loss_weight
                self.criterion.guide_loss_weight = min(old_weight * 1.5, 15.0)  # –ú–∞–∫—Å–∏–º—É–º 15.0
                adjustments_made.append(f"–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {old_weight:.1f} ‚Üí {self.criterion.guide_loss_weight:.1f}")
        
        # –ü—Ä–æ–±–ª–µ–º–∞ 3: –í—ã—Å–æ–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        avg_grad_norm = np.mean(grad_norms[-10:]) if len(grad_norms) >= 10 else 0
        if avg_grad_norm > 5.0:
            # –°–Ω–∏–∂–∞–µ–º learning rate
            for param_group in self.optimizer.param_groups:
                old_lr = param_group['lr']
                param_group['lr'] *= 0.8
                adjustments_made.append(f"–°–Ω–∏–∂–µ–Ω LR: {old_lr:.2e} ‚Üí {param_group['lr']:.2e}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫
        if adjustments_made:
            self.logger.info("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏:")
            for adj in adjustments_made:
                self.logger.info(f"  ‚Ä¢ {adj}")
                
            # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –≤ Telegram
            if self.telegram_monitor:
                message = "üß† –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏:\n" + "\n".join(f"‚Ä¢ {adj}" for adj in adjustments_made)
                self.telegram_monitor.send_message(message)
    
    def _save_checkpoint(self, epoch: int, is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
        try:
            checkpoint = {
                'epoch': epoch,
                'global_step': self.global_step,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'best_validation_loss': self.best_validation_loss,
                'hparams': vars(self.hparams),
                'training_mode': self.mode
            }
            
            filename = f"checkpoint_epoch_{epoch}.pt"
            if is_best:
                filename = "best_model.pt"
                
            torch.save(checkpoint, filename)
            self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {filename}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
    
    def _finalize_training(self, training_history: List[Dict], total_time: float):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å."""
        self.logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_stats = {
            'total_epochs': len(training_history),
            'total_time_hours': total_time / 3600,
            'best_train_loss': min(h['train_loss'] for h in training_history),
            'final_attention_score': training_history[-1]['train_attention_diagonality'],
            'final_gate_accuracy': training_history[-1]['train_gate_accuracy']
        }
        
        self.logger.info(f"üìä –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {final_stats['total_epochs']}")
        self.logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {final_stats['total_time_hours']:.1f} —á–∞—Å–æ–≤")
        self.logger.info(f"üèÜ –õ—É—á—à–∏–π train loss: {final_stats['best_train_loss']:.4f}")
        self.logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: {final_stats['final_attention_score']:.3f}")
        self.logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å gate: {final_stats['final_gate_accuracy']:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        try:
            import json
            import numpy as np
            
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ numpy —Ç–∏–ø–æ–≤ –∏ type –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã
            def convert_numpy_types(obj):
                if isinstance(obj, (np.integer, np.floating)):
                    return obj.item()
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, type):  # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º type –æ–±—ä–µ–∫—Ç—ã
                    return str(obj)
                elif hasattr(obj, '__dict__') and not isinstance(obj, (str, int, float, bool)):
                    # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    return str(obj)
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                return obj
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            report_data = {
                'final_stats': convert_numpy_types(final_stats),
                'training_history': convert_numpy_types(training_history),
                'mode': self.mode,
                'hparams': convert_numpy_types(vars(self.hparams))
            }
            
            with open('ultimate_training_report.json', 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            self.logger.info("üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: ultimate_training_report.json")
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        if self.telegram_monitor:
            try:
                message = f"""üéâ ULTIMATE TRAINING –ó–ê–í–ï–†–®–ï–ù–û!

üìä –≠–ø–æ—Ö: {final_stats['total_epochs']}
‚è±Ô∏è –í—Ä–µ–º—è: {final_stats['total_time_hours']:.1f}—á
üèÜ –õ—É—á—à–∏–π Loss: {final_stats['best_train_loss']:.4f}
üéØ –í–Ω–∏–º–∞–Ω–∏–µ: {final_stats['final_attention_score']:.3f}
üéØ Gate Acc: {final_stats['final_gate_accuracy']:.3f}

üöÄ –†–µ–∂–∏–º: {self.mode}
‚úÖ –í—Å–µ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ!"""
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                if hasattr(self.telegram_monitor, 'send_message'):
                    self.telegram_monitor.send_message(message)
                else:
                    self.logger.debug("Telegram monitor –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É —Å–æ–æ–±—â–µ–Ω–∏–π")
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
        
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–≥–≥–µ—Ä–æ–≤
        if self.tensorboard_writer:
            self.tensorboard_writer.close()
        if self.mlflow_run:
            mlflow.end_run()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Ultimate Enhanced Tacotron Trainer."""
    parser = argparse.ArgumentParser(description='Ultimate Enhanced Tacotron Trainer')
    parser.add_argument('--mode', choices=['simple', 'enhanced', 'auto_optimized', 'ultimate'], 
                       default='enhanced', help='–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--config', type=str, default='hparams.py', help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--epochs', type=int, default=3500, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--max-steps', type=int, default=None, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
    parser.add_argument('--dataset-path', type=str, required=True, help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    hparams = create_hparams()
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
    dataset_info = {}
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫ Ultimate Enhanced Tacotron Trainer –≤ —Ä–µ–∂–∏–º–µ '{args.mode}'")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ trainer'–∞
    trainer = UltimateEnhancedTacotronTrainer(
        hparams=hparams,
        mode=args.mode,
        dataset_info=dataset_info
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataLoader'–æ–≤
    try:
        print("üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataLoader'–æ–≤...")
        
        if DATA_UTILS_AVAILABLE:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
            trainset = TextMelLoader(args.dataset_path, hparams)
            valset = TextMelLoader(args.dataset_path.replace('train', 'val'), hparams)
            collate_fn = TextMelCollate(hparams.n_frames_per_step)
            
            train_loader = DataLoader(
                trainset, 
                num_workers=1, 
                shuffle=True,
                sampler=None,
                batch_size=hparams.batch_size, 
                pin_memory=False,
                drop_last=True, 
                collate_fn=collate_fn
            )
            
            val_loader = DataLoader(
                valset, 
                num_workers=1, 
                shuffle=False,
                sampler=None,
                batch_size=hparams.batch_size, 
                pin_memory=False,
                collate_fn=collate_fn
            )
            
            print(f"‚úÖ DataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä Train samples: {len(trainset)}")
            print(f"üìä Val samples: {len(valset)}")
            
        else:
            print("‚ö†Ô∏è data_utils –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Å–æ–∑–¥–∞–Ω–∏–µ mock DataLoader'–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ mock –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            class MockDataset(torch.utils.data.Dataset):
                def __init__(self, size=100):
                    self.size = size
                
                def __len__(self):
                    return self.size
                    
                def __getitem__(self, idx):
                    return (
                        torch.randint(0, 100, (50,)),  # text
                        torch.randn(80, 100),           # mel
                        torch.randint(0, 2, (100,))    # gate
                    )
            
            train_loader = DataLoader(MockDataset(1000), batch_size=8, shuffle=True)
            val_loader = DataLoader(MockDataset(200), batch_size=8, shuffle=False)
            
            print("‚úÖ Mock DataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        trainer.initialize_training()
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        if args.max_steps:
            print(f"üî¨ –ù–∞—á–∏–Ω–∞—é –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –≤ —Ä–µ–∂–∏–º–µ '{args.mode}' –Ω–∞ {args.max_steps} —à–∞–≥–æ–≤...")
            trainer.train(train_loader, val_loader, args.epochs, args.max_steps)
        else:
            print(f"üèÜ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º–µ '{args.mode}' –Ω–∞ {args.epochs} —ç–ø–æ—Ö...")
            trainer.train(train_loader, val_loader, args.epochs)
        
        print("üéâ Ultimate Enhanced Tacotron Trainer –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É!")
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –¥–µ–º–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
        print("\nüîß –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–º–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π...")
        try:
            trainer.initialize_training()
            print("‚úÖ –ë–∞–∑–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ!")
            print("‚ö†Ô∏è –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏")
        except Exception as e2:
            print(f"‚ùå –î–∞–∂–µ –±–∞–∑–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main() 