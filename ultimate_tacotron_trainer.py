#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üèÜ ULTIMATE ENHANCED TACOTRON TRAINER
–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –í–°–ï–• –ª—É—á—à–∏—Ö —Ä–µ—à–µ–Ω–∏–π –∏–∑ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º –æ–±—É—á–µ–Ω–∏—è

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ª—É—á—à–µ–µ –∏–∑:
- EnhancedTacotronTrainer (—Ñ–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
- train.py (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
- smart_tuner_main.py (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
- train_with_auto_fixes.py (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è)

–†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
- 'simple': –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- 'enhanced': –ü–æ–ª–Ω–æ–µ —Ñ–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º  
- 'auto_optimized': –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è + –æ–±—É—á–µ–Ω–∏–µ
- 'ultimate': –í—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ + –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.utils.data
from torch.utils.data import DataLoader
import numpy as np
import logging
import copy
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import yaml
import argparse
from tqdm import tqdm
import psutil
try:
    import GPUtil
    GPU_MONITORING_AVAILABLE = True
except ImportError:
    GPU_MONITORING_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from model import Tacotron2
from loss_function import Tacotron2Loss
from hparams import create_hparams
from audio_quality_enhancer import AudioQualityEnhancer

# üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ò–ó TRAIN.PY
try:
    from smart_tuner.gradient_clipper import AdaptiveGradientClipper, get_global_clipper, set_global_clipper
    GRADIENT_CLIPPER_AVAILABLE = True
except ImportError:
    GRADIENT_CLIPPER_AVAILABLE = False
    logging.warning("AdaptiveGradientClipper –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

try:
    from smart_tuner.smart_lr_adapter import SmartLRAdapter, get_global_lr_adapter, set_global_lr_adapter  
    SMART_LR_AVAILABLE = True
except ImportError:
    SMART_LR_AVAILABLE = False
    logging.warning("SmartLRAdapter –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# üß† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø (–∑–∞–º–µ–Ω–∞ AutoFixManager)
try:
    from context_aware_training_manager import ContextAwareTrainingManager, create_context_aware_manager
    CONTEXT_AWARE_AVAILABLE = True
    logging.info("‚úÖ Context-Aware Training Manager –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    CONTEXT_AWARE_AVAILABLE = False
    logging.warning("‚ùå Context-Aware Training Manager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# ü§ñ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø (–ü–û–õ–ù–û–°–¢–¨–Æ –£–î–ê–õ–ï–ù–û - –∑–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ —É–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É)
# AutoFixManager –£–î–ê–õ–ï–ù - –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ Context-Aware Training Manager
    AUTO_FIX_AVAILABLE = False
logging.info("üîß AutoFixManager –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω - –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ Context-Aware Manager")

# üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
try:
    from smart_tuner.optimization_engine import OptimizationEngine
    from smart_tuner.smart_tuner_integration import SmartTunerIntegration
    OPTIMIZATION_AVAILABLE = True
except ImportError:
    OPTIMIZATION_AVAILABLE = False
    logging.warning("OptimizationEngine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# üì± –ú–û–ù–ò–¢–û–†–ò–ù–ì –ò –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
try:
    from smart_tuner.telegram_monitor_enhanced import TelegramMonitorEnhanced
    from debug_reporter import initialize_debug_reporter, get_debug_reporter
    from alignment_diagnostics import AlignmentDiagnostics
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False
    logging.warning("–°–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

# üìä –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
try:
    import mlflow
    import mlflow.pytorch
    from torch.utils.tensorboard import SummaryWriter
    LOGGING_AVAILABLE = True
except ImportError:
    LOGGING_AVAILABLE = False
    logging.warning("MLflow/TensorBoard –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

# –ò–º–ø–æ—Ä—Ç –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
try:
    from data_utils import TextMelLoader, TextMelCollate
    DATA_UTILS_AVAILABLE = True
except ImportError:
    DATA_UTILS_AVAILABLE = False
    logging.warning("data_utils –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Å–æ–∑–¥–∞–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é")

class UltimateEnhancedTacotronTrainer:
    """
    üèÜ ULTIMATE Enhanced Tacotron Trainer
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–ï –ª—É—á—à–∏–µ —Ä–µ—à–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º:
    - –§–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ (4 —Ñ–∞–∑—ã —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π)
    - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è (Context-Aware Manager)
    - –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ (AdaptiveGradientClipper)
    - Smart LR –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (15+ –º–µ—Ç—Ä–∏–∫)
    - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    """
    
    def __init__(self, hparams, mode: str = 'enhanced', dataset_info: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ultimate Enhanced Trainer.
        
        Args:
            hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            mode: –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã ('simple', 'enhanced', 'auto_optimized', 'ultimate')
            dataset_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        self.hparams = hparams
        self.mode = mode
        self.dataset_info = dataset_info or {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = self._setup_logger()
        self.logger.info(f"üèÜ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ultimate Enhanced Tacotron Trainer (—Ä–µ–∂–∏–º: {mode})")
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        
        # üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ò–ó TRAIN.PY
        self.adaptive_gradient_clipper = None
        self.smart_lr_adapter = None
        
        # üß† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø
        self.context_aware_manager = None
        
        # ü§ñ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø (–û–¢–ö–õ–Æ–ß–ï–ù–û)
        # ü§ñ AutoFixManager –ü–û–õ–ù–û–°–¢–¨–Æ –£–î–ê–õ–ï–ù - –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ context_aware_manager
        
        # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
        self.optimization_engine = None
        self.smart_tuner = None
        
        # üì± –ú–û–ù–ò–¢–û–†–ò–ù–ì –ò –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
        self.telegram_monitor = None
        self.debug_reporter = None
        self.alignment_diagnostics = None
        
        # üìä –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
        self.tensorboard_writer = None
        self.mlflow_run = None
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.global_step = 0
        self.best_validation_loss = float('inf')
        self.training_metrics_history = []
        self.last_attention_diagonality = 0.0
        
        # üéØ –§–ê–ó–û–í–û–ï –û–ë–£–ß–ï–ù–ò–ï
        self.training_phases = {
            'pre_alignment': {'max_epoch': 500, 'focus': 'attention_learning'},
            'alignment_learning': {'max_epoch': 2000, 'focus': 'attention_stabilization'}, 
            'quality_optimization': {'max_epoch': 3000, 'focus': 'quality_improvement'},
            'fine_tuning': {'max_epoch': 3500, 'focus': 'final_polishing'}
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_components()
        
        self.logger.info("‚úÖ Ultimate Enhanced Tacotron Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å–µ–ª–µ–∫—Ç–∏–≤–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è."""
        logger = logging.getLogger('UltimateEnhancedTacotronTrainer')
        logger.setLevel(logging.WARNING)  # –¢–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        
        if not logger.handlers:
            # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π handler - —Ç–æ–ª—å–∫–æ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                'üîß %(message)s'  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            )
            console_handler.setFormatter(console_formatter)
            console_handler.setLevel(logging.WARNING)
            logger.addHandler(console_handler)
            
            # –§–∞–π–ª–æ–≤—ã–π handler - –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            file_handler = logging.FileHandler('ultimate_training.log')
            file_formatter = logging.Formatter(
                '%(asctime)s - [Ultimate] - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(file_formatter)
            file_handler.setLevel(logging.INFO)
            logger.addHandler(file_handler)
        
        return logger
    
    def _initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞."""
        self.logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–µ–∂–∏–º–∞ '{self.mode}'...")
        
        # üìä –í–°–ï–ì–î–ê: –ë–∞–∑–æ–≤–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self._initialize_logging()
        
        # üì± –í–°–ï–ì–î–ê: Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥  
        self._initialize_telegram_monitoring()
        
        if self.mode in ['enhanced', 'auto_optimized', 'ultimate']:
            # üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´
            self._initialize_critical_components()
            
        if self.mode in ['auto_optimized', 'ultimate']:
            # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
            self._initialize_optimization()
            
        if self.mode == 'ultimate':
            # ü§ñ –í–°–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò
            self._initialize_ultimate_features()
    
    def _initialize_logging(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏."""
        if not LOGGING_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è –°–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
            return
            
        try:
            # TensorBoard —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –º–µ—Ç—Ä–∏–∫
            from datetime import datetime
            log_dir = f'tensorboard_logs/ultimate_training_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
            self.tensorboard_writer = SummaryWriter(log_dir)
            self.logger.info(f"‚úÖ TensorBoard –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {log_dir}")
            
            # MLflow —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
            try:
                experiment_name = f"Ultimate_Tacotron_Training_{self.mode}"
                mlflow.set_experiment(experiment_name)
                
                # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö MLflow –∑–∞–ø—É—Å–∫–æ–≤ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10)
                self._cleanup_old_mlflow_runs(experiment_name, keep_last=10)
                
                self.mlflow_run = mlflow.start_run(
                    run_name=f"ultimate_{self.mode}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                )
                
                # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                self._log_initial_mlflow_params()
                
                self.logger.info(f"‚úÖ MLflow –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {experiment_name}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è MLflow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self.mlflow_run = None
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    def _cleanup_old_mlflow_runs(self, experiment_name: str, keep_last: int = 10):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö MLflow –∑–∞–ø—É—Å–∫–æ–≤."""
        try:
            from mlflow.tracking import MlflowClient
            client = MlflowClient()
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
            experiment = client.get_experiment_by_name(experiment_name)
            if not experiment:
                return
                
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø—É—Å–∫–∏
            runs = client.search_runs(
                experiment_ids=[experiment.experiment_id],
                order_by=["start_time DESC"]
            )
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—É—Å–∫–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ keep_last)
            for run in runs[keep_last:]:
                try:
                    client.delete_run(run.info.run_id)
                    self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π MLflow –∑–∞–ø—É—Å–∫: {run.info.run_id}")
                except Exception:
                    pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —É–¥–∞–ª–µ–Ω–∏—è
                    
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ MLflow: {e}")
    
    def _log_initial_mlflow_params(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ MLflow."""
        if not self.mlflow_run:
            return
            
        try:
            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            mlflow.log_params({
                'mode': self.mode,
                'learning_rate': getattr(self.hparams, 'learning_rate', 0),
                'batch_size': getattr(self.hparams, 'batch_size', 0),
                'max_decoder_steps': getattr(self.hparams, 'max_decoder_steps', 0),
                'guide_loss_weight': getattr(self.hparams, 'guide_loss_weight', 0),
                'gate_threshold': getattr(self.hparams, 'gate_threshold', 0),
            })
            
            # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            system_info = {
                'python_version': sys.version.split()[0],
                'torch_version': torch.__version__,
                'cuda_available': torch.cuda.is_available(),
                'gpu_count': torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
            
            if torch.cuda.is_available():
                system_info['gpu_name'] = torch.cuda.get_device_name(0)
                system_info['cuda_version'] = torch.version.cuda
            
            mlflow.log_params(system_info)
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            if self.dataset_info:
                dataset_params = {f'dataset_{k}': v for k, v in self.dataset_info.items()}
                mlflow.log_params(dataset_params)
                
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ MLflow: {e}")
    
    def _initialize_telegram_monitoring(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞."""
        if not MONITORING_AVAILABLE:
            return
            
        try:
            config_path = "smart_tuner/config.yaml"
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                telegram_config = config.get('telegram', {})
                if telegram_config.get('enabled', False):
                    self.telegram_monitor = TelegramMonitorEnhanced(
                        bot_token=telegram_config.get('bot_token'),
                        chat_id=telegram_config.get('chat_id'),
                        enabled=True
                    )
                    self.logger.info("‚úÖ Telegram Monitor Enhanced –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                    
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Telegram: {e}")
    
    def _initialize_critical_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–∑ train.py."""
        self.logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # üîß AdaptiveGradientClipper
        if GRADIENT_CLIPPER_AVAILABLE:
            try:
                self.adaptive_gradient_clipper = AdaptiveGradientClipper(
                    max_norm=1.0,
                    adaptive=True,
                    emergency_threshold=100.0,
                    history_size=1000,
                    percentile=95
                )
                set_global_clipper(self.adaptive_gradient_clipper)
                self.logger.info("‚úÖ AdaptiveGradientClipper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ AdaptiveGradientClipper: {e}")
        
        # üß† Context-Aware Training Manager
        if CONTEXT_AWARE_AVAILABLE:
            try:
                # Context-Aware Manager –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
                self.logger.info("üß† Context-Aware Training Manager –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ—Å–ª–µ –º–æ–¥–µ–ª–∏")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ Context-Aware Manager: {e}")
        
        # ü§ñ AutoFixManager (–ü–û–õ–ù–û–°–¢–¨–Æ –£–î–ê–õ–ï–ù)
        self.logger.info("üîß AutoFixManager –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω - –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ Context-Aware Manager")
        
        # üìä Alignment Diagnostics
        if MONITORING_AVAILABLE:
            try:
                self.alignment_diagnostics = AlignmentDiagnostics()
                self.logger.info("‚úÖ AlignmentDiagnostics –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ AlignmentDiagnostics: {e}")
                
        # üîç Debug Reporter
        if MONITORING_AVAILABLE:
            try:
                self.debug_reporter = initialize_debug_reporter(self.telegram_monitor)
                self.logger.info("‚úÖ Debug Reporter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Debug Reporter: {e}")
    
    def _initialize_optimization(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        if not OPTIMIZATION_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è OptimizationEngine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return
            
        try:
            self.optimization_engine = OptimizationEngine("smart_tuner/config.yaml")
            self.smart_tuner = SmartTunerIntegration()
            self.logger.info("‚úÖ OptimizationEngine –∏ SmartTuner –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
    
    def _initialize_ultimate_features(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π Ultimate —Ä–µ–∂–∏–º–∞."""
        self.logger.info("üèÜ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ultimate –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π...")
        
        # –ó–¥–µ—Å—å –±—É–¥—É—Ç —ç–∫—Å–∫–ª—é–∑–∏–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Ultimate —Ä–µ–∂–∏–º–∞
        # - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        # - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        # - –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã
        # - –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑
        
        self.logger.info("‚úÖ Ultimate –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã") 

    def initialize_training(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        self.logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
        # üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í (–¥–ª—è —Ä–µ–∂–∏–º–æ–≤ auto_optimized –∏ ultimate)
        if self.mode in ['auto_optimized', 'ultimate'] and self.smart_tuner:
            try:
                original_hparams = vars(self.hparams).copy()
                optimized_hparams = self.smart_tuner.on_training_start(
                    original_hparams, self.dataset_info
                )
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                for key, value in optimized_hparams.items():
                    if hasattr(self.hparams, key):
                        setattr(self.hparams, key, value)
                        
                self.logger.info("‚ú® –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ Smart Tuner")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = Tacotron2(self.hparams).cuda()
        self.logger.info(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {sum(p.numel() for p in self.model.parameters())} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è loss —Ñ—É–Ω–∫—Ü–∏–∏
        self.criterion = Tacotron2Loss(self.hparams)
        self.logger.info("üéØ Enhanced loss function –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.hparams.learning_rate,
            weight_decay=getattr(self.hparams, 'weight_decay', 1e-6)
        )
        self.logger.info("‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # üîß Smart LR Adapter (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if SMART_LR_AVAILABLE:
            try:
                self.smart_lr_adapter = SmartLRAdapter(
                    optimizer=self.optimizer,
                    patience=10,
                    factor=0.5,
                    min_lr=getattr(self.hparams, 'learning_rate_min', 1e-8),
                    max_lr=self.hparams.learning_rate * 2,
                    emergency_factor=0.1,
                    grad_norm_threshold=1000.0,
                    loss_nan_threshold=1e6
                )
                set_global_lr_adapter(self.smart_lr_adapter)
                self.logger.info("‚úÖ Smart LR Adapter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart LR Adapter: {e}")
        
        # üß† Context-Aware Training Manager (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞)
        if CONTEXT_AWARE_AVAILABLE and self.mode in ['enhanced', 'auto_optimized', 'ultimate']:
            try:
                self.context_aware_manager = create_context_aware_manager(self.hparams)
                self.logger.info("‚úÖ Context-Aware Training Manager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                self.logger.info("üéØ –°–∏—Å—Ç–µ–º–∞ —É–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞ (–∑–∞–º–µ–Ω–∞ AutoFixManager)")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Context-Aware Manager: {e}")
        
        # ü§ñ AutoFixManager (–ü–û–õ–ù–û–°–¢–¨–Æ –£–î–ê–õ–ï–ù - –∑–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ —É–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É)
        # AutoFixManager –±–æ–ª—å—à–µ –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø - –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ Context-Aware Manager
        self.logger.info("üîß AutoFixManager –ø–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Context-Aware Manager")
    
    def get_current_training_phase(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–ø–æ—Ö–∏."""
        for phase, config in self.training_phases.items():
            if self.current_epoch < config['max_epoch']:
                return phase
        return 'fine_tuning'  # –ü–æ—Å–ª–µ–¥–Ω—è—è —Ñ–∞–∑–∞
    
    def adjust_hyperparams_for_phase(self, phase: str):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è."""
        if self.mode not in ['enhanced', 'auto_optimized', 'ultimate']:
            return  # –§–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ä–µ–∂–∏–º–æ–≤
            
        phase_configs = {
            'pre_alignment': {
                'learning_rate_multiplier': 1.2,
                'guide_loss_weight_multiplier': 3.0,
                'batch_size_multiplier': 0.8
            },
            'alignment_learning': {
                'learning_rate_multiplier': 1.0, 
                'guide_loss_weight_multiplier': 2.0,
                'batch_size_multiplier': 1.0
            },
            'quality_optimization': {
                'learning_rate_multiplier': 0.8,
                'guide_loss_weight_multiplier': 1.5,
                'batch_size_multiplier': 1.2
            },
            'fine_tuning': {
                'learning_rate_multiplier': 0.5,
                'guide_loss_weight_multiplier': 1.0,
                'batch_size_multiplier': 1.0
            }
        }
        
        config = phase_configs.get(phase, phase_configs['alignment_learning'])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        if hasattr(self.criterion, 'guide_loss_weight'):
            new_weight = getattr(self.hparams, 'guide_loss_weight', 2.5) * config['guide_loss_weight_multiplier']
            self.criterion.guide_loss_weight = new_weight
            
        self.logger.info(f"üìä –§–∞–∑–∞ '{phase}': –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã "
                        f"lr_mult={config['learning_rate_multiplier']}")
    
    def train_step(self, batch):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏."""
        self.model.train()
        self.optimizer.zero_grad()
        
        # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ batch (TextMelCollate –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
        text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths, ctc_text, ctc_text_lengths, guide_mask = batch
        
        # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
        text_inputs = text_inputs.cuda()
        mel_targets = mel_targets.cuda() 
        gate_targets = gate_targets.cuda()
        
        # üîß –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê MODEL_OUTPUTS —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤
        try:
            x, y = self.model.parse_batch(batch)
            
            # üî• –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –†–ê–ó–ú–ï–†–û–í –ü–ï–†–ï–î FORWARD PASS
            text_inputs, text_lengths, mel_targets, max_len, output_lengths, ctc_text, ctc_text_lengths = x
            if text_inputs.size(0) != mel_targets.size(0):
                self.logger.error(f"üö® Batch size mismatch: text={text_inputs.size(0)}, mel={mel_targets.size(0)}")
                return {'total_loss': 10.0}
            
            # üî• –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ï –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–• –†–ê–ó–ú–ï–†–û–í –î–õ–Ø –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–ò
            max_text_len = min(text_inputs.size(1), 200)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
            max_mel_len = min(mel_targets.size(2), 1000)   # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É mel
            
            if text_inputs.size(1) > max_text_len:
                text_inputs = text_inputs[:, :max_text_len]
                text_lengths = torch.clamp(text_lengths, max=max_text_len)
            
            if mel_targets.size(2) > max_mel_len:
                mel_targets = mel_targets[:, :, :max_mel_len]
                output_lengths = torch.clamp(output_lengths, max=max_mel_len)
            
            # –ü–µ—Ä–µ—Å–æ–±–∏—Ä–∞–µ–º x —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏
            x = (text_inputs, text_lengths, mel_targets, max_len, output_lengths, ctc_text, ctc_text_lengths)
            
            model_outputs = self.model(x)
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ 1-7+ –∑–Ω–∞—á–µ–Ω–∏–π)
            if len(model_outputs) >= 4:
                mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model_outputs[:4]
            elif len(model_outputs) == 3:
                mel_outputs, mel_outputs_postnet, gate_outputs = model_outputs
                alignments = None
            elif len(model_outputs) == 2:
                mel_outputs, mel_outputs_postnet = model_outputs
                gate_outputs = None
                alignments = None
            else:
                mel_outputs = model_outputs[0] if len(model_outputs) > 0 else None
                mel_outputs_postnet = None
                gate_outputs = None
                alignments = None
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ forward pass: {e}")
            return {'total_loss': torch.tensor(float('inf'))}
        
        # üîç –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê
        attention_diagonality = 0.0
        gate_accuracy = 0.0
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º attention_diagonality
            if alignments is not None:
                attention_matrix = alignments.detach().cpu().numpy()
                if attention_matrix.ndim == 3:
                    batch_diagonalities = []
                    for b in range(attention_matrix.shape[0]):
                        attn = attention_matrix[b]
                        if attn.sum() > 0:
                            attn = attn / attn.sum(axis=1, keepdims=True)
                        
                        min_dim = min(attn.shape[0], attn.shape[1])
                        diagonal_elements = [attn[i, i] for i in range(min_dim)]
                        batch_diagonalities.append(np.mean(diagonal_elements) if diagonal_elements else 0.0)
                    
                    attention_diagonality = np.mean(batch_diagonalities) if batch_diagonalities else 0.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º gate_accuracy
            if gate_outputs is not None:
                gate_pred = (gate_outputs > 0.5).float()
                gate_targets_binary = (gate_targets > 0.5).float()
                correct = (gate_pred == gate_targets_binary).float().mean()
                gate_accuracy = correct.item()
                
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        self.last_attention_diagonality = attention_diagonality
        
        # üö® –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –°–ò–°–¢–ï–ú–ê GUIDED ATTENTION
        if hasattr(self.criterion, 'guide_loss_weight') and self.global_step > 0:
            current_weight = self.criterion.guide_loss_weight
            
            # üéØ –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –°–¢–†–ê–¢–ï–ì–ò–Ø –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú EMERGENCY MODE
            if attention_diagonality < 0.02:
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –†–ï–ñ–ò–ú: –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º emergency mode –≤ loss function
                if hasattr(self.criterion, 'guided_attention_loss') and hasattr(self.criterion.guided_attention_loss, 'activate_critical_mode'):
                    self.criterion.guided_attention_loss.activate_critical_mode()
                    self.logger.error(f"üö® –ê–ö–¢–ò–í–ò–†–û–í–ê–ù EMERGENCY MODE –¥–ª—è guided attention (weight=25.0)!")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π weight –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è  
                new_weight = min(current_weight * 2.0, 50.0)  # –£–±–∏—Ä–∞–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –ª–∏–º–∏—Ç
                self.criterion.guide_loss_weight = new_weight
                self.logger.error(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
                
                # –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ LR –¥–ª—è attention –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                for param_group in self.optimizer.param_groups:
                    if param_group['lr'] < 1e-4:
                        param_group['lr'] = min(param_group['lr'] * 3.0, 1e-4)
                        self.logger.warning(f"üöÄ –≠–ö–°–¢–†–ï–ù–ù–û–ï —É–≤–µ–ª–∏—á–µ–Ω–∏–µ LR: {param_group['lr']:.2e}")
                        
            elif attention_diagonality < 0.05:
                # –û—á–µ–Ω—å –Ω–∏–∑–∫–æ–µ - –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–æ 30.0
                new_weight = min(current_weight * 1.8, 30.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.warning(f"üö® –ê–ì–†–ï–°–°–ò–í–ù–û–ï —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
                
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º LR –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è attention
                for param_group in self.optimizer.param_groups:
                    if param_group['lr'] < 5e-5:
                        param_group['lr'] = min(param_group['lr'] * 2.0, 5e-5)
                        self.logger.info(f"üîÑ –£—Å–∏–ª–µ–Ω–∏–µ LR –¥–ª—è attention: {param_group['lr']:.2e}")
                
            elif attention_diagonality < 0.1:
                # –ù–∏–∑–∫–æ–µ - —É–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–æ 20.0
                new_weight = min(current_weight * 1.4, 20.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.warning(f"üö® –°–∏–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
                
            elif attention_diagonality < 0.3:
                # –£–ª—É—á—à–∞–µ—Ç—Å—è - –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–æ 15.0
                new_weight = min(current_weight * 1.1, 15.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.info(f"üìà –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
                
            elif attention_diagonality > 0.7:
                # –û—Ç–ª–∏—á–Ω–æ–µ attention - –¥–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º emergency mode –∏ —Å–Ω–∏–∂–∞–µ–º
                if hasattr(self.criterion, 'guided_attention_loss') and hasattr(self.criterion.guided_attention_loss, 'deactivate_critical_mode'):
                    self.criterion.guided_attention_loss.deactivate_critical_mode()
                    self.logger.info("‚úÖ –î–ï–ê–ö–¢–ò–í–ò–†–û–í–ê–ù EMERGENCY MODE - attention —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ!")
                    
                new_weight = max(current_weight * 0.8, 3.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.info(f"üìâ –°–Ω–∏–∂–µ–Ω–∏–µ guided attention weight (–æ—Ç–ª–∏—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ): {current_weight:.1f} ‚Üí {new_weight:.1f}")
                
            elif attention_diagonality > 0.5:
                # –•–æ—Ä–æ—à–µ–µ attention - –º—è–≥–∫–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
                new_weight = max(current_weight * 0.9, 5.0)
                self.criterion.guide_loss_weight = new_weight
                self.logger.info(f"üìä –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è guided attention weight: {current_weight:.1f} ‚Üí {new_weight:.1f}")
            
                    # üéØ –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ï –≠–ö–°–¢–†–ï–ù–ù–´–ï –ú–ï–†–´ –î–õ–Ø –ö–†–ò–¢–ò–ß–ï–°–ö–û–ì–û ATTENTION
        if attention_diagonality < 0.05 and self.global_step > 20:
            # 1. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å attention mechanism
            if hasattr(self.model, 'decoder') and hasattr(self.model.decoder, 'attention_layer'):
                self.model.decoder.attention_layer.score_mask_value = -1e6  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å
                self.logger.info("üéØ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å attention mechanism")
                
                # 2. –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º Location-Relative attention –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                if hasattr(self.model.decoder.attention_layer, 'use_location_relative'):
                    if not self.model.decoder.attention_layer.use_location_relative:
                        self.model.decoder.attention_layer.use_location_relative = True
                        self.model.decoder.attention_layer.relative_sigma = 2.0  # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–µ–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                        self.logger.error("üöÄ –ê–ö–¢–ò–í–ò–†–û–í–ê–ù Location-Relative attention!")
                    else:
                        # –î–µ–ª–∞–µ–º –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º
                        self.model.decoder.attention_layer.relative_sigma = max(
                            self.model.decoder.attention_layer.relative_sigma * 0.8, 1.0
                        )
                        self.logger.warning(f"üéØ –£—Å–∏–ª–µ–Ω–∏–µ Location-Relative: sigma={self.model.decoder.attention_layer.relative_sigma:.1f}")
            
            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –≤—ã–∑—ã–≤–∞–µ–º emergency mode –≤ loss function
            if hasattr(self.criterion, 'guided_attention_loss'):
                self.criterion.guided_attention_loss.check_diagonality_and_adapt(alignments)
            
            # 4. –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ –º–µ—Ä—ã –¥–ª—è Prenet (—Å–Ω–∏–∂–∞–µ–º dropout –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
            if hasattr(self.model, 'decoder') and hasattr(self.model.decoder, 'prenet'):
                if hasattr(self.model.decoder.prenet, 'dropout_rate'):
                    self.model.decoder.prenet.dropout_rate = min(self.model.decoder.prenet.dropout_rate * 0.5, 0.1)
                    self.logger.info(f"üîß –°–Ω–∏–∂–µ–Ω–∏–µ Prenet dropout: {self.model.decoder.prenet.dropout_rate:.3f}")
        
        # üåü –ü–†–û–ì–†–ï–°–°–ò–í–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï ATTENTION –ü–†–ò –•–û–†–û–®–ò–• –†–ï–ó–£–õ–¨–¢–ê–¢–ê–•
        elif attention_diagonality > 0.3 and self.global_step > 100:
            # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º Location-Relative –¥–ª—è –Ω–∞—Ç—É—Ä–∞–ª—å–Ω–æ—Å—Ç–∏
            if hasattr(self.model, 'decoder') and hasattr(self.model.decoder, 'attention_layer'):
                if hasattr(self.model.decoder.attention_layer, 'use_location_relative'):
                    if self.model.decoder.attention_layer.use_location_relative and attention_diagonality > 0.6:
                        self.model.decoder.attention_layer.relative_sigma = min(
                            self.model.decoder.attention_layer.relative_sigma * 1.1, 8.0
                        )
                        if self.model.decoder.attention_layer.relative_sigma > 6.0:
                            self.model.decoder.attention_layer.use_location_relative = False
                            self.logger.info("‚úÖ –û–¢–ö–õ–Æ–ß–ï–ù Location-Relative - attention —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
        
        # üéØ –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô ATTENTION WARM-UP –í –ù–ê–ß–ê–õ–ï –û–ë–£–ß–ï–ù–ò–Ø
        if self.global_step < 200:
            # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω—ã–µ attention –º–µ—Ö–∞–Ω–∏–∑–º—ã
            if hasattr(self.model, 'decoder') and hasattr(self.model.decoder, 'attention_layer'):
                # –í –Ω–∞—á–∞–ª–µ –¥–µ–ª–∞–µ–º –æ—á–µ–Ω—å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–µ attention
                target_mask_value = -1e4 - (self.global_step * 50)  # –°—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –±–æ–ª–µ–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–º —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
                self.model.decoder.attention_layer.score_mask_value = target_mask_value
                
                # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º Location-Relative —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ–π sigma
                if hasattr(self.model.decoder.attention_layer, 'use_location_relative'):
                    if self.global_step > 50 and not self.model.decoder.attention_layer.use_location_relative:
                        self.model.decoder.attention_layer.use_location_relative = True
                        self.model.decoder.attention_layer.relative_sigma = 3.0
                        self.logger.info("üéØ –ê–ö–¢–ò–í–ò–†–û–í–ê–ù Location-Relative –Ω–∞ warm-up!")
                
                if self.global_step % 50 == 0:
                    self.logger.info(f"üéØ Attention Warm-up: —à–∞–≥ {self.global_step}, mask_value={target_mask_value:.0e}")
        
        # üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ú–û–ù–ò–¢–û–†–ò–ù–ì ATTENTION –ü–†–û–ì–†–ï–°–°–ê
        if self.global_step % 100 == 0 and attention_diagonality < 0.1:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º attention patterns –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            if alignments is not None:
                try:
                    attention_stats = self._analyze_attention_patterns(alignments)
                    self.logger.warning(f"üîç Attention Analysis: –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å={attention_diagonality:.3f}, "
                                      f"—Ñ–æ–∫—É—Å={attention_stats.get('focus', 0):.3f}, "
                                      f"–º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å={attention_stats.get('monotonicity', 0):.3f}")
                except Exception as e:
                    self.logger.debug(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ attention: {e}")
        
        # üéØ –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–†–û–ì–†–ï–°–°–ê ATTENTION
        if self.global_step % 10 == 0:
            attention_trend = "üìà" if attention_diagonality > self.last_attention_diagonality else "üìâ" if attention_diagonality < self.last_attention_diagonality else "‚û°Ô∏è"
            if attention_diagonality < 0.1:
                self.logger.warning(f"üéØ Attention Progress {attention_trend}: {attention_diagonality:.3f} (TARGET: >0.7)")
            elif attention_diagonality < 0.3:
                self.logger.info(f"üéØ Attention Progress {attention_trend}: {attention_diagonality:.3f} (IMPROVING)")
            elif attention_diagonality >= 0.7:
                self.logger.info(f"üéØ Attention EXCELLENT {attention_trend}: {attention_diagonality:.3f} ‚úÖ")
        
        # üéØ –í–´–ß–ò–°–õ–ï–ù–ò–ï LOSS –° –ë–ï–ó–û–ü–ê–°–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–û–ô
        try:
            loss_components = self.criterion(
                model_outputs, 
                (mel_targets, gate_targets),
                attention_weights=alignments,
                gate_outputs=gate_outputs
            )
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            if isinstance(loss_components, (list, tuple)):
                if len(loss_components) == 4:
                    mel_loss, gate_loss, guide_loss, emb_loss = loss_components
                    loss = mel_loss + gate_loss + guide_loss + emb_loss
                    loss_dict = {
                        'mel_loss': mel_loss.item(),
                        'gate_loss': gate_loss.item(), 
                        'guide_loss': guide_loss.item(),
                        'emb_loss': emb_loss.item(),
                        'total_loss': loss.item()
                    }
                else:
                    loss = loss_components[0]
                    loss_dict = {'total_loss': loss.item()}
            else:
                loss = loss_components
                loss_dict = {'total_loss': loss.item()}
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss: {e}")
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í–º–µ—Å—Ç–æ inf –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            return {'total_loss': 10.0}  # –í—ã—Å–æ–∫–æ–µ, –Ω–æ –∫–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        
        # üî• –ü–†–û–í–ï–†–ö–ê LOSS –ü–ï–†–ï–î BACKWARD
        if torch.isnan(loss) or torch.isinf(loss):
            self.logger.error(f"üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω NaN/Inf loss: {loss}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º backward")
            return {'total_loss': 10.0}
        
        # Backward pass
        try:
            loss.backward()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ backward pass: {e}")
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í–º–µ—Å—Ç–æ inf –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            return {'total_loss': 10.0}
        
        # üîß –ü–†–û–î–í–ò–ù–£–¢–û–ï –£–ü–†–ê–í–õ–ï–ù–ò–ï –ì–†–ê–î–ò–ï–ù–¢–ê–ú–ò –° –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ú –ö–õ–ò–ü–ü–ò–ù–ì–û–ú
        if self.adaptive_gradient_clipper:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º AdaptiveGradientClipper
            was_clipped, grad_norm, clip_threshold = self.adaptive_gradient_clipper.clip_gradients(
                self.model, self.global_step
            )
            
            if was_clipped:
                self.logger.info(f"üîß –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ–±—Ä–µ–∑–∞–Ω—ã: {grad_norm:.2f} ‚Üí {clip_threshold:.2f}")
        else:
            # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–ª–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
            grad_norm = torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), 
                max_norm=1.0,  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º 1.0
                norm_type=2.0
            )
            
        # üö® –≠–ö–°–¢–†–ï–ù–ù–ê–Ø –ó–ê–©–ò–¢–ê: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –∫–ª–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è
        current_grad_norm = 0.0
        for param in self.model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                current_grad_norm += param_norm.item() ** 2
        current_grad_norm = current_grad_norm ** 0.5
        
        # –ï—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Å–µ –µ—â–µ –≤—ã—Å–æ–∫–∏–µ - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–µ–º –µ—â–µ —Ä–∞–∑
        if current_grad_norm > 2.0:
            self.logger.warning(f"üö® –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ï –≤—Ç–æ—Ä–∏—á–Ω–æ–µ –∫–ª–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ: {current_grad_norm:.2f} ‚Üí 1.0")
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0, norm_type=2.0)
            current_grad_norm = 1.0
            
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞–ª–µ—Ä—Ç—ã –¥–ª—è –≤—ã—Å–æ–∫–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if current_grad_norm > 5.0:
            self.logger.warning(f"üö® –í–´–°–û–ö–ê–Ø –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {current_grad_norm:.2f}")
        if current_grad_norm > 20.0:
            self.logger.error(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {current_grad_norm:.2f}")
            
        grad_norm = current_grad_norm
        
        # üß† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø (–∑–∞–º–µ–Ω–∞ AutoFixManager)
        if self.context_aware_manager:
            try:
                context_metrics = {
                    'grad_norm': float(grad_norm),
                    'attention_diagonality': attention_diagonality,
                    'gate_accuracy': gate_accuracy,
                    'loss': float(loss.item()),
                    'mel_loss': loss_dict.get('mel_loss', 0),
                    'gate_loss': loss_dict.get('gate_loss', 0),
                    'guided_attention_loss': loss_dict.get('guide_loss', 0),
                    'epoch': self.current_epoch
                }
                
                adaptations = self.context_aware_manager.analyze_and_adapt(
                    step=self.global_step,
                    metrics=context_metrics,
                    model=self.model,
                    optimizer=self.optimizer
                )
                
                if adaptations and len(adaptations) > 4:  # –ë–æ–ª–µ–µ 4 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–∑–Ω–∞—á–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
                    adapted_params = [k for k, v in adaptations.items() if k not in ['mel_weight', 'gate_weight']]
                    if adapted_params:
                        self.logger.info(f"üéØ Context-Aware –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: {adapted_params}")
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ Context-Aware Manager: {e}")
        
        # üîß Smart LR Adapter
        if self.smart_lr_adapter:
            try:
                lr_changed = self.smart_lr_adapter.step(
                    loss=float(loss.item()),
                    grad_norm=float(grad_norm),
                    step=self.global_step
                )
                if lr_changed:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ LR –Ω–µ —Å—Ç–∞–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–º
                    if current_lr < 1e-7:
                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–π LR
                        for param_group in self.optimizer.param_groups:
                            param_group['lr'] = 1e-6
                        current_lr = 1e-6
                        self.logger.info(f"üîÑ LR –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è: {current_lr:.2e}")
                    else:
                        self.logger.info(f"üîÑ Smart LR –∞–¥–∞–ø—Ç–∞—Ü–∏—è: LR –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ {current_lr:.2e}")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart LR Adapter: {e}")
        
        # üöÄ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø LEARNING RATE
        current_lr = self.optimizer.param_groups[0]['lr']
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –£–†–û–í–ï–ù–¨: LR < 1e-7 (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω—É–ª–µ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
        if current_lr < 1e-7:
            if attention_diagonality < 0.02:  # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –ø–ª–æ—Ö–æ–µ attention
                target_lr = 3e-5  # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è attention
                self.logger.error(f"üö® –≠–ö–°–¢–†–ï–ù–ù–û–ï –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ LR –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ attention: {current_lr:.2e} ‚Üí {target_lr:.2e}")
            elif attention_diagonality < 0.1:  # –ü–ª–æ—Ö–æ–µ attention  
                target_lr = 1e-5  # –£–º–µ—Ä–µ–Ω–Ω–æ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
                self.logger.error(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ LR: {current_lr:.2e} ‚Üí {target_lr:.2e}")
            else:  # –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ attention, –Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∏–π LR
                target_lr = 5e-6  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
                self.logger.warning(f"üîÑ –ê–≤—Ç–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ LR: {current_lr:.2e} ‚Üí {target_lr:.2e}")
                
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = target_lr
            
        # –û–ß–ï–ù–¨ –ù–ò–ó–ö–ò–ô –£–†–û–í–ï–ù–¨: LR < 5e-7
        elif current_lr < 5e-7 and (attention_diagonality < 0.1 or loss.item() > 15.0):
            target_lr = 2e-6 if attention_diagonality < 0.05 else 1e-6
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = target_lr
            self.logger.warning(f"üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–≥–æ LR: {current_lr:.2e} ‚Üí {target_lr:.2e}")
            
        # –ù–ò–ó–ö–ò–ô –£–†–û–í–ï–ù–¨ –í –ù–ê–ß–ê–õ–ï –û–ë–£–ß–ï–ù–ò–Ø: LR < 1e-6 –ø—Ä–∏ step < 1000
        elif current_lr < 1e-6 and self.global_step < 1000:
            target_lr = 1e-5 if self.global_step < 100 else 5e-6
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = target_lr
            self.logger.info(f"üöÄ –†–∞–Ω–Ω–µ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ LR: {current_lr:.2e} ‚Üí {target_lr:.2e}")
        
        # –ê–î–ê–ü–¢–ò–í–ù–û–ï –£–í–ï–õ–ò–ß–ï–ù–ò–ï –ø—Ä–∏ —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö  
        elif (current_lr < 5e-6 and attention_diagonality > 0.3 and loss.item() < 10.0 and 
              grad_norm < 2.0 and self.global_step % 50 == 0):
            target_lr = min(current_lr * 1.3, 2e-5)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = target_lr
            self.logger.info(f"üìà –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ LR: {current_lr:.2e} ‚Üí {target_lr:.2e}")
        
        # –ó–ê–©–ò–¢–ê –û–¢ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø: LR > 1e-3
        elif current_lr > 1e-3:
            target_lr = 1e-3
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = target_lr
            self.logger.warning(f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ LR: {current_lr:.2e} ‚Üí {target_lr:.2e}")
        
        # –ú–û–ù–ò–¢–û–†–ò–ù–ì –°–û–°–¢–û–Ø–ù–ò–Ø LR –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
        if self.global_step % 100 == 0:
            lr_status = "üî¥ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô" if current_lr < 1e-7 else "üü° –ù–ò–ó–ö–ò–ô" if current_lr < 1e-6 else "üü¢ –ù–û–†–ú–ê–õ–¨–ù–´–ô"
            if current_lr < 1e-6:  # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ LR
                self.logger.warning(f"üìä LR Status: {lr_status} | Current: {current_lr:.2e} | Attention: {attention_diagonality:.3f}")
        
        # –°–í–Ø–ó–¨ –° ATTENTION EMERGENCY MODE
        if current_lr < 1e-6 and attention_diagonality < 0.02:
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º —Å emergency mode guided attention
            if hasattr(self.criterion, 'guided_attention_loss'):
                if hasattr(self.criterion.guided_attention_loss, 'activate_critical_mode'):
                    self.criterion.guided_attention_loss.activate_critical_mode()
                    self.logger.error("üéØ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è LR + Guided Attention EMERGENCY MODE!")
        
        # Optimizer step
        try:
            self.optimizer.step()
            self.global_step += 1
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ optimizer step: {e}")
            return {'total_loss': torch.tensor(float('inf'))}
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –≤ —á–∏—Å–ª–∞)
        loss_dict.update({
            'grad_norm': grad_norm.cpu().item() if isinstance(grad_norm, torch.Tensor) else grad_norm,
            'attention_diagonality': attention_diagonality,
            'gate_accuracy': gate_accuracy,
            'learning_rate': self.optimizer.param_groups[0]['lr']
        })
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –≤ —á–∏—Å–ª–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        for key, value in loss_dict.items():
            if isinstance(value, torch.Tensor):
                loss_dict[key] = value.cpu().item()
        
        return loss_dict
    
    def validate_step(self, val_loader):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º."""
        self.model.eval()
        
        val_losses = []
        attention_scores = []
        gate_accuracies = []
        
        with torch.no_grad():
            for batch in val_loader:
                try:
                    # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∫ –≤ train_step, –Ω–æ –±–µ–∑ backward
                    text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths, ctc_text, ctc_text_lengths, guide_mask = batch
                    
                    text_inputs = text_inputs.cuda()
                    mel_targets = mel_targets.cuda()
                    gate_targets = gate_targets.cuda()
                    
                    x, y = self.model.parse_batch(batch)
                    model_outputs = self.model(x)
                    
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
                    loss_components = self.criterion(model_outputs, (mel_targets, gate_targets))
                    
                    if isinstance(loss_components, (list, tuple)):
                        val_loss = sum(loss_components)
                    else:
                        val_loss = loss_components
                        
                    val_losses.append(val_loss.item())
                    
                    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
                    if len(model_outputs) >= 4:
                        alignments = model_outputs[3]
                        gate_outputs = model_outputs[2]
                        
                        # Attention diagonality
                        if alignments is not None:
                            attention_matrix = alignments.detach().cpu().numpy()
                            if attention_matrix.ndim == 3:
                                diag_score = np.mean([np.trace(attention_matrix[b]) / min(attention_matrix[b].shape) 
                                                    for b in range(attention_matrix.shape[0])])
                                attention_scores.append(diag_score)
                        
                        # Gate accuracy
                        if gate_outputs is not None:
                            gate_pred = (gate_outputs > 0.5).float()
                            gate_targets_binary = (gate_targets > 0.5).float()
                            accuracy = (gate_pred == gate_targets_binary).float().mean()
                            gate_accuracies.append(accuracy.item())
                            
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                    continue
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation_metrics = {
            'val_loss': np.mean(val_losses) if val_losses else float('inf'),
            'val_attention_diagonality': np.mean(attention_scores) if attention_scores else 0.0,
            'val_gate_accuracy': np.mean(gate_accuracies) if gate_accuracies else 0.0
        }
        
        return validation_metrics 

    def train(self, train_loader, val_loader, num_epochs: int = 3500, max_steps: Optional[int] = None):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –∫—Ä–∞—Å–∏–≤—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ —Å–µ–ª–µ–∫—Ç–∏–≤–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        
        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            max_steps: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
        """
        # –ü–µ—á–∞—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ä—Ç
        print(f"üèÜ Ultimate Enhanced Training (—Ä–µ–∂–∏–º: {self.mode})")
        if max_steps:
            print(f"üî¨ –¢–ï–°–¢–û–í–´–ô –†–ï–ñ–ò–ú: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ {max_steps} —à–∞–≥–æ–≤")
        else:
            print(f"üìä –≠–ø–æ—Ö: {num_epochs}, –ë–∞—Ç—á–µ–π –Ω–∞ —ç–ø–æ—Ö—É: {len(train_loader)}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.initialize_training()
        
        # üì± Telegram —Å—Ç–∞—Ä—Ç–æ–≤–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        self._send_training_start_notification()
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞  
        epoch_start_time = time.time()
        best_val_loss = float('inf')
        patience_counter = 0
        max_patience = getattr(self.hparams, 'early_stopping_patience', 10)
        training_history = []
        
        # üéØ –ö–†–ê–°–ò–í–´–ô –ü–†–û–ì–†–ï–°–°-–ë–ê–† –î–õ–Ø –≠–ü–û–•
        epoch_progress = tqdm(
            range(num_epochs), 
            desc="üöÄ –û–±—É—á–µ–Ω–∏–µ", 
            unit="—ç–ø–æ—Ö–∞",
            ncols=100,
            leave=True
        )
        
        for epoch in epoch_progress:
            self.current_epoch = epoch
            epoch_start = time.time()
            
            # üéØ –§–ê–ó–û–í–û–ï –û–ë–£–ß–ï–ù–ò–ï
            current_phase = self.get_current_training_phase()
            if epoch % 100 == 0 or epoch < 5:
                self.adjust_hyperparams_for_phase(current_phase)
            
            # üìä –ú–ï–¢–†–ò–ö–ò –≠–ü–û–•–ò
            epoch_losses = []
            epoch_grad_norms = []
            epoch_attention_scores = []
            epoch_gate_accuracies = []
            fixes_applied = 0
            
            # üéØ –ü–†–û–ì–†–ï–°–°-–ë–ê–† –î–õ–Ø –ë–ê–¢–ß–ï–ô
            batch_progress = tqdm(
                train_loader,
                desc=f"–≠–ø–æ—Ö–∞ {epoch}",
                unit="–±–∞—Ç—á", 
                leave=False,
                ncols=80
            )
            
            for batch_idx, batch in enumerate(batch_progress):
                try:
                    # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
                    step_metrics = self.train_step(batch)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
                    batch_progress.set_postfix({
                        'Loss': f"{step_metrics['total_loss']:.2f}",
                        'Grad': f"{step_metrics.get('grad_norm', 0):.1f}",
                        'Gate': f"{step_metrics.get('gate_accuracy', 0):.3f}"
                    })
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º CUDA —Ç–µ–Ω–∑–æ—Ä—ã –≤ —á–∏—Å–ª–∞)
                    loss_val = step_metrics['total_loss']
                    if isinstance(loss_val, torch.Tensor):
                        loss_val = loss_val.cpu().item()
                    epoch_losses.append(loss_val)
                    
                    grad_norm_val = step_metrics.get('grad_norm', 0)
                    if isinstance(grad_norm_val, torch.Tensor):
                        grad_norm_val = grad_norm_val.cpu().item()
                    epoch_grad_norms.append(grad_norm_val)
                    
                    attention_val = step_metrics.get('attention_diagonality', 0)
                    if isinstance(attention_val, torch.Tensor):
                        attention_val = attention_val.cpu().item()
                    epoch_attention_scores.append(attention_val)
                    
                    gate_acc_val = step_metrics.get('gate_accuracy', 0)
                    if isinstance(gate_acc_val, torch.Tensor):
                        gate_acc_val = gate_acc_val.cpu().item()
                    epoch_gate_accuracies.append(gate_acc_val)
                    
                    # üß† –ö–û–ù–¢–ï–ö–°–¢–ù–û-–û–°–û–ó–ù–ê–ù–ù–´–ï –ê–î–ê–ü–¢–ê–¶–ò–ò - —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö
                    if self.context_aware_manager and step_metrics.get('total_loss', 0) > 50:
                        critical_metrics = {
                            'loss': step_metrics.get('total_loss', 0),
                            'attention_diagonality': step_metrics.get('attention_diagonality', 0),
                            'grad_norm': step_metrics.get('grad_norm', 0),
                            'gate_accuracy': step_metrics.get('gate_accuracy', 0),
                            'epoch': epoch
                        }
                        
                        emergency_adaptations = self.context_aware_manager.analyze_and_adapt(
                            step=self.global_step,
                            metrics=critical_metrics,
                            model=self.model,
                            optimizer=self.optimizer
                        )
                        
                        if emergency_adaptations and any(k in emergency_adaptations for k in ['learning_rate', 'guided_attention_weight']):
                            fixes_applied += 1  # –°—á–∏—Ç–∞–µ–º –∫–∞–∫ –æ–¥–Ω—É –∞–¥–∞–ø—Ç–∞—Ü–∏—é
                            self.logger.warning(f"üéØ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
                    
                    # üî¨ –õ–ò–ú–ò–¢ –®–ê–ì–û–í –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
                    if max_steps and self.global_step >= max_steps:
                        batch_progress.close()
                        break
                        
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –≤ —à–∞–≥–µ {batch_idx}: {e}")
                    continue
            
            # üìä –ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò –≠–ü–û–•–ò
            epoch_metrics = {
                'epoch': epoch,
                'train_loss': np.mean(epoch_losses) if epoch_losses else float('inf'),
                'train_grad_norm': np.mean(epoch_grad_norms) if epoch_grad_norms else 0,
                'train_attention_diagonality': np.mean(epoch_attention_scores) if epoch_attention_scores else 0,
                'train_gate_accuracy': np.mean(epoch_gate_accuracies) if epoch_gate_accuracies else 0,
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'phase': current_phase,
                'epoch_time': time.time() - epoch_start,
                'fixes_applied': fixes_applied
            }
            
            # üìä –í–ê–õ–ò–î–ê–¶–ò–Ø
            validation_frequency = 1 if max_steps else 10
            if epoch % validation_frequency == 0 or epoch < 5:
                try:
                    val_metrics = self.validate_step(val_loader)
                    epoch_metrics.update(val_metrics)
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è
                    if val_metrics['val_loss'] < best_val_loss:
                        best_val_loss = val_metrics['val_loss']
                        patience_counter = 0
                        self._save_checkpoint(epoch, is_best=True)
                        print(f"\nüéâ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: val_loss = {best_val_loss:.4f}")
                    else:
                        patience_counter += 1
                        
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
                    val_metrics = {'val_loss': float('inf')}
                    epoch_metrics.update(val_metrics)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            training_history.append(epoch_metrics)
            
            # üìä –û–ë–ù–û–í–õ–ï–ù–ò–ï –ü–†–û–ì–†–ï–°–°-–ë–ê–†–ê –≠–ü–û–•
            epoch_progress.set_postfix({
                'Loss': f"{epoch_metrics['train_loss']:.2f}",
                'Val': f"{epoch_metrics.get('val_loss', 0):.2f}",
                'Phase': current_phase[:8],
                'LR': f"{epoch_metrics['learning_rate']:.2e}"
            })
            
            # üì± TELEGRAM –£–í–ï–î–û–ú–õ–ï–ù–ò–Ø
            telegram_frequency = 5 if max_steps else 50 
            if epoch % telegram_frequency == 0 or epoch < 5:
                self._send_enhanced_telegram_update(epoch_metrics)
            
            # üîÑ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø (ultimate —Ä–µ–∂–∏–º)
            if self.mode == 'ultimate' and epoch > 10 and len(training_history) >= 10:
                adjustments = self._perform_intelligent_adjustments(training_history[-10:])
                if adjustments:
                    print(f"\nüß† –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏: {len(adjustments)}")
            
            # üéØ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–°–¢–ê–ù–û–í–ö–ê –ü–†–ò –î–û–°–¢–ò–ñ–ï–ù–ò–ò –ö–ê–ß–ï–°–¢–í–ê
            if self._check_training_completion(epoch_metrics):
                print(f"\nüéâ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ - —Ü–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞!")
                break
            
            # üõë EARLY STOPPING
            if patience_counter >= max_patience:
                print(f"\nüõë Early stopping –ø–æ—Å–ª–µ {patience_counter} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π")
                break
                
            # üî¨ –í–´–•–û–î –ü–†–ò –î–û–°–¢–ò–ñ–ï–ù–ò–ò –õ–ò–ú–ò–¢–ê –®–ê–ì–û–í
            if max_steps and self.global_step >= max_steps:
                print(f"\nüî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –Ω–∞ {self.global_step} —à–∞–≥–∞—Ö")
                break
            
            # üîß –ü–ï–†–ò–û–î–ò–ß–ï–°–ö–û–ï –°–û–•–†–ê–ù–ï–ù–ò–ï
            save_frequency = 20 if max_steps else 100
            if epoch % save_frequency == 0:
                self._save_checkpoint(epoch, is_best=False)
        
        epoch_progress.close()
        
        # üéâ –ó–ê–í–ï–†–®–ï–ù–ò–ï –û–ë–£–ß–ï–ù–ò–Ø
        total_time = time.time() - epoch_start_time
        self._finalize_training(training_history, total_time)
        
        return training_history
    
    def _log_training_step(self, metrics: Dict, epoch: int, batch_idx: int):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏."""
        # –ü–æ–ª—É—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        system_metrics = self._get_system_metrics()
        
        # TensorBoard - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if self.tensorboard_writer:
            try:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
                self.tensorboard_writer.add_scalar('Train/Loss', metrics['total_loss'], self.global_step)
                self.tensorboard_writer.add_scalar('Train/GradNorm', metrics.get('grad_norm', 0), self.global_step)
                self.tensorboard_writer.add_scalar('Train/AttentionDiagonality', metrics.get('attention_diagonality', 0), self.global_step)
                self.tensorboard_writer.add_scalar('Train/GateAccuracy', metrics.get('gate_accuracy', 0), self.global_step)
                self.tensorboard_writer.add_scalar('Train/LearningRate', metrics.get('learning_rate', 0), self.global_step)
                
                # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ loss
                if 'mel_loss' in metrics:
                    self.tensorboard_writer.add_scalar('Loss/Mel', metrics['mel_loss'], self.global_step)
                if 'gate_loss' in metrics:
                    self.tensorboard_writer.add_scalar('Loss/Gate', metrics['gate_loss'], self.global_step)
                if 'attention_loss' in metrics:
                    self.tensorboard_writer.add_scalar('Loss/Attention', metrics['attention_loss'], self.global_step)
                
                # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                if system_metrics:
                    self.tensorboard_writer.add_scalar('System/RAM_Usage', system_metrics['ram_usage'], self.global_step)
                    self.tensorboard_writer.add_scalar('System/CPU_Usage', system_metrics['cpu_usage'], self.global_step)
                    
                    if system_metrics.get('gpu_usage'):
                        self.tensorboard_writer.add_scalar('System/GPU_Usage', system_metrics['gpu_usage'], self.global_step)
                    if system_metrics.get('gpu_memory'):
                        self.tensorboard_writer.add_scalar('System/GPU_Memory', system_metrics['gpu_memory'], self.global_step)
                    if system_metrics.get('gpu_temperature'):
                        self.tensorboard_writer.add_scalar('System/GPU_Temperature', system_metrics['gpu_temperature'], self.global_step)
                
                # –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
                current_phase = self.get_current_training_phase()
                self.tensorboard_writer.add_text('Training/Phase', current_phase, self.global_step)
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ TensorBoard: {e}")
        
        # MLflow - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if self.mlflow_run:
            try:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                mlflow_metrics = {
                    'train_loss': metrics['total_loss'],
                    'grad_norm': metrics.get('grad_norm', 0),
                    'attention_diagonality': metrics.get('attention_diagonality', 0),
                    'gate_accuracy': metrics.get('gate_accuracy', 0),
                    'learning_rate': metrics.get('learning_rate', 0)
                }
                
                # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ loss
                if 'mel_loss' in metrics:
                    mlflow_metrics['mel_loss'] = metrics['mel_loss']
                if 'gate_loss' in metrics:
                    mlflow_metrics['gate_loss'] = metrics['gate_loss']
                if 'attention_loss' in metrics:
                    mlflow_metrics['attention_loss'] = metrics['attention_loss']
                
                # –°–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                if system_metrics:
                    mlflow_metrics.update({
                        'system_ram_usage': system_metrics['ram_usage'],
                        'system_cpu_usage': system_metrics['cpu_usage']
                    })
                    
                    if system_metrics.get('gpu_usage'):
                        mlflow_metrics['system_gpu_usage'] = system_metrics['gpu_usage']
                    if system_metrics.get('gpu_memory'):
                        mlflow_metrics['system_gpu_memory'] = system_metrics['gpu_memory']
                    if system_metrics.get('gpu_temperature'):
                        mlflow_metrics['system_gpu_temperature'] = system_metrics['gpu_temperature']
                
                mlflow.log_metrics(mlflow_metrics, step=self.global_step)
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ MLflow: {e}")
    
    def _log_epoch_summary(self, metrics: Dict):
        """–ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–æ—Ö–∏."""
        epoch = metrics['epoch']
        
        self.logger.info("=" * 80)
        self.logger.info(f"üìä –≠–ü–û–•–ê {epoch} –ó–ê–í–ï–†–®–ï–ù–ê")
        self.logger.info(f"üéØ –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è: {metrics.get('phase', 'unknown')}")
        self.logger.info(f"üìà Train Loss: {metrics['train_loss']:.4f}")
        
        if 'val_loss' in metrics:
            self.logger.info(f"üìâ Val Loss: {metrics['val_loss']:.4f}")
            self.logger.info(f"üéØ Val Attention: {metrics.get('val_attention_diagonality', 0):.3f}")
            self.logger.info(f"üéØ Val Gate Acc: {metrics.get('val_gate_accuracy', 0):.3f}")
        
        self.logger.info(f"üîß Grad Norm: {metrics['train_grad_norm']:.2f}")
        self.logger.info(f"üìä Attention Diag: {metrics['train_attention_diagonality']:.3f}")
        self.logger.info(f"üéØ Gate Accuracy: {metrics['train_gate_accuracy']:.3f}")
        self.logger.info(f"‚öôÔ∏è Learning Rate: {metrics['learning_rate']:.2e}")
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è —ç–ø–æ—Ö–∏: {metrics['epoch_time']:.1f}s")
        self.logger.info("=" * 80)
    
    def _send_training_start_notification(self):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å—Ç–∞—Ä—Ç–æ–≤–æ–≥–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Telegram."""
        if not self.telegram_monitor:
            return
            
        try:
            message = f"üöÄ –ù–∞—á–∏–Ω–∞—é Ultimate Enhanced Training (—Ä–µ–∂–∏–º: {self.mode})!"
            if hasattr(self.telegram_monitor, 'send_message'):
                self.telegram_monitor.send_message(message)
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ Telegram
    
    def _send_enhanced_telegram_update(self, metrics: Dict):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –≤ Telegram —Å –∞–Ω–∞–ª–∏–∑–æ–º, –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π."""
        if not self.telegram_monitor:
            return
            
        try:
            # üèÜ –ó–ê–ì–û–õ–û–í–û–ö –° –≠–ú–û–î–ó–ò –°–¢–ê–¢–£–°–ê
            status_emoji = "üî•" if metrics['train_loss'] < 20 else "üìà" if metrics['train_loss'] < 50 else "‚ö†Ô∏è"
            message = f"{status_emoji} –≠–ø–æ—Ö–∞ {metrics['epoch']} | {metrics['phase'][:8]}\n"
            
            # üìä –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò –° –¢–†–ï–ù–î–ê–ú–ò
            message += f"üìà Loss: {metrics['train_loss']:.3f}"
            if 'val_loss' in metrics:
                trend = "üìâ" if metrics['val_loss'] < metrics['train_loss'] else "üìà"
                message += f" {trend} {metrics['val_loss']:.3f}"
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–Ω–æ—Å—Ç—å
                diff = abs(metrics['val_loss'] - metrics['train_loss'])
                message += f" (Œî{diff:.2f})"
            
            message += f"\nüéØ Gate: {metrics['train_gate_accuracy']:.3f}"
            message += f" | Attn: {metrics['train_attention_diagonality']:.3f}"
            message += f"\nüîß Grad: {metrics['train_grad_norm']:.1f}"
            message += f" | LR: {metrics['learning_rate']:.1e}"
            
            # üíª –°–ò–°–¢–ï–ú–ù–´–ï –ú–ï–¢–†–ò–ö–ò
            system_metrics = self._get_system_metrics()
            if system_metrics:
                message += f"\nüíª RAM: {system_metrics['ram_usage']:.1f}%"
                if system_metrics.get('gpu_usage'):
                    message += f" | GPU: {system_metrics['gpu_usage']:.1f}%"
                if system_metrics.get('gpu_memory'):
                    message += f" | VRAM: {system_metrics['gpu_memory']:.1f}%"
            
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ò–ó –ü–†–ï–î–´–î–£–©–ò–• –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô
            if hasattr(self, 'last_recommendations'):
                fixed_issues = self._check_fixed_issues(metrics)
                if fixed_issues:
                    message += f"\n‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: {', '.join(fixed_issues)}"
            
            # üîß –ù–û–í–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø
            if metrics.get('fixes_applied', 0) > 0:
                message += f"\nüîß –ê–≤—Ç–æ–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {metrics['fixes_applied']}"
            
            # üö® –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú –ò –ù–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
            problems, recommendations = self._analyze_training_issues(metrics)
            
            if problems:
                message += f"\n‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã: {', '.join(problems[:2])}"
                
            if recommendations:
                message += f"\nüí° –ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é: {', '.join(recommendations[:2])}"
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –≤ —Å–ª–µ–¥—É—é—â–µ–º —Å–æ–æ–±—â–µ–Ω–∏–∏
                self.last_recommendations = recommendations
            
            # üìà ASCII –ì–†–ê–§–ò–ö–ò –¢–†–ï–ù–î–û–í
            if len(self.training_metrics_history) >= 8:
                loss_chart = self._create_ascii_chart(
                    [h['train_loss'] for h in self.training_metrics_history[-8:]], 
                    "Loss"
                )
                if loss_chart:
                    message += f"\nüìà –¢—Ä–µ–Ω–¥: {loss_chart}"
                    
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
                if metrics['train_gate_accuracy'] < 0.9:
                    gate_chart = self._create_ascii_chart(
                        [h['train_gate_accuracy'] for h in self.training_metrics_history[-8:]],
                        "Gate"
                    )
                    if gate_chart:
                        message += f"\nüéØ Gate: {gate_chart}"
            
            # üîç –°–¢–ê–¢–£–° –û–ë–£–ß–ï–ù–ò–Ø
            training_status = self._get_training_status(metrics)
            message += f"\n{training_status}"
            
            # üèÜ –î–û–°–¢–ò–ñ–ï–ù–ò–Ø –ò –ü–†–û–ì–†–ï–°–°
            achievements = self._check_achievements(metrics)
            if achievements:
                message += f"\nüèÜ {achievements}"
            
            # üìä –ü–†–û–ì–ù–û–ó –í–†–ï–ú–ï–ù–ò –ó–ê–í–ï–†–®–ï–ù–ò–Ø (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
            if len(self.training_metrics_history) >= 10:
                eta = self._estimate_completion_time(metrics)
                if eta:
                    message += f"\n‚è±Ô∏è ETA: {eta}"
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞
            if hasattr(self.telegram_monitor, 'send_message'):
                self.telegram_monitor.send_message(message)
                
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ Telegram
            
    def _get_system_metrics(self) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞."""
        try:
            # RAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            ram_percent = psutil.virtual_memory().percent
            
            system_metrics = {
                'ram_usage': ram_percent,
                'cpu_usage': psutil.cpu_percent(interval=0.1)
            }
            
            # GPU –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
            if GPU_MONITORING_AVAILABLE:
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]  # –ü–µ—Ä–≤–∞—è GPU
                        system_metrics['gpu_usage'] = gpu.load * 100
                        system_metrics['gpu_memory'] = gpu.memoryUtil * 100
                        system_metrics['gpu_temperature'] = gpu.temperature
                except Exception:
                    pass
                    
            return system_metrics
        except Exception:
            return None
    
    def _check_fixed_issues(self, current_metrics: Dict) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        if not hasattr(self, 'last_recommendations'):
            return []
            
        fixed = []
        
        for recommendation in self.last_recommendations:
            if "–≤–Ω–∏–º–∞–Ω–∏–µ" in recommendation.lower():
                if current_metrics['train_attention_diagonality'] > 0.1:
                    fixed.append("–í–Ω–∏–º–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–æ")
                    
            elif "–≥—Ä–∞–¥–∏–µ–Ω—Ç" in recommendation.lower():
                if current_metrics['train_grad_norm'] < 50:
                    fixed.append("–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                    
            elif "loss" in recommendation.lower():
                if current_metrics['train_loss'] < 100:
                    fixed.append("Loss —Å–Ω–∏–∂–µ–Ω")
                    
            elif "gate" in recommendation.lower():
                if current_metrics['train_gate_accuracy'] > 0.8:
                    fixed.append("Gate accuracy —É–ª—É—á—à–µ–Ω–∞")
        
        return fixed
    
    def _analyze_training_issues(self, metrics: Dict) -> Tuple[List[str], List[str]]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        problems = []
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ attention
        if metrics['train_attention_diagonality'] < 0.05:
            problems.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ")
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å guided attention weight")
        elif metrics['train_attention_diagonality'] < 0.2:
            problems.append("–ù–∏–∑–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å alignment")
            
        # –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if metrics['train_grad_norm'] > 100:
            problems.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π gradient explosion")
            recommendations.append("–°–Ω–∏–∑–∏—Ç—å LR –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ")
        elif metrics['train_grad_norm'] > 50:
            problems.append("–ë–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã")
            recommendations.append("–£—Å–∏–ª–∏—Ç—å clipping")
            
        # –ê–Ω–∞–ª–∏–∑ loss
        if metrics['train_loss'] > 200:
            problems.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤—ã—Å–æ–∫–∏–π loss")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        elif metrics['train_loss'] > 100:
            problems.append("–í—ã—Å–æ–∫–∏–π loss")
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å epochs")
            
        # –ê–Ω–∞–ª–∏–∑ gate accuracy
        if metrics['train_gate_accuracy'] < 0.5:
            problems.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è gate accuracy")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å")
        elif metrics['train_gate_accuracy'] < 0.8:
            problems.append("–ù–∏–∑–∫–∞—è gate accuracy")
            recommendations.append("–ë–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏—è")
            
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏
        if len(self.training_metrics_history) >= 10:
            recent_losses = [h['train_loss'] for h in self.training_metrics_history[-10:]]
            if max(recent_losses) - min(recent_losses) < 0.01:
                problems.append("–°—Ç–∞–≥–Ω–∞—Ü–∏—è loss")
                recommendations.append("–ò–∑–º–µ–Ω–∏—Ç—å LR –∏–ª–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É")
        
        return problems, recommendations
    
    def _create_ascii_chart(self, values: List[float], name: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ ASCII –≥—Ä–∞—Ñ–∏–∫–∞."""
        if len(values) < 3:
            return ""
            
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
            min_val, max_val = min(values), max(values)
            if max_val == min_val:
                return f"{name}: ‚óÜ‚óÜ‚óÜ‚óÜ‚óÜ‚óÜ"
                
            normalized = [(v - min_val) / (max_val - min_val) for v in values]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞—á–µ–Ω–∏–π
            symbols = []
            for i, val in enumerate(normalized):
                if i == 0:
                    symbols.append("‚ñ™")
                else:
                    prev_val = normalized[i-1]
                    if val > prev_val + 0.1:
                        symbols.append("‚ñ≤")
                    elif val < prev_val - 0.1:
                        symbols.append("‚ñº")
                    elif val > 0.7:
                        symbols.append("‚óÜ")
                    elif val > 0.3:
                        symbols.append("‚óá")
                    else:
                        symbols.append("‚ñ´")
            
            return ''.join(symbols)
        except Exception:
            return ""
    
    def _get_training_status(self, metrics: Dict) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—É—á–µ–Ω–∏—è."""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
        if metrics['train_gate_accuracy'] > 0.95 and metrics['train_grad_norm'] < 10:
            if metrics['train_attention_diagonality'] > 0.5:
                return "üü¢ –û—Ç–ª–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
            else:
                return "üü° –•–æ—Ä–æ—à–æ, –Ω–æ –≤–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–±–æ—Ç—ã"
        elif metrics['train_loss'] < 50 and metrics['train_gate_accuracy'] > 0.8:
            return "üü° –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
        elif metrics['train_loss'] > 200 or metrics['train_grad_norm'] > 100:
            return "üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"
        else:
            return "üü† –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
    
    def _check_achievements(self, metrics: Dict) -> str:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π –≤ –æ–±—É—á–µ–Ω–∏–∏."""
        achievements = []
        
        # –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ loss
        if metrics['train_loss'] < 10:
            achievements.append("üî• Loss < 10!")
        elif metrics['train_loss'] < 20:
            achievements.append("üéØ Loss < 20")
            
        # –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ gate accuracy
        if metrics['train_gate_accuracy'] > 0.99:
            achievements.append("‚≠ê Gate 99%+")
        elif metrics['train_gate_accuracy'] > 0.95:
            achievements.append("üéØ Gate 95%+")
            
        # –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è –ø–æ attention
        if metrics['train_attention_diagonality'] > 0.8:
            achievements.append("üéØ –û—Ç–ª–∏—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ")
        elif metrics['train_attention_diagonality'] > 0.5:
            achievements.append("‚úÖ –•–æ—Ä–æ—à–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ")
            
        # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if metrics['train_grad_norm'] < 5:
            achievements.append("üîß –°—É–ø–µ—Ä-—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã")
            
        return ' | '.join(achievements) if achievements else ""
    
    def _estimate_completion_time(self, metrics: Dict) -> Optional[str]:
        """–ü—Ä–æ–≥–Ω–æ–∑ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è."""
        try:
            if len(self.training_metrics_history) < 10:
                return None
                
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ loss
            recent_losses = [h['train_loss'] for h in self.training_metrics_history[-10:]]
            if max(recent_losses) - min(recent_losses) < 0.1:
                return "‚àû (—Å—Ç–∞–≥–Ω–∞—Ü–∏—è)"
                
            # –ü—Ä–æ—Å—Ç–æ–π –ª–∏–Ω–µ–π–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑
            current_loss = metrics['train_loss']
            target_loss = 10.0  # –¶–µ–ª–µ–≤–æ–π loss
            
            if current_loss <= target_loss:
                return "üéâ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞!"
                
            # –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∏—è loss –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ø–æ—Ö–∏
            loss_decrease_rate = (recent_losses[0] - recent_losses[-1]) / len(recent_losses)
            
            if loss_decrease_rate <= 0:
                return "‚àû (loss –Ω–µ —Å–Ω–∏–∂–∞–µ—Ç—Å—è)"
                
            epochs_needed = (current_loss - target_loss) / loss_decrease_rate
            
            if epochs_needed < 50:
                return f"~{int(epochs_needed)} —ç–ø–æ—Ö –¥–æ —Ü–µ–ª–∏"
            elif epochs_needed < 200:
                return f"~{int(epochs_needed/10)*10} —ç–ø–æ—Ö"
            else:
                return "200+ —ç–ø–æ—Ö"
                
        except Exception:
            return None
            
    def _check_training_completion(self, metrics: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è."""
        try:
            # üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            completion_criteria = []
            
            # 1. –û—Ç–ª–∏—á–Ω—ã–π loss –∏ gate accuracy
            if metrics['train_loss'] < 8.0 and metrics['train_gate_accuracy'] > 0.995:
                completion_criteria.append("excellent_loss_and_gate")
            
            # 2. –û—Ç–ª–∏—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
            if metrics['train_attention_diagonality'] > 0.85:
                completion_criteria.append("excellent_attention")
            
            # 3. –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            if metrics['train_grad_norm'] < 3.0:
                completion_criteria.append("stable_gradients")
            
            # 4. –•–æ—Ä–æ—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å val_loss)
            if 'val_loss' in metrics:
                if metrics['val_loss'] < 10.0 and abs(metrics['val_loss'] - metrics['train_loss']) < 2.0:
                    completion_criteria.append("good_generalization")
            
            # 5. –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —ç–ø–æ—Ö–∏
            if len(self.training_metrics_history) >= 20:
                recent_losses = [h['train_loss'] for h in self.training_metrics_history[-20:]]
                recent_gates = [h['train_gate_accuracy'] for h in self.training_metrics_history[-20:]]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
                loss_variance = np.var(recent_losses)
                gate_variance = np.var(recent_gates)
                
                if loss_variance < 0.5 and gate_variance < 0.001:  # –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è
                    if np.mean(recent_losses) < 12.0 and np.mean(recent_gates) > 0.99:
                        completion_criteria.append("stable_excellent_metrics")
            
            # üèÜ –£–°–õ–û–í–ò–Ø –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ô –û–°–¢–ê–ù–û–í–ö–ò
            
            # Ultimate –∫–∞—á–µ—Å—Ç–≤–æ: –≤—Å–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã
            if len(completion_criteria) >= 4:
                self.logger.info(f"üèÜ ULTIMATE –ö–ê–ß–ï–°–¢–í–û –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ! –ö—Ä–∏—Ç–µ—Ä–∏–∏: {completion_criteria}")
                return True
            
            # –û—Ç–ª–∏—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: –æ—Å–Ω–æ–≤–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ + —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
            if ("excellent_loss_and_gate" in completion_criteria and 
                "stable_gradients" in completion_criteria and
                "stable_excellent_metrics" in completion_criteria):
                self.logger.info(f"üéØ –û–¢–õ–ò–ß–ù–û–ï –ö–ê–ß–ï–°–¢–í–û –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ! –ö—Ä–∏—Ç–µ—Ä–∏–∏: {completion_criteria}")
                return True
            
            # –†–µ–∂–∏–º ultimate - –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
            if self.mode == 'ultimate':
                if len(completion_criteria) >= 3 and "excellent_loss_and_gate" in completion_criteria:
                    self.logger.info(f"üèÜ ULTIMATE —Ä–µ–∂–∏–º: –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–æ! –ö—Ä–∏—Ç–µ—Ä–∏–∏: {completion_criteria}")
                    return True
            
            return False
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False
    
    def _perform_intelligent_adjustments(self, recent_history: List[Dict]):
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è Ultimate —Ä–µ–∂–∏–º–∞."""
        if not recent_history:
            return
            
        self.logger.info("üß† –í—ã–ø–æ–ª–Ω—è—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
        
        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        losses = [h['train_loss'] for h in recent_history]
        attention_scores = [h['train_attention_diagonality'] for h in recent_history]
        grad_norms = [h['train_grad_norm'] for h in recent_history]
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫
        adjustments_made = []
        
        # –ü—Ä–æ–±–ª–µ–º–∞ 1: –°—Ç–∞–≥–Ω–∞—Ü–∏—è loss
        if len(losses) > 10:
            recent_loss_trend = np.mean(losses[-5:]) - np.mean(losses[-10:-5])
            if abs(recent_loss_trend) < 0.001:  # –°—Ç–∞–≥–Ω–∞—Ü–∏—è
                if hasattr(self.criterion, 'guide_loss_weight'):
                    old_weight = self.criterion.guide_loss_weight
                    self.criterion.guide_loss_weight *= 1.5
                    adjustments_made.append(f"–£–≤–µ–ª–∏—á–µ–Ω guide_loss_weight: {old_weight:.1f} ‚Üí {self.criterion.guide_loss_weight:.1f}")
        
        # –ü—Ä–æ–±–ª–µ–º–∞ 2: –ù–∏–∑–∫–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
        avg_attention = np.mean(attention_scores[-10:]) if len(attention_scores) >= 10 else 0
        if avg_attention < 0.3:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ñ–æ–∫—É—Å –Ω–∞ attention –ë–ï–ó–û–ü–ê–°–ù–û
            if hasattr(self.criterion, 'guide_loss_weight'):
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ - –ù–ï –ë–û–õ–ï–ï 20.0!
                old_weight = self.criterion.guide_loss_weight
                self.criterion.guide_loss_weight = min(old_weight * 1.5, 15.0)  # –ú–∞–∫—Å–∏–º—É–º 15.0
                adjustments_made.append(f"–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {old_weight:.1f} ‚Üí {self.criterion.guide_loss_weight:.1f}")
        
        # –ü—Ä–æ–±–ª–µ–º–∞ 3: –í—ã—Å–æ–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        avg_grad_norm = np.mean(grad_norms[-10:]) if len(grad_norms) >= 10 else 0
        if avg_grad_norm > 5.0:
            # –°–Ω–∏–∂–∞–µ–º learning rate
            for param_group in self.optimizer.param_groups:
                old_lr = param_group['lr']
                param_group['lr'] *= 0.8
                adjustments_made.append(f"–°–Ω–∏–∂–µ–Ω LR: {old_lr:.2e} ‚Üí {param_group['lr']:.2e}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫
        if adjustments_made:
            self.logger.info("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏:")
            for adj in adjustments_made:
                self.logger.info(f"  ‚Ä¢ {adj}")
                
            # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –≤ Telegram
            if self.telegram_monitor:
                message = "üß† –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏:\n" + "\n".join(f"‚Ä¢ {adj}" for adj in adjustments_made)
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Å fallback –º–µ—Ç–æ–¥–∞–º–∏
                try:
                    if hasattr(self.telegram_monitor, 'send_message'):
                        self.telegram_monitor.send_message(message)
                    elif hasattr(self.telegram_monitor, 'send_training_update'):
                        self.telegram_monitor.send_training_update({'message': message})
                    elif hasattr(self.telegram_monitor, 'send_auto_improvement_notification'):
                        self.telegram_monitor.send_auto_improvement_notification(message)
                    else:
                        self.logger.debug("Telegram monitor –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫")
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫: {e}")
    
    def _analyze_attention_patterns(self, alignments):
        """
        üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó ATTENTION PATTERNS –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è.
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ attention matrix –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.
        """
        try:
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ numpy –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞)
            if alignments.dim() == 3:
                attention = alignments[0].detach().cpu().numpy()
            else:
                attention = alignments.detach().cpu().numpy()
            
            T_out, T_in = attention.shape
            
            # 1. –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
            diagonality = self._calculate_attention_diagonality(attention)
            
            # 2. –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å (–≤–∞–∂–Ω–æ –¥–ª—è TTS)
            monotonicity = self._calculate_attention_monotonicity(attention)
            
            # 3. –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è attention)
            focus = self._calculate_attention_focus(attention)
            
            # 4. –ü–æ–∫—Ä—ã—Ç–∏–µ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            coverage = self._calculate_attention_coverage(attention)
            
            # 5. –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å (–Ω–∏–∑–∫–∏–µ —Å–∫–∞—á–∫–∏)
            stability = self._calculate_attention_stability(attention)
            
            return {
                'diagonality': diagonality,
                'monotonicity': monotonicity,
                'focus': focus,
                'coverage': coverage,
                'stability': stability
            }
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ attention patterns: {e}")
            return {'diagonality': 0.0, 'monotonicity': 0.0, 'focus': 0.0, 'coverage': 0.0, 'stability': 0.0}
    
    def _calculate_attention_diagonality(self, attention_matrix):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention matrix."""
        try:
            T_out, T_in = attention_matrix.shape
            if T_out == 0 or T_in == 0:
                return 0.0
            
            # –°–æ–∑–¥–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –¥–∏–∞–≥–æ–Ω–∞–ª—å
            ideal_diagonal = np.zeros_like(attention_matrix)
            min_dim = min(T_out, T_in)
            
            for i in range(T_out):
                diagonal_pos = int(i * T_in / T_out) if T_out > 0 else i
                if diagonal_pos < T_in:
                    ideal_diagonal[i, diagonal_pos] = 1.0
            
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –∏–¥–µ–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–æ–Ω–∞–ª—å—é
            attention_flat = attention_matrix.flatten()
            ideal_flat = ideal_diagonal.flatten()
            
            if np.std(attention_flat) == 0 or np.std(ideal_flat) == 0:
                return 0.0
            
            correlation = np.corrcoef(attention_flat, ideal_flat)[0, 1]
            return max(0.0, correlation) if not np.isnan(correlation) else 0.0
            
        except Exception:
            return 0.0
    
    def _calculate_attention_monotonicity(self, attention_matrix):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å attention (–≤–∞–∂–Ω–æ –¥–ª—è TTS)."""
        try:
            T_out, T_in = attention_matrix.shape
            if T_out < 2:
                return 1.0
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏ attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —à–∞–≥–∞
            peaks = np.argmax(attention_matrix, axis=1)
            
            # –°—á–∏—Ç–∞–µ–º –Ω–∞—Ä—É—à–µ–Ω–∏—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏
            violations = 0
            for i in range(1, len(peaks)):
                if peaks[i] < peaks[i-1]:  # –î–≤–∏–∂–µ–Ω–∏–µ –Ω–∞–∑–∞–¥
                    violations += 1
            
            # –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å = 1 - –¥–æ–ª—è –Ω–∞—Ä—É—à–µ–Ω–∏–π
            monotonicity = 1.0 - (violations / (T_out - 1))
            return max(0.0, monotonicity)
            
        except Exception:
            return 0.0
    
    def _calculate_attention_focus(self, attention_matrix):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫—É attention (–æ–±—Ä–∞—Ç–Ω–æ–µ –∫ —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏)."""
        try:
            # –°—Ä–µ–¥–Ω—è—è —ç–Ω—Ç—Ä–æ–ø–∏—è –ø–æ –≤—Å–µ–º –≤—ã—Ö–æ–¥–Ω—ã–º —à–∞–≥–∞–º
            entropies = []
            
            for t_out in range(attention_matrix.shape[0]):
                att_weights = attention_matrix[t_out]
                att_weights_safe = np.clip(att_weights, 1e-10, 1.0)
                entropy = -np.sum(att_weights_safe * np.log(att_weights_safe))
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é
                max_entropy = np.log(len(att_weights_safe))
                normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
                entropies.append(normalized_entropy)
            
            # Focus = 1 - —Å—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
            avg_entropy = np.mean(entropies)
            return max(0.0, 1.0 - avg_entropy)
            
        except Exception:
            return 0.0
    
    def _calculate_attention_coverage(self, attention_matrix):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        try:
            # –°—É–º–º–∞ attention –≤–µ—Å–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –≤—Ö–æ–¥–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏
            input_coverage = np.sum(attention_matrix, axis=0)
            
            # –î–æ–ª—è –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π —Å –∑–Ω–∞—á–∏–º—ã–º attention (>1% –æ—Ç –º–∞–∫—Å–∏–º—É–º–∞)
            threshold = 0.01 * np.max(input_coverage)
            covered_positions = np.sum(input_coverage > threshold)
            total_positions = len(input_coverage)
            
            coverage = covered_positions / total_positions if total_positions > 0 else 0.0
            return coverage
            
        except Exception:
            return 0.0
    
    def _calculate_attention_stability(self, attention_matrix):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å attention (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–µ–∑–∫–∏—Ö —Å–∫–∞—á–∫–æ–≤)."""
        try:
            if attention_matrix.shape[0] < 2:
                return 1.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è attention –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —à–∞–≥–∞–º–∏
            differences = []
            
            for i in range(1, attention_matrix.shape[0]):
                diff = np.abs(attention_matrix[i] - attention_matrix[i-1])
                differences.append(np.mean(diff))
            
            # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å = 1 - —Å—Ä–µ–¥–Ω—è—è –≤–µ–ª–∏—á–∏–Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            avg_change = np.mean(differences)
            stability = max(0.0, 1.0 - avg_change)
            return stability
            
        except Exception:
            return 0.0

    def _save_checkpoint(self, epoch: int, is_best: bool = False):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
        try:
            checkpoint = {
                'epoch': epoch,
                'global_step': self.global_step,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'best_validation_loss': self.best_validation_loss,
                'hparams': vars(self.hparams),
                'training_mode': self.mode
            }
            
            filename = f"checkpoint_epoch_{epoch}.pt"
            if is_best:
                filename = "best_model.pt"
                
            torch.save(checkpoint, filename)
            self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {filename}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
    
    def _finalize_training(self, training_history: List[Dict], total_time: float):
        """–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å."""
        self.logger.info("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        final_stats = {
            'total_epochs': len(training_history),
            'total_time_hours': total_time / 3600,
            'best_train_loss': min(h['train_loss'] for h in training_history),
            'final_attention_score': training_history[-1]['train_attention_diagonality'],
            'final_gate_accuracy': training_history[-1]['train_gate_accuracy']
        }
        
        self.logger.info(f"üìä –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {final_stats['total_epochs']}")
        self.logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {final_stats['total_time_hours']:.1f} —á–∞—Å–æ–≤")
        self.logger.info(f"üèÜ –õ—É—á—à–∏–π train loss: {final_stats['best_train_loss']:.4f}")
        self.logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ: {final_stats['final_attention_score']:.3f}")
        self.logger.info(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å gate: {final_stats['final_gate_accuracy']:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        try:
            import json
            import numpy as np
            
            # üîß –£–õ–£–ß–®–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –ö–û–ù–í–ï–†–¢–ê–¶–ò–ò –î–õ–Ø JSON –°–ï–†–ò–ê–õ–ò–ó–ê–¶–ò–ò
            def convert_numpy_types(obj):
                """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–∏–ø—ã –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ."""
                try:
                    # Numpy —Ç–∏–ø—ã
                    if isinstance(obj, (np.integer, np.floating)):
                        return obj.item()
                    elif isinstance(obj, np.ndarray):
                        return obj.tolist()
                    elif isinstance(obj, np.bool_):
                        return bool(obj)
                    
                    # Python type objects (–∫–ª–∞—Å—Å, —Ñ—É–Ω–∫—Ü–∏—è –∏ —Ç.–¥.)
                    elif isinstance(obj, type):
                        return f"<type:{obj.__name__}>"
                    elif callable(obj):
                        return f"<callable:{getattr(obj, '__name__', str(obj))}>"
                    
                    # Tensor –æ–±—ä–µ–∫—Ç—ã
                    elif hasattr(obj, 'detach') and hasattr(obj, 'cpu'):  # PyTorch tensor
                        return obj.detach().cpu().numpy().tolist()
                    elif hasattr(obj, 'numpy'):  # TensorFlow tensor
                        return obj.numpy().tolist()
                    
                    # Datetime –æ–±—ä–µ–∫—Ç—ã
                    elif hasattr(obj, 'isoformat'):  # datetime objects
                        return obj.isoformat()
                    
                    # Pathlib –ø—É—Ç–∏
                    elif hasattr(obj, '__fspath__'):  # pathlib.Path
                        return str(obj)
                    
                    # Enum –æ–±—ä–µ–∫—Ç—ã
                    elif hasattr(obj, 'value') and hasattr(obj, 'name'):  # Enum
                        return obj.value
                    
                    # Complex —á–∏—Å–ª–∞
                    elif isinstance(obj, complex):
                        return {'real': obj.real, 'imag': obj.imag}
                    
                    # Bytes –æ–±—ä–µ–∫—Ç—ã
                    elif isinstance(obj, (bytes, bytearray)):
                        return obj.decode('utf-8', errors='replace')
                    
                    # –ú–Ω–æ–∂–µ—Å—Ç–≤–∞
                    elif isinstance(obj, set):
                        return list(obj)
                    elif isinstance(obj, frozenset):
                        return list(obj)
                    
                    # –ö–æ–ª–ª–µ–∫—Ü–∏–∏
                    elif isinstance(obj, dict):
                        return {str(k): convert_numpy_types(v) for k, v in obj.items()}
                    elif isinstance(obj, (list, tuple)):
                        return [convert_numpy_types(item) for item in obj]
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    elif obj is None:
                        return None
                    elif isinstance(obj, (str, int, float, bool)):
                        return obj
                    elif obj == float('inf'):
                        return "infinity"
                    elif obj == float('-inf'):
                        return "-infinity"
                    elif obj != obj:  # NaN check
                        return "NaN"
                    
                    # –°–ª–æ–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã —Å __dict__
                    elif hasattr(obj, '__dict__'):
                        if hasattr(obj, '__class__'):
                            class_name = obj.__class__.__name__
                            # –ò—Å–∫–ª—é—á–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∫–ª–∞—Å—Å—ã
                            if class_name in ['Logger', 'TextIOWrapper', 'Thread', 'Lock']:
                                return f"<{class_name}>"
                            # –ü—ã—Ç–∞–µ–º—Å—è —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã–µ –æ–±—ä–µ–∫—Ç—ã
                            try:
                                return {
                                    '_class': class_name,
                                    **{k: convert_numpy_types(v) for k, v in obj.__dict__.items() 
                                       if not k.startswith('_') and not callable(v)}
                                }
                            except:
                                return f"<{class_name}:not_serializable>"
                        return str(obj)
                    
                    # Fallback: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
                    else:
                        return str(obj)
                        
                except Exception as e:
                    # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
                    return f"<serialization_error:{type(obj).__name__}>"
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            report_data = {
                'final_stats': convert_numpy_types(final_stats),
                'training_history': convert_numpy_types(training_history),
                'mode': self.mode,
                'hparams': convert_numpy_types(vars(self.hparams))
            }
            
            with open('ultimate_training_report.json', 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            self.logger.info("üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: ultimate_training_report.json")
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        if self.telegram_monitor:
            try:
                message = f"""üéâ ULTIMATE TRAINING –ó–ê–í–ï–†–®–ï–ù–û!

üìä –≠–ø–æ—Ö: {final_stats['total_epochs']}
‚è±Ô∏è –í—Ä–µ–º—è: {final_stats['total_time_hours']:.1f}—á
üèÜ –õ—É—á—à–∏–π Loss: {final_stats['best_train_loss']:.4f}
üéØ –í–Ω–∏–º–∞–Ω–∏–µ: {final_stats['final_attention_score']:.3f}
üéØ Gate Acc: {final_stats['final_gate_accuracy']:.3f}

üöÄ –†–µ–∂–∏–º: {self.mode}
‚úÖ –í—Å–µ —Å–∏—Å—Ç–µ–º—ã —Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ!"""
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                if hasattr(self.telegram_monitor, 'send_message'):
                    self.telegram_monitor.send_message(message)
                else:
                    self.logger.debug("Telegram monitor –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É —Å–æ–æ–±—â–µ–Ω–∏–π")
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
        
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–≥–≥–µ—Ä–æ–≤
        if self.tensorboard_writer:
            self.tensorboard_writer.close()
        if self.mlflow_run:
            mlflow.end_run()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Ultimate Enhanced Tacotron Trainer."""
    parser = argparse.ArgumentParser(description='Ultimate Enhanced Tacotron Trainer')
    parser.add_argument('--mode', choices=['simple', 'enhanced', 'auto_optimized', 'ultimate'], 
                       default='enhanced', help='–†–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--config', type=str, default='hparams.py', help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--epochs', type=int, default=3500, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö')
    parser.add_argument('--max-steps', type=int, default=None, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)')
    parser.add_argument('--dataset-path', type=str, required=True, help='–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    hparams = create_hparams()
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
    dataset_info = {}
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫ Ultimate Enhanced Tacotron Trainer –≤ —Ä–µ–∂–∏–º–µ '{args.mode}'")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ trainer'–∞
    trainer = UltimateEnhancedTacotronTrainer(
        hparams=hparams,
        mode=args.mode,
        dataset_info=dataset_info
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataLoader'–æ–≤
    try:
        print("üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataLoader'–æ–≤...")
        
        if DATA_UTILS_AVAILABLE:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º CSV
            import os
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º CSV
            if os.path.isdir(args.dataset_path):
                # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –∏—â–µ–º train.csv –∏ val.csv –≤–Ω—É—Ç—Ä–∏
                train_file = os.path.join(args.dataset_path, 'train.csv')
                val_file = os.path.join(args.dataset_path, 'val.csv')
            else:
                # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é
                train_file = args.dataset_path
                val_file = args.dataset_path.replace('train', 'val')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
            if not os.path.exists(train_file):
                raise FileNotFoundError(f"–§–∞–π–ª –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω: {train_file}")
            if not os.path.exists(val_file):
                print(f"‚ö†Ô∏è –§–∞–π–ª –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {val_file}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª –æ–±—É—á–µ–Ω–∏—è")
                val_file = train_file
            
            print(f"üìÇ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª—ã:")
            print(f"   –û–±—É—á–µ–Ω–∏–µ: {train_file}")
            print(f"   –í–∞–ª–∏–¥–∞—Ü–∏—è: {val_file}")
            
            trainset = TextMelLoader(train_file, hparams)
            valset = TextMelLoader(val_file, hparams)
            collate_fn = TextMelCollate(hparams.n_frames_per_step)
            
            train_loader = DataLoader(
                trainset, 
                num_workers=1, 
                shuffle=True,
                sampler=None,
                batch_size=hparams.batch_size, 
                pin_memory=False,
                drop_last=True, 
                collate_fn=collate_fn
            )
            
            val_loader = DataLoader(
                valset, 
                num_workers=1, 
                shuffle=False,
                sampler=None,
                batch_size=hparams.batch_size, 
                pin_memory=False,
                collate_fn=collate_fn
            )
            
            print(f"‚úÖ DataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä Train samples: {len(trainset)}")
            print(f"üìä Val samples: {len(valset)}")
            
        else:
            print("‚ö†Ô∏è data_utils –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Å–æ–∑–¥–∞–Ω–∏–µ mock DataLoader'–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ mock –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            class MockDataset(torch.utils.data.Dataset):
                def __init__(self, size=100):
                    self.size = size
                
                def __len__(self):
                    return self.size
                    
                def __getitem__(self, idx):
                    return (
                        torch.randint(0, 100, (50,)),  # text
                        torch.randn(80, 100),           # mel
                        torch.randint(0, 2, (100,))    # gate
                    )
            
            train_loader = DataLoader(MockDataset(1000), batch_size=8, shuffle=True)
            val_loader = DataLoader(MockDataset(200), batch_size=8, shuffle=False)
            
            print("‚úÖ Mock DataLoader'—ã —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        trainer.initialize_training()
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        if args.max_steps:
            print(f"üî¨ –ù–∞—á–∏–Ω–∞—é –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –≤ —Ä–µ–∂–∏–º–µ '{args.mode}' –Ω–∞ {args.max_steps} —à–∞–≥–æ–≤...")
            trainer.train(train_loader, val_loader, args.epochs, args.max_steps)
        else:
            print(f"üèÜ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –≤ —Ä–µ–∂–∏–º–µ '{args.mode}' –Ω–∞ {args.epochs} —ç–ø–æ—Ö...")
            trainer.train(train_loader, val_loader, args.epochs)
        
        print("üéâ Ultimate Enhanced Tacotron Trainer –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É!")
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –¥–µ–º–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
        print("\nüîß –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–µ–º–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –±–∞–∑–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π...")
        try:
            trainer.initialize_training()
            print("‚úÖ –ë–∞–∑–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ!")
            print("‚ö†Ô∏è –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏")
        except Exception as e2:
            print(f"‚ùå –î–∞–∂–µ –±–∞–∑–æ–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main() 