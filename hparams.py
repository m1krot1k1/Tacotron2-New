# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è hparams –¥–ª—è Tacotron2
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å —Ç–æ–ª—Å—Ç—ã–º–∏ –ø–æ–ª–æ—Å–∞–º–∏ attention –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
# –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Smart Tuner V2 —Å–∏—Å—Ç–µ–º–æ–π

from tools import HParams
from text import symbols
import logging

def create_hparams(hparams_string=None, verbose=False):
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ –¥–ª—è attention alignment."""

    hparams = HParams(
        ################################
        # Experiment Parameters        #
        ################################
        epochs=500000,
        iters_per_checkpoint=1000,          # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        save_interval=2000,                 # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
        validate_interval=200,              # –ë–æ–ª–µ–µ —á–∞—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        validation_freq=50,                 # –ß–∞—Å—Ç–æ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (iterations) - —É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        warmup_steps=2000,                  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        seed=1234,
        dynamic_loss_scaling=True,
        fp16_run=True,                     # –û—Ç–∫–ª—é—á–µ–Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (—Ç—Ä–µ–±—É–µ—Ç NVIDIA Apex)
        distributed_run=False,
        dist_backend="nccl",
        dist_url="tcp://localhost:54321",
        cudnn_enabled=True,
        cudnn_benchmark=False,
        ignore_layers=['embedding.weight'],
        mmi_ignore_layers=["decoder.linear_projection.linear_layer.weight", 
                          "decoder.linear_projection.linear_layer.bias", 
                          "decoder.gate_layer.linear_layer.weight"],

        ################################
        # Data Parameters             #
        ################################
        load_mel_from_disk=False,
        dataset_path='data/',
        training_files='data/dataset/train.csv',
        validation_files='data/dataset/val.csv',
        text_cleaners=['transliteration_cleaners_with_stress'],

        ################################
        # Audio Parameters             #
        ################################
        max_wav_value=32768.0,
        sampling_rate=22050,
        filter_length=1024,
        hop_length=256,
        win_length=1024,
        n_mel_channels=80,
        mel_fmin=0.0,
        mel_fmax=8000.0,

        ################################
        # Model Parameters             #
        ################################
        n_symbols=len(symbols),
        symbols_embedding_dim=512,

        # Encoder parameters - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
        encoder_kernel_size=5,
        encoder_n_convolutions=3,
        encoder_embedding_dim=512,

        # Decoder parameters - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è attention alignment
        n_frames_per_step=1,                # –û—Å—Ç–∞–≤–ª—è–µ–º 1 –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        decoder_rnn_dim=1024,
        prenet_dim=256,
        max_decoder_steps=2500,             # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        gate_threshold=0.5,
        # üîß –ù–û–í–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ gate threshold
        adaptive_gate_threshold=True,       # –í–∫–ª—é—á–∏—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π gate threshold
        gate_min_threshold=0.3,            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ gate
        gate_max_threshold=0.8,            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ gate
        
        p_attention_dropout=0.005,          # üî• –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–û –°–ù–ò–ñ–ï–ù–û! Attention dropout —É–±–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ
        p_decoder_dropout=0.01,             # üî• –ú–ò–ù–ò–ú–ò–ó–ò–†–û–í–ê–ù–û –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ decoder
        
        # üîß –ù–û–í–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è curriculum learning
        p_teacher_forcing=1.0,             # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        curriculum_teacher_forcing=True,    # –í–∫–ª—é—á–∏—Ç—å curriculum learning
        teacher_forcing_decay=0.999,       # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è
        min_teacher_forcing=0.7,           # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

        # Attention parameters - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ç–æ–Ω–∫–∏—Ö –ø–æ–ª–æ—Å
        attention_rnn_dim=1024,
        attention_dim=128,

        # Location Layer parameters - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è attention
        attention_location_n_filters=32,
        attention_location_kernel_size=31,

        # Mel-post processing network parameters
        postnet_embedding_dim=512,
        postnet_kernel_size=5,
        postnet_n_convolutions=5,

        # GST - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Global Style Tokens
        use_gst=True,  # ‚úÖ –í–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç–∏–ª–µ–π —Ä–µ—á–∏
        ref_enc_filters=[32, 32, 64, 64, 128, 128],
        ref_enc_size=[3, 3],
        ref_enc_strides=[2, 2],
        ref_enc_pad=[1, 1],
        ref_enc_gru_size=128,

        # Style Token Layer - —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        token_embedding_size=256,
        token_num=8,                        # –£–ú–ï–ù–¨–®–ï–ù–û —Å 10 –¥–æ 8 –¥–ª—è –ª—É—á—à–µ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        num_heads=4,                        # –£–ú–ï–ù–¨–®–ï–ù–û —Å 8 –¥–æ 4 –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏

        no_dga=False,

        ################################
        # Optimization Hyperparameters #
        ################################
        use_saved_learning_rate=False,
        learning_rate=1e-5,                 # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –£–õ–£–ß–®–ï–ù–ò–ï: —Å–Ω–∏–∂–µ–Ω–æ —Å 5e-4 –¥–æ 1e-5 –ø–æ Very Attentive Tacotron
        learning_rate_decay=0.95,           # üîß –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
        learning_rate_decay_patience=3000,  # üîß –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ç–µ—Ä–ø–µ–Ω–∏–µ –¥–ª—è TTS
        learning_rate_min=5e-7,             # üîß –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π learning rate
        weight_decay=1e-7,                  # üîß –ï—â–µ –º–µ–Ω—å—à–µ regularization
        grad_clip_thresh=0.5,               # üîß –ú–µ–Ω—å—à–∏–π gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        batch_size=32,                      # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: —É–≤–µ–ª–∏—á–µ–Ω–æ –¥–æ 32 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ attention
        mask_padding=True,

        ################################
        # FINE-TUNE - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è #
        ################################
        use_mmi=True,
        mmi_map=None,                       # MMI –∫–∞—Ä—Ç–∞ (None –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è)
        mmi_weight=0.1,                     # –í–µ—Å MMI loss

        # üî• –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: Guided Attention –ø–æ Very Attentive Tacotron 2025
        drop_frame_rate=0.01,               # üî• –ú–ò–ù–ò–ú–ò–ó–ò–†–û–í–ê–ù–û –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        use_gaf=True,
        update_gaf_every_n_step=1,          # üî• –ö–ê–ñ–î–´–ô –®–ê–ì! –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ alignment
        max_gaf=0.1,                        # üî• –ú–ò–ù–ò–ú–ê–õ–¨–ù–û–ï –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –º—è–≥–∫–æ–≥–æ –Ω–∞—á–∞–ª–∞
        
        # üî• –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ô –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤–µ—Å –¥–ª—è Guide Loss 
        use_dynamic_guide_loss=True,
        guide_loss_initial_weight=15.0,     # üî• –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –£–í–ï–õ–ò–ß–ï–ù–û! –ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ alignment
        guide_loss_decay_start=2000,        # üî• –†–∞–Ω—å—à–µ –Ω–∞—á–∏–Ω–∞–µ–º —Å–Ω–∏–∂–∞—Ç—å –ø–æ—Å–ª–µ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        guide_loss_decay_steps=25000,       # üî• –ë—ã—Å—Ç—Ä–µ–µ —Å–Ω–∏–∂–∞–µ–º –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        
        global_mean_npy='ruslan_global_mean.npy',
        
        ################################
        # Regularization & Stability  #
        ################################
        # üî• –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–´–ï –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è dropout –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ alignment
        dropout_rate=0.01,                  # üî• –ú–ò–ù–ò–ú–ò–ó–ò–†–û–í–ê–ù–û! Dropout —É–±–∏–≤–∞–µ—Ç attention quality
        encoder_dropout_rate=0.005,         # üî• –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò –£–ë–†–ê–ù–û –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ encoder
        postnet_dropout_rate=0.01,          # üî• –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô dropout –¥–ª—è —á–∏—Å—Ç–æ–≥–æ postnet
        
        # Early stopping parameters - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
        early_stopping=True,
        early_stopping_patience=15,         # –£–í–ï–õ–ò–ß–ï–ù–û —Å 10 –¥–æ 15
        early_stopping_min_delta=0.0005,    # –£–ú–ï–ù–¨–®–ï–ù–û —Å 0.001 –¥–æ 0.0005

        ################################
        # NEW: Attention Optimization  #
        ################################
        # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è attention alignment
        attention_alignment_loss_weight=1.0,  # –í–µ—Å loss –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
        use_attention_smoothing=True,         # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ attention
        attention_smoothing_factor=0.1,      # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
        
        # Teacher forcing scheduling - –Ω–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        use_scheduled_teacher_forcing=True,   # –í–∫–ª—é—á–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ teacher forcing
        teacher_forcing_start_ratio=1.0,     # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        teacher_forcing_end_ratio=0.5,       # –ö–æ–Ω–µ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        teacher_forcing_decay_start=10000,   # –ù–∞—á–∞–ª–æ —Å–Ω–∏–∂–µ–Ω–∏—è
        teacher_forcing_decay_steps=50000,   # –®–∞–≥–∏ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è

        ################################
        # NEW: Advanced Training       #
        ################################
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        use_balanced_sampling=True,          # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
        gradient_accumulation_steps=1,       # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        max_grad_norm=1.0,                  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
        
        # Loss scaling –¥–ª—è FP16 - —É–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        loss_scale_init=512.0,              # –ù–∞—á–∞–ª—å–Ω—ã–π –º–∞—Å—à—Ç–∞–± loss
        loss_scale_factor=2.0,              # –§–∞–∫—Ç–æ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∞
        
        # Attention mechanism improvements
        use_forward_attention=False,         # –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –¥–ª—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏
        use_location_sensitive_attention=True,  # –£–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º
        
        # Memory optimization
        use_memory_efficient_attention=True, # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è attention
        attention_chunk_size=None,          # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è attention
        
        ################################
        # NEW: Modern Loss Functions   #
        ################################
        # üéµ –ù–æ–≤—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 2024-2025
        mel_loss_weight=1.0,                # –û—Å–Ω–æ–≤–Ω–æ–π mel loss
        gate_loss_weight=1.0,               # Gate loss
        guide_loss_weight=2.5,              # üî• –£–í–ï–õ–ò–ß–ï–ù–û –¥–ª—è –ª—É—á—à–µ–≥–æ alignment
        
        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏
        spectral_loss_weight=0.3,           # SpectralMelLoss –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞—Å—Ç–æ—Ç
        perceptual_loss_weight=0.2,         # PerceptualLoss –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è
        style_loss_weight=0.1,              # StyleLoss –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞
        monotonic_loss_weight=0.1,          # MonotonicAlignmentLoss –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        # Audio Quality Enhancement
        use_audio_quality_enhancement=True, # –í–∫–ª—é—á–∏—Ç—å —Å–∏—Å—Ç–µ–º—É —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        quality_enhancement_threshold=0.6,  # –ü–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏–π
        noise_gate_enabled=True,            # –í–∫–ª—é—á–∏—Ç—å noise gate
        spectral_enhancement_enabled=True,  # –í–∫–ª—é—á–∏—Ç—å —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
        
        ################################
        # NEW: Smart Tuner V2 TTS Parameters #
        ################################
        # TTS-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Smart Tuner V2 (—Ç–æ–ª—å–∫–æ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ)
        guided_attention_enabled=True,       # –í–∫–ª—é—á–µ–Ω–∏–µ guided attention
        # guide_loss_weight —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤—ã—à–µ –∫–∞–∫ 2.5
        
        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤—ã—à–µ:
        # p_attention_dropout (–∫–∞–∫ attention_dropout), gate_threshold, 
        # dropout_rate/encoder_dropout_rate/postnet_dropout_rate (–∫–∞–∫ prenet/postnet_dropout)
        
        ################################
        # NEW: Monitoring & Diagnostics #
        ################################
        # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        log_attention_weights=True,          # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ attention
        log_alignment_metrics=True,          # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
        alignment_diagonal_threshold=0.5,    # –ü–æ—Ä–æ–≥ –¥–ª—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        
        # Validation enhancements
        validation_attention_analysis=True,  # –ê–Ω–∞–ª–∏–∑ attention –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        save_attention_plots=True,          # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ attention
        attention_plot_frequency=1000,      # –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤

        # Guided Attention
        use_guided_attn=True,
        guided_attn_weight=1.0,             # –í–µ—Å guided attention loss
    )

    if hparams_string:
        logging.info('Parsing command line hparams: %s', hparams_string)
        hparams.parse(hparams_string)

    if verbose:
        logging.info('Final parsed hparams: %s', hparams.values())

    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    _validate_attention_params(hparams)
    
    return hparams

def _validate_attention_params(hparams):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ attention alignment."""
    warnings = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ dropout –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if hparams.dropout_rate > 0.4:
        warnings.append("dropout_rate > 0.4 –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å attention alignment")
    
    if hparams.p_attention_dropout > 0.15:
        warnings.append("p_attention_dropout > 0.15 –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å —Ç–æ–ª—Å—Ç—ã–µ –ø–æ–ª–æ—Å—ã")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ guided attention –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if hparams.max_gaf > 0.7:
        warnings.append("max_gaf > 0.7 –º–æ–∂–µ—Ç —Ä–∞–∑–º–∞–∑–∞—Ç—å attention")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ batch size –¥–ª—è TTS (–º–µ–Ω—å—à–∏–π batch_size –ª—É—á—à–µ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞)
    if hparams.batch_size < 4:
        warnings.append("batch_size < 4 —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
    elif hparams.batch_size > 32:
        warnings.append("batch_size > 32 –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ attention –¥–ª—è TTS")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GST –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if hparams.token_num > 10:
        warnings.append("token_num > 10 –º–æ–∂–µ—Ç —É—Ö—É–¥—à–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å GST")
    
    if warnings:
        logging.warning("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
        for warning in warnings:
            logging.warning(f"  - {warning}")
    else:
        logging.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ attention alignment")