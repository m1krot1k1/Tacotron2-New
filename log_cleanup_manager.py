#!/usr/bin/env python3
"""
üóëÔ∏è UNIVERSAL LOG CLEANUP MANAGER –¥–ª—è Tacotron2-New
–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –í–°–ï–• –ª–æ–≥–æ–≤ –∏ –¥–∞—à–±–æ—Ä–¥–æ–≤ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –æ–±—É—á–µ–Ω–∏—è

–û—á–∏—â–∞–µ—Ç:
‚úÖ TensorBoard –ª–æ–≥–∏ (tensorboard_logs/)
‚úÖ MLflow –ª–æ–≥–∏ (mlruns/)
‚úÖ Unified Logging System (unified_logs/)
‚úÖ –î–∞—à–±–æ—Ä–¥—ã –ë–î (monitoring.db, dashboard_metrics.db –∏ —Ç.–¥.)
‚úÖ Smart Tuner –ª–æ–≥–∏ –∏ –ë–î
‚úÖ –í—Å–µ .log —Ñ–∞–π–ª—ã
‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ (plots/)
‚úÖ Checkpoint backup'—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
"""

import os
import shutil
import sqlite3
import logging
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timedelta
import json
from dataclasses import dataclass, asdict

@dataclass
class CleanupStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏"""
    directories_cleaned: int = 0
    files_removed: int = 0
    databases_cleaned: int = 0
    space_freed_mb: float = 0.0
    cleanup_time_seconds: float = 0.0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []

class LogCleanupManager:
    """
    üóëÔ∏è –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –æ—á–∏—Å—Ç–∫–∏ –ª–æ–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤
    """
    
    def __init__(self, project_root: str = ".", keep_last_days: int = 7, dry_run: bool = False):
        self.project_root = Path(project_root)
        self.keep_last_days = keep_last_days
        self.dry_run = dry_run
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = self._setup_logger()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏
        self.cleanup_config = self._get_cleanup_config()
        
        self.logger.info(f"üóëÔ∏è LogCleanupManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        self.logger.info(f"üìÅ –ü—Ä–æ–µ–∫—Ç: {self.project_root}")
        self.logger.info(f"üìÖ –•—Ä–∞–Ω–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ: {self.keep_last_days} –¥–Ω–µ–π")
        self.logger.info(f"üîç Dry run: {'–î–∞' if self.dry_run else '–ù–µ—Ç'}")
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger("LogCleanupManager")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - üóëÔ∏è %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _get_cleanup_config(self) -> Dict:
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        return {
            # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ª–æ–≥–æ–≤
            'log_directories': [
                'tensorboard_logs',
                'mlruns',
                'unified_logs',
                'logs',
                'logs_auto_fixes',
                'smart_tuner/logs',
                'plots',
                'output'  # TensorBoard –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            ],
            
            # –ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤
            'dashboard_databases': [
                'monitoring.db',
                'dashboard_metrics.db',
                'rollback_states.db',
                'demo_monitoring.db',
                'demo_risk_assessment.db',
                'risk_assessments.db',
                'smart_tuner/optuna_studies.db',
                'smart_tuner/epoch_optimizer_history.db', 
                'smart_tuner/quality_control_history.db',
                'smart_tuner/advisor_kb.db',
                'meta_learning_data/meta_learning_data.db'
            ],
            
            # –õ–æ–≥ —Ñ–∞–π–ª—ã
            'log_files': [
                '*.log',
                'ultimate_training.log',
                'mlflow.log',
                'tensorboard.log',
                'production_dashboard.log',
                'smart_tuner_web.log',
                'risk_assessment_demo.log'
            ],
            
            # Checkpoint backup (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            'checkpoint_backups': [
                'checkpoint_backup',
                'checkpoint_compressed',
                'rollback_checkpoints'
            ],
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            'temp_files': [
                'emergency_restart_*.json',
                '*.backup',
                'temp_*',
                'tmp_*'
            ]
        }
    
    def cleanup_all(self, include_checkpoints: bool = False) -> CleanupStats:
        """
        üóëÔ∏è –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –ª–æ–≥–æ–≤ –∏ –¥–∞—à–±–æ—Ä–¥–æ–≤
        
        Args:
            include_checkpoints: –í–∫–ª—é—á–∏—Ç—å –æ—á–∏—Å—Ç–∫—É checkpoint backup'–æ–≤
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏
        """
        start_time = time.time()
        stats = CleanupStats()
        
        self.logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –ø–æ–ª–Ω—É—é –æ—á–∏—Å—Ç–∫—É –ª–æ–≥–æ–≤ –∏ –¥–∞—à–±–æ—Ä–¥–æ–≤...")
        if self.dry_run:
            self.logger.info("üîç DRY RUN —Ä–µ–∂–∏–º - –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        
        try:
            # 1. –û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ª–æ–≥–æ–≤
            dir_stats = self._cleanup_log_directories()
            stats.directories_cleaned += dir_stats[0]
            stats.files_removed += dir_stats[1]
            stats.space_freed_mb += dir_stats[2]
            
            # 2. –û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤
            db_stats = self._cleanup_dashboard_databases()
            stats.databases_cleaned += db_stats[0]
            stats.space_freed_mb += db_stats[1]
            
            # 3. –û—á–∏—Å—Ç–∫–∞ –ª–æ–≥ —Ñ–∞–π–ª–æ–≤
            file_stats = self._cleanup_log_files()
            stats.files_removed += file_stats[0]
            stats.space_freed_mb += file_stats[1]
            
            # 4. –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            temp_stats = self._cleanup_temp_files()
            stats.files_removed += temp_stats[0]
            stats.space_freed_mb += temp_stats[1]
            
            # 5. –û—á–∏—Å—Ç–∫–∞ checkpoint backup'–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if include_checkpoints:
                checkpoint_stats = self._cleanup_checkpoint_backups()
                stats.directories_cleaned += checkpoint_stats[0]
                stats.files_removed += checkpoint_stats[1]
                stats.space_freed_mb += checkpoint_stats[2]
            
            stats.cleanup_time_seconds = time.time() - start_time
            
            # –û—Ç—á–µ—Ç
            self._print_cleanup_report(stats)
            self._save_cleanup_report(stats)
            
            return stats
            
        except Exception as e:
            error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}"
            self.logger.error(error_msg)
            stats.errors.append(error_msg)
            return stats
    
    def _cleanup_log_directories(self) -> Tuple[int, int, float]:
        """–û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ª–æ–≥–æ–≤"""
        self.logger.info("üìÅ –û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –ª–æ–≥–æ–≤...")
        
        dirs_cleaned = 0
        files_removed = 0
        space_freed = 0.0
        cutoff_date = datetime.now() - timedelta(days=self.keep_last_days)
        
        for dir_name in self.cleanup_config['log_directories']:
            dir_path = self.project_root / dir_name
            
            if not dir_path.exists():
                continue
                
            self.logger.info(f"  üßπ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {dir_path}")
            
            try:
                # –ü–æ–¥—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –¥–æ –æ—á–∏—Å—Ç–∫–∏
                dir_size_before = self._get_directory_size(dir_path)
                
                if dir_name in ['tensorboard_logs', 'mlruns']:
                    # –î–ª—è TensorBoard –∏ MLflow —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ runs
                    removed_count = self._cleanup_timestamped_directories(dir_path, cutoff_date)
                    files_removed += removed_count
                else:
                    # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
                    if not self.dry_run:
                        shutil.rmtree(dir_path)
                        dir_path.mkdir(exist_ok=True)
                    
                    files_removed += len(list(dir_path.rglob('*'))) if dir_path.exists() else 0
                
                # –ü–æ–¥—Å—á–µ—Ç –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–Ω–æ–≥–æ –º–µ—Å—Ç–∞
                dir_size_after = self._get_directory_size(dir_path) if dir_path.exists() else 0
                space_freed += (dir_size_before - dir_size_after) / (1024 * 1024)  # MB
                
                dirs_cleaned += 1
                self.logger.info(f"    ‚úÖ –û—á–∏—â–µ–Ω–æ: {dir_name}")
                
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ {dir_path}: {e}"
                self.logger.error(f"    ‚ùå {error_msg}")
        
        return dirs_cleaned, files_removed, space_freed
    
    def _cleanup_dashboard_databases(self) -> Tuple[int, float]:
        """–û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤"""
        self.logger.info("üóÑÔ∏è –û—á–∏—Å—Ç–∫–∞ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤...")
        
        databases_cleaned = 0
        space_freed = 0.0
        
        for db_path_str in self.cleanup_config['dashboard_databases']:
            db_path = self.project_root / db_path_str
            
            if not db_path.exists():
                continue
            
            self.logger.info(f"  üßπ –û—á–∏—â–∞—é –ë–î: {db_path}")
            
            try:
                # –†–∞–∑–º–µ—Ä –¥–æ –æ—á–∏—Å—Ç–∫–∏
                size_before = db_path.stat().st_size / (1024 * 1024)  # MB
                
                if not self.dry_run:
                    # –û—á–∏—Å—Ç–∫–∞ SQLite –ë–î (–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —É–¥–∞–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ)
                    self._clean_sqlite_database(db_path)
                
                # –†–∞–∑–º–µ—Ä –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏  
                size_after = db_path.stat().st_size / (1024 * 1024) if db_path.exists() else 0
                space_freed += (size_before - size_after)
                
                databases_cleaned += 1
                self.logger.info(f"    ‚úÖ –ë–î –æ—á–∏—â–µ–Ω–∞: {db_path.name}")
                
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ë–î {db_path}: {e}"
                self.logger.error(f"    ‚ùå {error_msg}")
        
        return databases_cleaned, space_freed
    
    def _cleanup_log_files(self) -> Tuple[int, float]:
        """–û—á–∏—Å—Ç–∫–∞ –ª–æ–≥ —Ñ–∞–π–ª–æ–≤"""
        self.logger.info("üìÑ –û—á–∏—Å—Ç–∫–∞ –ª–æ–≥ —Ñ–∞–π–ª–æ–≤...")
        
        files_removed = 0
        space_freed = 0.0
        
        for pattern in self.cleanup_config['log_files']:
            log_files = list(self.project_root.glob(pattern))
            
            for log_file in log_files:
                if not log_file.is_file():
                    continue
                
                try:
                    size_mb = log_file.stat().st_size / (1024 * 1024)
                    
                    if not self.dry_run:
                        log_file.unlink()
                    
                    space_freed += size_mb
                    files_removed += 1
                    self.logger.info(f"  üóëÔ∏è –£–¥–∞–ª–µ–Ω –ª–æ–≥: {log_file.name}")
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {log_file}: {e}"
                    self.logger.error(f"  ‚ùå {error_msg}")
        
        return files_removed, space_freed
    
    def _cleanup_temp_files(self) -> Tuple[int, float]:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        self.logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        files_removed = 0
        space_freed = 0.0
        
        for pattern in self.cleanup_config['temp_files']:
            temp_files = list(self.project_root.glob(pattern))
            
            for temp_file in temp_files:
                try:
                    size_mb = temp_file.stat().st_size / (1024 * 1024)
                    
                    if not self.dry_run:
                        temp_file.unlink()
                    
                    space_freed += size_mb
                    files_removed += 1
                    self.logger.info(f"  üóëÔ∏è –£–¥–∞–ª–µ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {temp_file.name}")
                    
                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {temp_file}: {e}"
                    self.logger.error(f"  ‚ùå {error_msg}")
        
        return files_removed, space_freed
    
    def _cleanup_checkpoint_backups(self) -> Tuple[int, int, float]:
        """–û—á–∏—Å—Ç–∫–∞ checkpoint backup'–æ–≤"""
        self.logger.info("üíæ –û—á–∏—Å—Ç–∫–∞ checkpoint backup'–æ–≤...")
        
        dirs_cleaned = 0
        files_removed = 0
        space_freed = 0.0
        
        for backup_dir_name in self.cleanup_config['checkpoint_backups']:
            backup_path = self.project_root / backup_dir_name
            
            if not backup_path.exists():
                continue
            
            try:
                size_before = self._get_directory_size(backup_path)
                file_count = len(list(backup_path.rglob('*')))
                
                if not self.dry_run:
                    shutil.rmtree(backup_path)
                    backup_path.mkdir(exist_ok=True)
                
                space_freed += size_before / (1024 * 1024)  # MB
                files_removed += file_count
                dirs_cleaned += 1
                
                self.logger.info(f"  üóëÔ∏è –û—á–∏—â–µ–Ω backup: {backup_dir_name}")
                
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ backup {backup_path}: {e}"
                self.logger.error(f"  ‚ùå {error_msg}")
        
        return dirs_cleaned, files_removed, space_freed
    
    def _cleanup_timestamped_directories(self, parent_dir: Path, cutoff_date: datetime) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π —Å timestamp (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ)"""
        removed_count = 0
        
        try:
            subdirs = [d for d in parent_dir.iterdir() if d.is_dir()]
            
            for subdir in subdirs:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
                    mod_time = datetime.fromtimestamp(subdir.stat().st_mtime)
                    
                    if mod_time < cutoff_date:
                        if not self.dry_run:
                            shutil.rmtree(subdir)
                        removed_count += 1
                        self.logger.info(f"    üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {subdir.name}")
                        
                except Exception as e:
                    self.logger.error(f"    ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {subdir}: {e}")
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {parent_dir}: {e}")
        
        return removed_count
    
    def _clean_sqlite_database(self, db_path: Path):
        """–û—á–∏—Å—Ç–∫–∞ SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)"""
        try:
            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
            for table in tables:
                table_name = table[0]
                if table_name != 'sqlite_sequence':  # –°–∏—Å—Ç–µ–º–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
                    cursor.execute(f"DELETE FROM {table_name}")
            
            # –°–±—Ä–æ—Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            cursor.execute("DELETE FROM sqlite_sequence")
            
            conn.commit()
            conn.close()
            
            # –°–∂–∞—Ç–∏–µ –ë–î
            conn = sqlite3.connect(str(db_path))
            conn.execute("VACUUM")
            conn.close()
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ SQLite –ë–î {db_path}: {e}")
    
    def _get_directory_size(self, dir_path: Path) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤ –±–∞–π—Ç–∞—Ö"""
        try:
            return sum(f.stat().st_size for f in dir_path.rglob('*') if f.is_file())
        except:
            return 0
    
    def _print_cleanup_report(self, stats: CleanupStats):
        """–ü–µ—á–∞—Ç—å –æ—Ç—á–µ—Ç–∞ –æ–± –æ—á–∏—Å—Ç–∫–µ"""
        self.logger.info("=" * 60)
        self.logger.info("üìä –û–¢–ß–ï–¢ –û–ë –û–ß–ò–°–¢–ö–ï –õ–û–ì–û–í –ò –î–ê–®–ë–û–†–î–û–í")
        self.logger.info("=" * 60)
        self.logger.info(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –æ—á–∏—â–µ–Ω–æ: {stats.directories_cleaned}")
        self.logger.info(f"üìÑ –§–∞–π–ª–æ–≤ —É–¥–∞–ª–µ–Ω–æ: {stats.files_removed}")
        self.logger.info(f"üóÑÔ∏è –ë–î –æ—á–∏—â–µ–Ω–æ: {stats.databases_cleaned}")
        self.logger.info(f"üíæ –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ –º–µ—Å—Ç–∞: {stats.space_freed_mb:.2f} MB")
        self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—á–∏—Å—Ç–∫–∏: {stats.cleanup_time_seconds:.2f} —Å–µ–∫")
        
        if stats.errors:
            self.logger.info(f"‚ùå –û—à–∏–±–æ–∫: {len(stats.errors)}")
            for error in stats.errors:
                self.logger.error(f"  ‚Ä¢ {error}")
        else:
            self.logger.info("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –±–µ–∑ –æ—à–∏–±–æ–∫")
        
        self.logger.info("=" * 60)
    
    def _save_cleanup_report(self, stats: CleanupStats):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—á–∏—Å—Ç–∫–µ"""
        try:
            report_file = self.project_root / f"cleanup_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'stats': asdict(stats),
                'config': self.cleanup_config,
                'dry_run': self.dry_run
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")

def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Universal Log Cleanup Manager')
    parser.add_argument('--project-root', default='.', help='–ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø—Ä–æ–µ–∫—Ç–∞')
    parser.add_argument('--keep-days', type=int, default=7, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–æ–≤')
    parser.add_argument('--include-checkpoints', action='store_true', help='–í–∫–ª—é—á–∏—Ç—å –æ—á–∏—Å—Ç–∫—É checkpoint backup')
    parser.add_argument('--dry-run', action='store_true', help='–†–µ–∂–∏–º —Å–∏–º—É–ª—è—Ü–∏–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –æ—á–∏—Å—Ç–∫–∏
    cleanup_manager = LogCleanupManager(
        project_root=args.project_root,
        keep_last_days=args.keep_days,
        dry_run=args.dry_run
    )
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—á–∏—Å—Ç–∫–∏
    stats = cleanup_manager.cleanup_all(include_checkpoints=args.include_checkpoints)
    
    # –í–æ–∑–≤—Ä–∞—Ç –∫–æ–¥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    return 0 if not stats.errors else 1

if __name__ == "__main__":
    exit(main()) 