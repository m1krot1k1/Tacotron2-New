# TODO по проекту Tacotron2-New \n\n## Фаза 1 – Критические исправления (высший приоритет)\n- [x] Увеличить `guide_loss_initial_weight` до **20.0**\n- [x] Уменьшить `guide_loss_decay_start` до **1000**\n- [x] Установить `p_attention_dropout` = **0.001**\n- [x] Снизить `learning_rate` до **5e-6**\n- [x] Уменьшить `grad_clip_thresh` до **0.3**\n- [x] Обновить пороги `min_attention_alignment`, `min_gate_accuracy`, `max_validation_loss` в `smart_tuner/config.yaml`\n- [x] Сократить `report_interval` в `debug_reporter.py` до **250**\n- [x] Реализовать детекцию **NaN** каждые 10 шагов и автоматический перезапуск\n- [x] Внедрить динамическое масштабирование loss (адаптивный масштаб при FP32)\n- [x] Добавить композитную loss функцию с новыми весами\n- [x] Мониторинг нормы градиента с авто-коррекцией LR\n- [x] Экспоненциальное сглаживание градиентов\n\n## Фаза 2 – Архитектурные улучшения (высокий приоритет)\n- [x] Внедрить Double Decoder Consistency (DDC)\n- [x] Реализовать curriculum learning и scheduled sampling\n- [x] Добавить Location-Relative Attention\n- [x] Модернизировать поиск гиперпараметров (расширить диапазоны, BO)\n\n## Фаза 3 – Продвинутые техники (средний/низкий приоритет)\n- [ ] Интегрировать Non-Attentive Tacotron\n- [ ] Teacher-Student training\n- [ ] Автоматическая оптимизация датасета (TTSOps, очистка, аугментация)\n- [ ] Современные архитектурные улучшения (Parallel Tacotron, Transformer-based и др.)\n\n> Файл будет обновляться по мере выполнения задач. Пожалуйста, отмечайте пункты галочками при завершении изменений. 