"""
üéØ ADAPTIVE LOSS SYSTEM - –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö loss —Ñ—É–Ω–∫—Ü–∏–π
========================================================================

–ó–∞–º–µ–Ω–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ AdaptiveLossController –Ω–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É:

1. üßÆ Dynamic Tversky Loss - –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è loss –¥–ª—è unbalanced data
2. üìä Intelligent Weight Manager - —É–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Å–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞  
3. üîÑ Context-Based Loss Scaling - –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —Ñ–∞–∑–∞–º –æ–±—É—á–µ–Ω–∏—è
4. üìà Phase-Aware Loss Optimization - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ —Ñ–∞–∑–∞–º

–í–µ—Ä—Å–∏—è: 1.0.0
–ê–≤—Ç–æ—Ä: Enhanced Tacotron2 AI System
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
from dataclasses import dataclass
from collections import deque


class LossPhase(Enum):
    """–§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ loss —Ñ—É–Ω–∫—Ü–∏–π"""
    PRE_ALIGNMENT = "pre_alignment"        # –ù–∞—á–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞ - —Ñ–æ–∫—É—Å –Ω–∞ attention
    ALIGNMENT_LEARNING = "alignment"       # –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è alignment
    REFINEMENT = "refinement"              # –§–∞–∑–∞ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
    CONVERGENCE = "convergence"            # –§–∞–∑–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏


@dataclass
class LossContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ loss —Ñ—É–Ω–∫—Ü–∏–π"""
    phase: LossPhase
    global_step: int
    attention_quality: float               # –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention
    gate_accuracy: float                   # –¢–æ—á–Ω–æ—Å—Ç—å gate –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    mel_consistency: float                 # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å mel spectrogram
    gradient_norm: float                   # –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    loss_stability: float                  # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å loss (std)
    learning_rate: float                   # –¢–µ–∫—É—â–∏–π learning rate


class DynamicTverskyLoss(nn.Module):
    """
    üßÆ Dynamic Tversky Loss - –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è loss —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è unbalanced data
    
    –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö Focal Loss –∏ Tversky Loss (2017-2024):
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è alpha/beta –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
    - –§–æ–∫—É—Å –Ω–∞ —Ç—Ä—É–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö (hard negative mining)
    """
    
    def __init__(self, 
                 initial_alpha: float = 0.7,
                 initial_beta: float = 0.3,
                 adapt_rate: float = 0.01,
                 min_alpha: float = 0.1,
                 max_alpha: float = 0.9):
        super().__init__()
        
        self.alpha = initial_alpha                    # –í–µ—Å False Positives
        self.beta = initial_beta                      # –í–µ—Å False Negatives  
        self.adapt_rate = adapt_rate                  # –°–∫–æ—Ä–æ—Å—Ç—å –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.min_alpha = min_alpha
        self.max_alpha = max_alpha
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.accuracy_history = deque(maxlen=100)
        self.adaptation_history = []
        
        print(f"üßÆ DynamicTverskyLoss –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: Œ±={self.alpha:.3f}, Œ≤={self.beta:.3f}")
    
    def forward(self, 
                predictions: torch.Tensor, 
                targets: torch.Tensor,
                context: Optional[LossContext] = None) -> torch.Tensor:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Dynamic Tversky Loss —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        
        Args:
            predictions: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ [B, ...]
            targets: –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è [B, ...]
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
            
        Returns:
            torch.Tensor: –í—ã—á–∏—Å–ª–µ–Ω–Ω–∞—è loss
        """
        # –ü—Ä–∏–º–µ–Ω—è–µ–º sigmoid –¥–ª—è gate predictions
        if predictions.dim() == targets.dim():
            probs = torch.sigmoid(predictions)
        else:
            probs = predictions
            
        # Clamp –¥–ª—è —á–∏—Å–ª–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        probs = torch.clamp(probs, min=1e-7, max=1.0 - 1e-7)
        targets = torch.clamp(targets, min=0.0, max=1.0)
        
        # True Positives, False Positives, False Negatives
        tp = (probs * targets).sum()
        fp = (probs * (1 - targets)).sum()
        fn = ((1 - probs) * targets).sum()
        
        # Tversky Index
        tversky_index = tp / (tp + self.alpha * fp + self.beta * fn + 1e-7)
        
        # Tversky Loss (1 - Tversky Index)
        loss = 1 - tversky_index
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if context is not None:
            self._adapt_parameters(context)
        
        return loss
    
    def _adapt_parameters(self, context: LossContext):
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è Œ± –∏ Œ≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ gate accuracy
        current_accuracy = context.gate_accuracy
        self.accuracy_history.append(current_accuracy)
        
        if len(self.accuracy_history) >= 10:
            recent_accuracy = np.mean(list(self.accuracy_history)[-10:])
            
            # –ï—Å–ª–∏ accuracy –Ω–∏–∑–∫–∞—è - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ FP (—É–≤–µ–ª–∏—á–∏–≤–∞–µ–º alpha)
            if recent_accuracy < 0.7:
                target_alpha = min(self.max_alpha, self.alpha + self.adapt_rate)
            # –ï—Å–ª–∏ accuracy –≤—ã—Å–æ–∫–∞—è - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ FN (—É–º–µ–Ω—å—à–∞–µ–º alpha)
            elif recent_accuracy > 0.85:
                target_alpha = max(self.min_alpha, self.alpha - self.adapt_rate)
            else:
                target_alpha = self.alpha
            
            # –ü–ª–∞–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
            self.alpha = 0.9 * self.alpha + 0.1 * target_alpha
            self.beta = 1.0 - self.alpha
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if abs(target_alpha - self.alpha) > 0.01:
                self.adaptation_history.append({
                    'step': context.global_step,
                    'alpha': self.alpha,
                    'beta': self.beta,
                    'accuracy': recent_accuracy,
                    'phase': context.phase.value
                })


class IntelligentWeightManager(nn.Module):
    """
    üìä Intelligent Weight Manager - —É–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Å–∞–º–∏ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    
    –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –§–∞–∑–æ–≤–æ-–∑–∞–≤–∏—Å–∏–º–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤
    - –†–µ–∞–∫—Ü–∏—è –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è  
    - –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏—Ö —Å–±–æ–µ–≤
    - –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    """
    
    def __init__(self, hparams):
        super().__init__()
        
        # –ë–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ –∏–∑ hparams
        self.base_weights = {
            'mel': getattr(hparams, 'mel_loss_weight', 1.0),
            'gate': getattr(hparams, 'gate_loss_weight', 1.0),
            'guided_attention': getattr(hparams, 'guide_loss_weight', 2.0),
            'spectral': getattr(hparams, 'spectral_loss_weight', 0.3),
            'perceptual': getattr(hparams, 'perceptual_loss_weight', 0.2),
            'style': getattr(hparams, 'style_loss_weight', 0.1),
            'monotonic': getattr(hparams, 'monotonic_loss_weight', 0.1)
        }
        
        # –¢–µ–∫—É—â–∏–µ –≤–µ—Å–∞ (–Ω–∞—á–∏–Ω–∞–µ–º —Å –±–∞–∑–æ–≤—ã—Ö)
        self.current_weights = self.base_weights.copy()
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–æ–≤ –∫–∞–∫ –≤ AutoFixManager)
        self.weight_limits = {
            'mel': (0.1, 3.0),
            'gate': (0.1, 2.5),
            'guided_attention': (0.1, 15.0),        # –í–º–µ—Å—Ç–æ 200 –≤ AutoFixManager!
            'spectral': (0.0, 1.0),
            'perceptual': (0.0, 1.0),
            'style': (0.0, 0.5),
            'monotonic': (0.0, 0.5)
        }
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.adaptation_rate = getattr(hparams, 'weight_adaptation_rate', 0.02)
        self.stability_threshold = getattr(hparams, 'loss_stability_threshold', 2.0)
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.weight_history = []
        self.performance_history = deque(maxlen=50)
        
        print(f"üìä IntelligentWeightManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –≤–µ—Å–∞–º–∏: {self.current_weights}")
    
    def get_adaptive_weights(self, context: LossContext) -> Dict[str, float]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            Dict[str, float]: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ loss
        """
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Ç–µ–∫—É—â–∏—Ö –≤–µ—Å–æ–≤
        adapted_weights = self.current_weights.copy()
        
        # 1. –§–∞–∑–æ–≤–æ-–∑–∞–≤–∏—Å–∏–º–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
        adapted_weights = self._adapt_by_phase(adapted_weights, context)
        
        # 2. –ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏
        adapted_weights = self._adapt_by_quality(adapted_weights, context)
        
        # 3. –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö
        adapted_weights = self._stabilize_weights(adapted_weights, context)
        
        # 4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        adapted_weights = self._apply_safety_limits(adapted_weights)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞ –ø–ª–∞–≤–Ω–æ
        self._smooth_weight_update(adapted_weights, context)
        
        return self.current_weights
    
    def _adapt_by_phase(self, weights: Dict[str, float], context: LossContext) -> Dict[str, float]:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è"""
        
        if context.phase == LossPhase.PRE_ALIGNMENT:
            # –§–æ–∫—É—Å –Ω–∞ guided attention –∏ gate –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ alignment
            weights['guided_attention'] *= 1.5
            weights['gate'] *= 1.2
            weights['mel'] *= 0.8
            
        elif context.phase == LossPhase.ALIGNMENT_LEARNING:
            # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É alignment –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º
            weights['guided_attention'] *= 1.2  
            weights['mel'] *= 1.0
            weights['spectral'] *= 1.1
            
        elif context.phase == LossPhase.REFINEMENT:
            # –§–æ–∫—É—Å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ mel –∏ perceptual
            weights['mel'] *= 1.3
            weights['spectral'] *= 1.4
            weights['perceptual'] *= 1.5
            weights['guided_attention'] *= 0.8
            
        elif context.phase == LossPhase.CONVERGENCE:
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π guided attention
            weights['mel'] *= 1.5
            weights['perceptual'] *= 2.0
            weights['style'] *= 1.5
            weights['guided_attention'] *= 0.4
            
        return weights
    
    def _adapt_by_quality(self, weights: Dict[str, float], context: LossContext) -> Dict[str, float]:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        
        # –†–µ–∞–∫—Ü–∏—è –Ω–∞ –ø–ª–æ—Ö–æ–µ attention alignment
        if context.attention_quality < 0.3:
            weights['guided_attention'] *= 1.8
            weights['monotonic'] *= 1.5
            
        # –†–µ–∞–∫—Ü–∏—è –Ω–∞ –ø–ª–æ—Ö—É—é gate accuracy
        if context.gate_accuracy < 0.6:
            weights['gate'] *= 1.4
            
        # –†–µ–∞–∫—Ü–∏—è –Ω–∞ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å gradients
        if context.gradient_norm > 5.0:
            # –°–Ω–∏–∂–∞–µ–º –≤—Å–µ –≤–µ—Å–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
            for key in weights:
                weights[key] *= 0.9
                
        return weights
    
    def _stabilize_weights(self, weights: Dict[str, float], context: LossContext) -> Dict[str, float]:
        """–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º"""
        
        # –ï—Å–ª–∏ loss –Ω–µ—Å—Ç–∞–±–∏–ª–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –±–∞–∑–æ–≤—ã–º –≤–µ—Å–∞–º
        if context.loss_stability > self.stability_threshold:
            stabilization_factor = 0.7  # –ü–ª–∞–≤–Ω—ã–π –≤–æ–∑–≤—Ä–∞—Ç
            for key in weights:
                weights[key] = (
                    stabilization_factor * self.base_weights[key] + 
                    (1 - stabilization_factor) * weights[key]
                )
                
        return weights
    
    def _apply_safety_limits(self, weights: Dict[str, float]) -> Dict[str, float]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        
        for key, (min_val, max_val) in self.weight_limits.items():
            if key in weights:
                weights[key] = torch.clamp(torch.tensor(weights[key]), min_val, max_val).item()
                
        return weights
    
    def _smooth_weight_update(self, target_weights: Dict[str, float], context: LossContext):
        """–ü–ª–∞–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –≤–µ—Å–æ–≤"""
        
        smoothing_factor = 0.95  # –°–∏–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        for key in self.current_weights:
            if key in target_weights:
                self.current_weights[key] = (
                    smoothing_factor * self.current_weights[key] + 
                    (1 - smoothing_factor) * target_weights[key]
                )
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        if context.global_step % 100 == 0:
            self.weight_history.append({
                'step': context.global_step,
                'weights': self.current_weights.copy(),
                'phase': context.phase.value,
                'attention_quality': context.attention_quality
            })


class ContextBasedLossScaler(nn.Module):
    """
    üîÑ Context-Based Loss Scaling - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ loss
    
    –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤  
    - –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ gradient explosion/vanishing
    - –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
    - –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ —Ñ–∞–∑–∞–º –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self, 
                 initial_scale: float = 1.0,
                 target_grad_norm: float = 2.0,
                 adaptation_rate: float = 0.1):
        super().__init__()
        
        self.current_scale = initial_scale
        self.target_grad_norm = target_grad_norm
        self.adaptation_rate = adaptation_rate
        
        # –ò—Å—Ç–æ—Ä–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.gradient_history = deque(maxlen=20)
        self.scale_history = []
        
        print(f"üîÑ ContextBasedLossScaler –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: scale={self.current_scale}")
    
    def get_loss_scale(self, context: LossContext) -> float:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞ loss
        
        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            float: –ú–∞—Å—à—Ç–∞–± –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∫ loss
        """
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.gradient_history.append(context.gradient_norm)
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –º–∞—Å—à—Ç–∞–±–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ—Ä–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if len(self.gradient_history) >= 5:
            recent_grad_norm = np.mean(list(self.gradient_history)[-5:])
            
            # –ï—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ - —É–º–µ–Ω—å—à–∞–µ–º scale
            if recent_grad_norm > self.target_grad_norm * 1.5:
                target_scale = self.current_scale * 0.8
            # –ï—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º scale  
            elif recent_grad_norm < self.target_grad_norm * 0.5:
                target_scale = self.current_scale * 1.2
            else:
                target_scale = self.current_scale
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –º–∞—Å—à—Ç–∞–±–∞
            target_scale = np.clip(target_scale, 0.1, 10.0)
            
            # –ü–ª–∞–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
            self.current_scale = (
                (1 - self.adaptation_rate) * self.current_scale + 
                self.adaptation_rate * target_scale
            )
        
        # –§–∞–∑–æ–≤–æ-–∑–∞–≤–∏—Å–∏–º—ã–µ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏
        phase_multiplier = self._get_phase_multiplier(context.phase)
        final_scale = self.current_scale * phase_multiplier
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        if context.global_step % 200 == 0:
            self.scale_history.append({
                'step': context.global_step,
                'scale': final_scale,
                'grad_norm': context.gradient_norm,
                'phase': context.phase.value
            })
        
        return final_scale
    
    def _get_phase_multiplier(self, phase: LossPhase) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–Ω–æ–∂–∏—Ç–µ–ª—è –º–∞—Å—à—Ç–∞–±–∞ –¥–ª—è —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è"""
        
        phase_multipliers = {
            LossPhase.PRE_ALIGNMENT: 1.2,        # –ë–æ–ª—å—à–µ scale –≤ –Ω–∞—á–∞–ª–µ
            LossPhase.ALIGNMENT_LEARNING: 1.0,   # –ë–∞–∑–æ–≤—ã–π scale
            LossPhase.REFINEMENT: 0.9,           # –ú–µ–Ω—å—à–µ scale –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            LossPhase.CONVERGENCE: 0.8           # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π scale –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        }
        
        return phase_multipliers.get(phase, 1.0)


class PhaseAwareLossOptimizer(nn.Module):
    """
    üìà Phase-Aware Loss Optimization - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ loss –ø–æ —Ñ–∞–∑–∞–º
    
    –ö–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã:
    - Dynamic Tversky Loss
    - Intelligent Weight Manager  
    - Context-Based Loss Scaler
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
    """
    
    def __init__(self, hparams):
        super().__init__()
        
        self.hparams = hparams
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
        self.tversky_loss = DynamicTverskyLoss()
        self.weight_manager = IntelligentWeightManager(hparams)
        self.loss_scaler = ContextBasedLossScaler()
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        self.optimization_history = []
        self.performance_metrics = deque(maxlen=100)
        
        print("üìà PhaseAwareLossOptimizer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–∫—Ç–∏–≤–Ω–∞!")
    
    def optimize_loss_computation(self, 
                                  loss_components: Dict[str, torch.Tensor],
                                  context: LossContext) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ loss —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
        
        Args:
            loss_components: –°–ª–æ–≤–∞—Ä—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ loss {'mel': tensor, 'gate': tensor, ...}
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            Tuple[torch.Tensor, Dict[str, Any]]: (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è loss, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
        """
        # 1. –ü–æ–ª—É—á–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞
        adaptive_weights = self.weight_manager.get_adaptive_weights(context)
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–∞—Å—à—Ç–∞–±
        loss_scale = self.loss_scaler.get_loss_scale(context)
        
        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º Dynamic Tversky –∫ gate loss –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if 'gate' in loss_components and 'gate_predictions' in loss_components:
            tversky_gate_loss = self.tversky_loss(
                loss_components['gate_predictions'],
                loss_components['gate_targets'], 
                context
            )
            loss_components['gate'] = tversky_gate_loss
        
        # 4. –í—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é loss
        total_loss = torch.tensor(0.0, device=next(iter(loss_components.values())).device)
        
        component_contributions = {}
        for component, loss_value in loss_components.items():
            if component in adaptive_weights and loss_value is not None:
                weighted_loss = adaptive_weights[component] * loss_value
                total_loss += weighted_loss
                component_contributions[component] = weighted_loss.item()
        
        # 5. –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        scaled_loss = total_loss * loss_scale
        
        # 6. –°–æ–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        diagnostics = {
            'adaptive_weights': adaptive_weights,
            'loss_scale': loss_scale,
            'component_contributions': component_contributions,
            'total_loss_unscaled': total_loss.item(),
            'total_loss_scaled': scaled_loss.item(),
            'tversky_params': {
                'alpha': self.tversky_loss.alpha,
                'beta': self.tversky_loss.beta
            },
            'optimization_phase': context.phase.value
        }
        
        # 7. –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._update_performance_metrics(context, scaled_loss.item(), diagnostics)
        
        return scaled_loss, diagnostics
    
    def _update_performance_metrics(self, 
                                    context: LossContext, 
                                    final_loss: float, 
                                    diagnostics: Dict[str, Any]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        
        metric = {
            'step': context.global_step,
            'phase': context.phase.value,
            'final_loss': final_loss,
            'attention_quality': context.attention_quality,
            'gate_accuracy': context.gate_accuracy,
            'gradient_norm': context.gradient_norm,
            'loss_stability': context.loss_stability,
            'adaptive_weights': diagnostics['adaptive_weights'].copy(),
            'loss_scale': diagnostics['loss_scale']
        }
        
        self.performance_metrics.append(metric)
        
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if context.global_step % 500 == 0:
            self.optimization_history.append({
                'step': context.global_step,
                'recent_performance': list(self.performance_metrics)[-10:],
                'summary': self._compute_performance_summary()
            })
    
    def _compute_performance_summary(self) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã"""
        
        if not self.performance_metrics:
            return {}
        
        recent_metrics = list(self.performance_metrics)[-20:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —à–∞–≥–æ–≤
        
        return {
            'avg_loss': np.mean([m['final_loss'] for m in recent_metrics]),
            'loss_std': np.std([m['final_loss'] for m in recent_metrics]),
            'avg_attention_quality': np.mean([m['attention_quality'] for m in recent_metrics]),
            'avg_gate_accuracy': np.mean([m['gate_accuracy'] for m in recent_metrics]),
            'avg_gradient_norm': np.mean([m['gradient_norm'] for m in recent_metrics]),
            'system_stability': 1.0 / (1.0 + np.std([m['final_loss'] for m in recent_metrics]))
        }
    
    def get_system_diagnostics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
        
        return {
            'tversky_loss': {
                'current_alpha': self.tversky_loss.alpha,
                'current_beta': self.tversky_loss.beta,
                'adaptation_history': self.tversky_loss.adaptation_history[-10:],
                'accuracy_trend': list(self.tversky_loss.accuracy_history)[-10:]
            },
            'weight_manager': {
                'current_weights': self.weight_manager.current_weights,
                'base_weights': self.weight_manager.base_weights,
                'weight_history': self.weight_manager.weight_history[-5:],
                'safety_limits': self.weight_manager.weight_limits
            },
            'loss_scaler': {
                'current_scale': self.loss_scaler.current_scale,
                'target_grad_norm': self.loss_scaler.target_grad_norm,
                'gradient_history': list(self.loss_scaler.gradient_history)[-10:],
                'scale_history': self.loss_scaler.scale_history[-5:]
            },
            'performance_summary': self._compute_performance_summary(),
            'optimization_history_length': len(self.optimization_history)
        }


def create_adaptive_loss_system(hparams) -> PhaseAwareLossOptimizer:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã loss —Ñ—É–Ω–∫—Ü–∏–π
    
    Args:
        hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        
    Returns:
        PhaseAwareLossOptimizer: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö loss —Ñ—É–Ω–∫—Ü–∏–π
    """
    print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ Enhanced Adaptive Loss System...")
    
    system = PhaseAwareLossOptimizer(hparams)
    
    print("‚úÖ Enhanced Adaptive Loss System —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
    print("üîß –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: Dynamic Tversky Loss, Intelligent Weight Manager, Context-Based Loss Scaler")
    
    return system


# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π

def convert_training_phase_to_loss_phase(training_phase: str) -> LossPhase:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–∑—ã –∏–∑ Context-Aware Manager –≤ LossPhase"""
    
    phase_mapping = {
        'PRE_ALIGNMENT': LossPhase.PRE_ALIGNMENT,
        'ALIGNMENT_LEARNING': LossPhase.ALIGNMENT_LEARNING,
        'REFINEMENT': LossPhase.REFINEMENT,
        'CONVERGENCE': LossPhase.CONVERGENCE
    }
    
    return phase_mapping.get(training_phase, LossPhase.ALIGNMENT_LEARNING)


def create_loss_context_from_metrics(training_metrics: Dict[str, Any], 
                                     current_phase: str,
                                     global_step: int) -> LossContext:
    """–°–æ–∑–¥–∞–Ω–∏–µ LossContext –∏–∑ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
    
    return LossContext(
        phase=convert_training_phase_to_loss_phase(current_phase),
        global_step=global_step,
        attention_quality=training_metrics.get('attention_quality', 0.5),
        gate_accuracy=training_metrics.get('gate_accuracy', 0.5),
        mel_consistency=training_metrics.get('mel_consistency', 0.5),
        gradient_norm=training_metrics.get('gradient_norm', 1.0),
        loss_stability=training_metrics.get('loss_stability', 1.0),
        learning_rate=training_metrics.get('learning_rate', 1e-3)
    )


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Enhanced Adaptive Loss System...")
    
    class MockHParams:
        mel_loss_weight = 1.0
        gate_loss_weight = 1.0
        guide_loss_weight = 2.0
        spectral_loss_weight = 0.3
        perceptual_loss_weight = 0.2
        style_loss_weight = 0.1
        monotonic_loss_weight = 0.1
    
    hparams = MockHParams()
    system = create_adaptive_loss_system(hparams)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context = LossContext(
        phase=LossPhase.ALIGNMENT_LEARNING,
        global_step=1000,
        attention_quality=0.4,
        gate_accuracy=0.7,
        mel_consistency=0.6,
        gradient_norm=3.2,
        loss_stability=1.8,
        learning_rate=1e-3
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    loss_components = {
        'mel': torch.tensor(2.5),
        'gate': torch.tensor(0.8),
        'guided_attention': torch.tensor(1.2),
        'spectral': torch.tensor(0.4)
    }
    
    # –¢–µ—Å—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    optimized_loss, diagnostics = system.optimize_loss_computation(loss_components, context)
    
    print(f"‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    print(f"   –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è loss: {optimized_loss.item():.4f}")
    print(f"   –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞: {diagnostics['adaptive_weights']}")
    print(f"   Loss scale: {diagnostics['loss_scale']:.3f}") 