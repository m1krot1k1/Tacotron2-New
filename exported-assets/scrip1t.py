# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –ª–æ–≥–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
import re
import pandas as pd
from datetime import datetime

# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –ª–æ–≥–∞–º–∏
with open("paste.txt", "r", encoding="utf-8") as f:
    log_content = f.read()

# –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ª–æ–≥–æ–≤
loss_pattern = r"–®–∞–≥ (\d+): Loss: ([\d.]+), Grad: ([\d.]+), Attn: ([\d.]+), Gate: ([\d.]+)"
guided_attention_pattern = r"guided attention weight: ([\d.]+) ‚Üí ([\d.]+)"
lr_pattern = r"LR –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ ([\d.e-]+)"
gradient_pattern = r"–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ–±—Ä–µ–∑–∞–Ω—ã: ([\d.]+) ‚Üí ([\d.]+)"

# –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —à–∞–≥–∞–º
step_data = []
for match in re.finditer(loss_pattern, log_content):
    step, loss, grad, attn, gate = match.groups()
    step_data.append({
        'step': int(step),
        'loss': float(loss),
        'grad_norm': float(grad), 
        'attention': float(attn),
        'gate_accuracy': float(gate)
    })

# –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
if step_data:
    df = pd.DataFrame(step_data)
    print("=== –ê–ù–ê–õ–ò–ó –ú–ï–¢–†–ò–ö –û–ë–£–ß–ï–ù–ò–Ø ===")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö —à–∞–≥–æ–≤: {len(df)}")
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω —à–∞–≥–æ–≤: {df['step'].min()} - {df['step'].max()}")
    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–ï–¢–†–ò–ö:")
    print(df.describe())
    
    print("\nüîç –ê–ù–ê–õ–ò–ó –ü–†–û–ë–õ–ï–ú:")
    
    # –ê–Ω–∞–ª–∏–∑ loss
    initial_loss = df['loss'].iloc[0]
    final_loss = df['loss'].iloc[-1]
    print(f"‚Ä¢ Loss: {initial_loss:.2f} ‚Üí {final_loss:.2f} (–∏–∑–º–µ–Ω–µ–Ω–∏–µ: {((final_loss-initial_loss)/initial_loss*100):+.1f}%)")
    
    # –ê–Ω–∞–ª–∏–∑ attention
    avg_attention = df['attention'].mean()
    print(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è attention diagonality: {avg_attention:.3f} (–Ω–æ—Ä–º–∞: >0.7)")
    if avg_attention < 0.1:
        print("  ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ò –ù–ò–ó–ö–ê–Ø attention diagonality")
    elif avg_attention < 0.3:
        print("  ‚ö†Ô∏è –ù–∏–∑–∫–∞—è attention diagonality")
        
    # –ê–Ω–∞–ª–∏–∑ gate accuracy
    avg_gate = df['gate_accuracy'].mean()
    print(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è gate accuracy: {avg_gate:.3f} (–Ω–æ—Ä–º–∞: >0.8)")
    if avg_gate < 0.5:
        print("  ‚ö†Ô∏è –ù–∏–∑–∫–∞—è gate accuracy")
        
    # –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    avg_grad = df['grad_norm'].mean()
    print(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {avg_grad:.2f}")
    
    # –ü–æ–¥—Å—á–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    guided_attention_changes = len(re.findall(guided_attention_pattern, log_content))
    lr_changes = len(re.findall(lr_pattern, log_content))
    gradient_clips = len(re.findall(gradient_pattern, log_content))
    
    print(f"\nüîß –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:")
    print(f"‚Ä¢ –ò–∑–º–µ–Ω–µ–Ω–∏—è guided attention weight: {guided_attention_changes}")
    print(f"‚Ä¢ –ò–∑–º–µ–Ω–µ–Ω–∏—è learning rate: {lr_changes}")
    print(f"‚Ä¢ –û–±—Ä–µ–∑–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {gradient_clips}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
    loss_std = df['loss'].std()
    print(f"\nüìà –°–¢–ê–ë–ò–õ–¨–ù–û–°–¢–¨:")
    print(f"‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ loss: {loss_std:.2f}")
    if loss_std > 5:
        print("  ‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å loss")
        
else:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–≥–æ–≤")