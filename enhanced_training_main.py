#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Tacotron2 Training with Smart Tuner Integration
–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è TTS —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ —É–ª—É—á—à–µ–Ω–∏—è –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 2024-2025:
- Very Attentive Tacotron (Google, 2025)
- MonoAlign robust alignment (INTERSPEECH 2024)
- XTTS Advanced training practices
- DLPO reinforcement learning
- Style-BERT-VITS2 optimizations
- Smart Tuner intelligent automation
"""

import os
import sys
import time
import torch
import torch.nn as nn
import numpy as np
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import yaml

# –ò–º–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from model import Tacotron2
from loss_function import Tacotron2Loss
from hparams import create_hparams
from audio_quality_enhancer import AudioQualityEnhancer

# –ò–º–ø–æ—Ä—Ç Smart Tuner –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    from smart_tuner.smart_tuner_integration import SmartTunerIntegration
    from smart_tuner.telegram_monitor import TelegramMonitor
    SMART_TUNER_AVAILABLE = True
except ImportError:
    SMART_TUNER_AVAILABLE = False
    logging.warning("Smart Tuner –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")

class EnhancedTacotronTrainer:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–Ω–µ—Ä Tacotron2 —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Smart Tuner –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    2. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    3. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–ø–æ—Ö–∞–º–∏
    4. –§–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    5. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 2025
    """
    
    def __init__(self, hparams, dataset_info: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è enhanced —Ç—Ä–µ–Ω–µ—Ä–∞.
        
        Args:
            hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            dataset_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        self.hparams = hparams
        self.dataset_info = dataset_info or {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = self._setup_logger()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Smart Tuner
        self.smart_tuner = None
        if SMART_TUNER_AVAILABLE:
            try:
                self.smart_tuner = SmartTunerIntegration()
                self.logger.info("üöÄ Smart Tuner —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Smart Tuner: {e}")
        
        # üì± –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram Monitor
        self.telegram_monitor = None
        if SMART_TUNER_AVAILABLE:
            try:
                self.telegram_monitor = TelegramMonitor()
                self.logger.info("üì± Telegram Monitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Telegram Monitor: {e}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.audio_enhancer = AudioQualityEnhancer()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.global_step = 0
        self.best_validation_loss = float('inf')
        self.training_metrics_history = []
        
        # –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
        self.training_phases = {
            'pre_alignment': {'max_epoch': 500, 'focus': 'attention_learning'},
            'alignment_learning': {'max_epoch': 2000, 'focus': 'attention_stabilization'},
            'quality_optimization': {'max_epoch': 3000, 'focus': 'quality_improvement'},
            'fine_tuning': {'max_epoch': 3500, 'focus': 'final_polishing'}
        }
        
        self.logger.info("‚úÖ Enhanced Tacotron Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–µ–Ω–µ—Ä–∞."""
        logger = logging.getLogger('EnhancedTacotronTrainer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π handler
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                '%(asctime)s - [Enhanced Tacotron] - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            
            # –§–∞–π–ª–æ–≤—ã–π handler
            file_handler = logging.FileHandler('enhanced_training.log')
            file_handler.setFormatter(console_formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def initialize_training(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        self.logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Smart Tuner
        if self.smart_tuner:
            original_hparams = vars(self.hparams).copy()
            optimized_hparams = self.smart_tuner.on_training_start(
                original_hparams, self.dataset_info
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            for key, value in optimized_hparams.items():
                if hasattr(self.hparams, key):
                    setattr(self.hparams, key, value)
                    
            self.logger.info("‚ú® –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ Smart Tuner")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = Tacotron2(self.hparams).cuda()
        self.logger.info(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {sum(p.numel() for p in self.model.parameters())} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è loss —Ñ—É–Ω–∫—Ü–∏–∏ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏
        self.criterion = Tacotron2Loss(self.hparams)
        self.logger.info("üéØ Enhanced loss function –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.hparams.learning_rate,
            weight_decay=getattr(self.hparams, 'weight_decay', 1e-6)
        )
        self.logger.info("‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, 
            T_0=getattr(self.hparams, 'scheduler_T_0', 1000),
            eta_min=getattr(self.hparams, 'min_learning_rate', 1e-6)
        )
        
        self.logger.info("üöÄ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    def get_current_training_phase(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è."""
        for phase_name, phase_info in self.training_phases.items():
            if self.current_epoch <= phase_info['max_epoch']:
                return phase_name
        return 'fine_tuning'
    
    def adjust_hyperparams_for_phase(self, phase: str):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è."""
        phase_configs = {
            'pre_alignment': {
                'guided_attention_weight': 10.0,
                'learning_rate_multiplier': 1.0,
                'teacher_forcing_ratio': 1.0
            },
            'alignment_learning': {
                'guided_attention_weight': 3.0,
                'learning_rate_multiplier': 0.8,
                'teacher_forcing_ratio': 0.9
            },
            'quality_optimization': {
                'guided_attention_weight': 1.0,
                'learning_rate_multiplier': 0.5,
                'teacher_forcing_ratio': 0.8
            },
            'fine_tuning': {
                'guided_attention_weight': 0.5,
                'learning_rate_multiplier': 0.3,
                'teacher_forcing_ratio': 0.7
            }
        }
        
        if phase in phase_configs:
            config = phase_configs[phase]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ loss —Ñ—É–Ω–∫—Ü–∏–π
            self.criterion.guide_loss_weight = config['guided_attention_weight']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
            base_lr = self.hparams.learning_rate
            new_lr = base_lr * config['learning_rate_multiplier']
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = new_lr
            
            self.logger.info(f"üîÑ –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∞–∑—ã '{phase}': "
                           f"guided_weight={config['guided_attention_weight']}, "
                           f"lr_mult={config['learning_rate_multiplier']}")
    
    def train_step(self, batch):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å enhanced –∫–∞—á–µ—Å—Ç–≤–æ–º."""
        self.model.train()
        self.optimizer.zero_grad()
        
        # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ batch
        text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths = batch
        
        # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
        text_inputs = text_inputs.cuda()
        mel_targets = mel_targets.cuda() 
        gate_targets = gate_targets.cuda()
        
        # Forward pass
        model_outputs = self.model(text_inputs, mel_targets)
        mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model_outputs
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏
        loss, loss_dict = self.criterion(
            model_outputs, 
            (mel_targets, gate_targets),
            attention_weights=alignments,
            gate_outputs=gate_outputs
        )
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(), 
            getattr(self.hparams, 'grad_clip_thresh', 1.0)
        )
        
        self.optimizer.step()
        self.scheduler.step()
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ Smart Tuner
        quality_analysis = {}
        if self.smart_tuner:
            try:
                quality_analysis = self.smart_tuner.on_batch_end(
                    self.current_epoch,
                    self.global_step,
                    loss_dict,
                    (mel_outputs_postnet, gate_outputs, alignments)
                )
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
        
        # üì± Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if self.telegram_monitor:
            try:
                self.telegram_monitor.send_training_update(
                    step=self.global_step,
                    metrics=loss_dict,
                    attention_weights=alignments,
                    gate_outputs=gate_outputs
                )
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
        
        self.global_step += 1
        
        return {
            'loss': loss.item(),
            'loss_breakdown': loss_dict,
            'quality_analysis': quality_analysis
        }
    
    def validate_step(self, val_loader):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞—á–µ—Å—Ç–≤–∞."""
        self.model.eval()
        val_losses = []
        quality_metrics = []
        
        with torch.no_grad():
            for batch in val_loader:
                text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths = batch
                
                # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
                text_inputs = text_inputs.cuda()
                mel_targets = mel_targets.cuda()
                gate_targets = gate_targets.cuda()
                
                # Forward pass
                model_outputs = self.model(text_inputs, mel_targets)
                mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model_outputs
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
                loss, loss_dict = self.criterion(
                    model_outputs,
                    (mel_targets, gate_targets),
                    attention_weights=alignments,
                    gate_outputs=gate_outputs
                )
                
                val_losses.append(loss.item())
                
                # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
                if self.smart_tuner and len(quality_metrics) < 5:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 –±–∞—Ç—á–µ–π
                    try:
                        quality_analysis = self.smart_tuner.on_batch_end(
                            self.current_epoch,
                            self.global_step,
                            loss_dict,
                            (mel_outputs_postnet, gate_outputs, alignments)
                        )
                        quality_metrics.append(quality_analysis.get('quality_score', 0.5))
                    except Exception as e:
                        self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        
        avg_val_loss = np.mean(val_losses)
        avg_quality_score = np.mean(quality_metrics) if quality_metrics else 0.5
        
        return {
            'val_loss': avg_val_loss,
            'quality_score': avg_quality_score
        }
    
    def train_epoch(self, train_loader, val_loader):
        """–¢—Ä–µ–Ω–∏—Ä—É–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∫–∞—á–µ—Å—Ç–≤–∞."""
        epoch_start_time = time.time()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è
        current_phase = self.get_current_training_phase()
        self.adjust_hyperparams_for_phase(current_phase)
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ Smart Tuner –æ –Ω–∞—á–∞–ª–µ —ç–ø–æ—Ö–∏
        current_hyperparams = {
            'learning_rate': self.optimizer.param_groups[0]['lr'],
            'guided_attention_weight': self.criterion.guide_loss_weight,
            'epoch': self.current_epoch,
            'phase': current_phase
        }
        
        if self.smart_tuner:
            try:
                updated_hyperparams = self.smart_tuner.on_epoch_start(
                    self.current_epoch, current_hyperparams
                )
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                if updated_hyperparams != current_hyperparams:
                    self.logger.info("üîß Smart Tuner –æ–±–Ω–æ–≤–∏–ª –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart Tuner –Ω–∞ —Å—Ç–∞—Ä—Ç–µ —ç–ø–æ—Ö–∏: {e}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_losses = []
        quality_issues_count = 0
        
        for batch_idx, batch in enumerate(train_loader):
            step_result = self.train_step(batch)
            train_losses.append(step_result['loss'])
            
            # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
            quality_analysis = step_result.get('quality_analysis', {})
            quality_issues_count += len(quality_analysis.get('quality_issues', []))
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if batch_idx % 100 == 0:
                self.logger.info(
                    f"–≠–ø–æ—Ö–∞ {self.current_epoch}, –±–∞—Ç—á {batch_idx}: "
                    f"loss={step_result['loss']:.4f}, "
                    f"quality_score={quality_analysis.get('quality_score', 0):.3f}"
                )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_result = self.validate_step(val_loader)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏
        epoch_metrics = {
            'train_loss': np.mean(train_losses),
            'val_loss': val_result['val_loss'],
            'quality_score': val_result['quality_score'],
            'epoch_time': time.time() - epoch_start_time,
            'phase': current_phase,
            'quality_issues': quality_issues_count
        }
        
        if self.smart_tuner:
            try:
                decision = self.smart_tuner.on_epoch_end(
                    self.current_epoch, epoch_metrics, current_hyperparams
                )
                
                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ—à–µ–Ω–∏–π Smart Tuner
                if decision and isinstance(decision, dict):
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ—à–µ–Ω–∏–π Smart Tuner
                    if decision.get('early_stop', False):
                        self.logger.info(f"üõë Smart Tuner —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫—É: {decision.get('reason')}")
                        return False  # –°–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è
                    
                    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    if decision.get('hyperparameter_updates'):
                        self.apply_hyperparameter_updates(decision['hyperparameter_updates'])
                        self.logger.info("üîß Smart Tuner –ø—Ä–∏–º–µ–Ω–∏–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                        
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart Tuner –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —ç–ø–æ—Ö–∏: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏
        self.training_metrics_history.append(epoch_metrics)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à—É—é validation loss
        if val_result['val_loss'] < self.best_validation_loss:
            self.best_validation_loss = val_result['val_loss']
            self.logger.info(f"üèÜ –ù–æ–≤—ã–π —Ä–µ–∫–æ—Ä–¥ validation loss: {self.best_validation_loss:.4f}")
        
        # üéµ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞—É–¥–∏–æ –∫–∞–∂–¥—ã–µ 5000 —à–∞–≥–æ–≤
        if self.telegram_monitor and self.global_step % 5000 == 0:
            try:
                self.telegram_monitor.generate_and_send_test_audio(
                    step=self.global_step,
                    model=self.model,
                    hparams=self.hparams,
                    device=self.device
                )
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞—É–¥–∏–æ: {e}")
        
        # üì± –û–±—ã—á–Ω–æ–µ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if self.telegram_monitor and self.global_step % 1000 == 0:
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
                telegram_metrics = {
                    'step': self.global_step,
                    'train_loss': train_loss,
                    'val_loss': getattr(self, 'last_val_loss', 0),
                    'attention_diagonality': attention_diagonality,
                    'gate_accuracy': gate_accuracy,
                    'quality_score': overall_quality,
                    'learning_rate': self.optimizer.param_groups[0]['lr'],
                    'phase': current_phase,
                    'epoch': getattr(self, 'current_epoch', 0)
                }
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ attention
                self.telegram_monitor.send_training_update(
                    step=self.global_step,
                    metrics=telegram_metrics,
                    alignments=alignments
                )
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–æ—Ö–∏
        self.logger.info(
            f"üìä –≠–ø–æ—Ö–∞ {self.current_epoch} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ [{current_phase}]: "
            f"train_loss={epoch_metrics['train_loss']:.4f}, "
            f"val_loss={epoch_metrics['val_loss']:.4f}, "
            f"quality={epoch_metrics['quality_score']:.3f}, "
            f"–ø—Ä–æ–±–ª–µ–º={quality_issues_count}, "
            f"–≤—Ä–µ–º—è={epoch_metrics['epoch_time']:.1f}—Å"
        )
        
        return True  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
    
    def apply_hyperparameter_updates(self, updates: Dict[str, Any]):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Ç Smart Tuner."""
        for param_name, new_value in updates.items():
            if param_name == 'learning_rate':
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = new_value
                self.logger.info(f"üîß –û–±–Ω–æ–≤–ª–µ–Ω learning_rate: {new_value}")
                
            elif param_name == 'guide_loss_weight':
                self.criterion.guide_loss_weight = new_value
                self.logger.info(f"üîß –û–±–Ω–æ–≤–ª–µ–Ω guide_loss_weight: {new_value}")
                
            elif hasattr(self.hparams, param_name):
                setattr(self.hparams, param_name, new_value)
                self.logger.info(f"üîß –û–±–Ω–æ–≤–ª–µ–Ω {param_name}: {new_value}")
    
    def save_checkpoint(self, filename: str, metrics: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç checkpoint –º–æ–¥–µ–ª–∏."""
        checkpoint = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_validation_loss': self.best_validation_loss,
            'hparams': vars(self.hparams),
            'metrics': metrics,
            'training_history': self.training_metrics_history
        }
        
        torch.save(checkpoint, filename)
        self.logger.info(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
    
    def train(self, train_loader, val_loader, max_epochs: Optional[int] = None):
        """
        –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Smart Tuner.
        
        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö  
            max_epochs: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (None = –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        """
        self.logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º enhanced –æ–±—É—á–µ–Ω–∏–µ Tacotron2")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.initialize_training()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö
        if max_epochs is None:
            if self.smart_tuner and hasattr(self.smart_tuner, 'epoch_optimizer'):
                try:
                    analysis = self.smart_tuner.analyze_dataset_for_training(self.dataset_info)
                    max_epochs = analysis.get('optimal_epochs', 3000)
                    self.logger.info(f"üìä Smart Tuner —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç {max_epochs} —ç–ø–æ—Ö")
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
                    max_epochs = 3000
            else:
                max_epochs = 3000
        
        training_start_time = time.time()
        
        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
            for epoch in range(max_epochs):
                self.current_epoch = epoch
                
                # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —ç–ø–æ—Ö–∏
                should_continue = self.train_epoch(train_loader, val_loader)
                
                if not should_continue:
                    self.logger.info("üèÅ –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ Smart Tuner")
                    break
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if epoch % 100 == 0:
                    self.save_checkpoint(f'checkpoint_epoch_{epoch}.pth', 
                                       self.training_metrics_history[-1])
            
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
            total_training_time = (time.time() - training_start_time) / 3600  # –≤ —á–∞—Å–∞—Ö
            
            final_metrics = self.training_metrics_history[-1] if self.training_metrics_history else {}
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self.save_checkpoint('final_model.pth', final_metrics)
            
            # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ Smart Tuner –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            if self.smart_tuner:
                try:
                    training_summary = self.smart_tuner.on_training_complete(
                        final_metrics, self.current_epoch, total_training_time
                    )
                    self.logger.info(f"üìà Smart Tuner –∞–Ω–∞–ª–∏–∑: {training_summary}")
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            
            self.logger.info(
                f"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –≠–ø–æ—Ö: {self.current_epoch}, "
                f"–í—Ä–µ–º—è: {total_training_time:.1f}—á, "
                f"–õ—É—á—à–∏–π val_loss: {self.best_validation_loss:.4f}"
            )
            
        except KeyboardInterrupt:
            self.logger.info("‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            self.save_checkpoint('interrupted_model.pth', 
                               self.training_metrics_history[-1] if self.training_metrics_history else {})
        
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            raise
        
        finally:
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            if self.training_metrics_history:
                self._print_training_summary()
    
    def _print_training_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø–æ –æ–±—É—á–µ–Ω–∏—é."""
        metrics = self.training_metrics_history
        
        if not metrics:
            return
        
        first_epoch = metrics[0]
        last_epoch = metrics[-1]
        
        train_loss_improvement = first_epoch['train_loss'] - last_epoch['train_loss']
        val_loss_improvement = first_epoch['val_loss'] - last_epoch['val_loss']
        quality_improvement = last_epoch.get('quality_score', 0) - first_epoch.get('quality_score', 0)
        
        self.logger.info("üìã –°–í–û–î–ö–ê –û–ë–£–ß–ï–ù–ò–Ø:")
        self.logger.info(f"   –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–æ: {len(metrics)}")
        self.logger.info(f"   –£–ª—É—á—à–µ–Ω–∏–µ train_loss: {train_loss_improvement:.4f}")
        self.logger.info(f"   –£–ª—É—á—à–µ–Ω–∏–µ val_loss: {val_loss_improvement:.4f}")
        self.logger.info(f"   –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞: {quality_improvement:.3f}")
        self.logger.info(f"   –õ—É—á—à–∏–π val_loss: {self.best_validation_loss:.4f}")
        
        # –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
        phases_used = set(m.get('phase', 'unknown') for m in metrics)
        self.logger.info(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–∑—ã: {', '.join(phases_used)}")
        
        # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏
        avg_epoch_time = np.mean([m.get('epoch_time', 0) for m in metrics])
        self.logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏: {avg_epoch_time:.1f}—Å")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ enhanced –æ–±—É—á–µ–Ω–∏—è."""
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    hparams = create_hparams()
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω–∞ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö)
    dataset_info = {
        'total_duration_minutes': 120,  # –ü—Ä–∏–º–µ—Ä: 2 —á–∞—Å–∞ –∞—É–¥–∏–æ
        'num_speakers': 1,
        'voice_complexity': 'moderate',  # simple, moderate, complex, very_complex
        'audio_quality': 'good',         # poor, fair, good, excellent
        'language': 'en'
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = EnhancedTacotronTrainer(hparams, dataset_info)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ data loaders (–∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
    # train_loader = create_train_dataloader(hparams)
    # val_loader = create_val_dataloader(hparams)
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    # trainer.train(train_loader, val_loader)
    
    print("üöÄ Enhanced Tacotron2 Training System –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
    print("üìã –î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è:")
    print("   1. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç")
    print("   2. –°–æ–∑–¥–∞–π—Ç–µ data loaders")
    print("   3. –í—ã–∑–æ–≤–∏—Ç–µ trainer.train(train_loader, val_loader)")


if __name__ == "__main__":
    main() 