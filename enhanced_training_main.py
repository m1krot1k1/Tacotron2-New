#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Tacotron2 Training with Smart Tuner Integration
–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è TTS —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ —É–ª—É—á—à–µ–Ω–∏—è –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 2024-2025:
- Very Attentive Tacotron (Google, 2025)
- MonoAlign robust alignment (INTERSPEECH 2024)
- XTTS Advanced training practices
- DLPO reinforcement learning
- Style-BERT-VITS2 optimizations
- Smart Tuner intelligent automation
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.distributed as dist
import numpy as np
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import yaml

# –ò–º–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from model import Tacotron2
from loss_function import Tacotron2Loss
from hparams import create_hparams
from audio_quality_enhancer import AudioQualityEnhancer

# –ò–º–ø–æ—Ä—Ç Smart Tuner –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    from smart_tuner.smart_tuner_integration import SmartTunerIntegration
    from smart_tuner.telegram_monitor import TelegramMonitor
    from smart_tuner.integration_manager import SmartTunerIntegrationManager
    SMART_TUNER_AVAILABLE = True
except ImportError:
    SMART_TUNER_AVAILABLE = False
    logging.warning("Smart Tuner –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")

# –ò–º–ø–æ—Ä—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏–∑ train.py
try:
    from debug_reporter import initialize_debug_reporter, get_debug_reporter
    DEBUG_REPORTER_AVAILABLE = True
except ImportError:
    DEBUG_REPORTER_AVAILABLE = False
    logging.warning("Debug Reporter –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –ò–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
try:
    from training_utils.dynamic_padding import DynamicPaddingCollator
    from training_utils.bucket_batching import BucketBatchSampler
    UTILS_AVAILABLE = True
except ImportError:
    UTILS_AVAILABLE = False
    logging.warning("–£—Ç–∏–ª–∏—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

# === MLflow: –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
try:
    import mlflow
    import mlflow.pytorch
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logging.warning("MLflow –Ω–µ –Ω–∞–π–¥–µ–Ω, –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è")

# === TensorBoard: –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    logging.warning("TensorBoard –Ω–µ –Ω–∞–π–¥–µ–Ω, –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è")

class EnhancedTacotronTrainer:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–Ω–µ—Ä Tacotron2 —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Smart Tuner –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    2. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    3. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–ø–æ—Ö–∞–º–∏
    4. –§–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    5. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 2025
    """
    
    def __init__(self, hparams, dataset_info: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è enhanced —Ç—Ä–µ–Ω–µ—Ä–∞.
        
        Args:
            hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            dataset_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        self.hparams = hparams
        self.dataset_info = dataset_info or {}
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = self._setup_logger()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Smart Tuner
        self.smart_tuner = None
        if SMART_TUNER_AVAILABLE:
            try:
                self.smart_tuner = SmartTunerIntegration()
                self.logger.info("üöÄ Smart Tuner —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Smart Tuner: {e}")
        
        # üì± –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram Monitor
        self.telegram_monitor = None
        if SMART_TUNER_AVAILABLE:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º TelegramMonitorEnhanced —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                from smart_tuner.telegram_monitor_enhanced import TelegramMonitorEnhanced
                import yaml
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –Ω–∞–ø—Ä—è–º—É—é
                config_path = "smart_tuner/config.yaml"
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f)
                except Exception as e:
                    self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥: {e}")
                    config = {}
                
                telegram_config = config.get('telegram', {})
                bot_token = telegram_config.get('bot_token')
                chat_id = telegram_config.get('chat_id')
                enabled = telegram_config.get('enabled', False)
                
                if bot_token and chat_id and enabled:
                    self.telegram_monitor = TelegramMonitorEnhanced(
                        bot_token=bot_token,
                        chat_id=chat_id,
                        enabled=enabled
                    )
                    self.logger.info("üì± Telegram Monitor Enhanced –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                else:
                    self.telegram_monitor = None
                    self.logger.warning("üì± Telegram Monitor –æ—Ç–∫–ª—é—á–µ–Ω (–Ω–µ–ø–æ–ª–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)")
            except Exception as e:
                self.telegram_monitor = None
                self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Telegram Monitor: {e}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.model = None
        self.criterion = None
        self.optimizer = None
        self.audio_enhancer = AudioQualityEnhancer()
        
        # üîß –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ö–û–ú–ü–û–ù–ï–ù–¢–û–í –ò–ó TRAIN.PY
        # Integration Manager –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.integration_manager = None
        if SMART_TUNER_AVAILABLE:
            try:
                self.integration_manager = SmartTunerIntegrationManager()
                self.logger.info("üîß Integration Manager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Integration Manager: {e}")
        
        # Debug Reporter –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        self.debug_reporter = None
        if DEBUG_REPORTER_AVAILABLE:
            try:
                self.debug_reporter = initialize_debug_reporter(self.telegram_monitor)
                self.logger.info("üîç Debug Reporter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Debug Reporter: {e}")
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.global_step = 0
        self.best_validation_loss = float('inf')
        self.training_metrics_history = []
        
        # –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
        self.training_phases = {
            'pre_alignment': {'max_epoch': 500, 'focus': 'attention_learning'},
            'alignment_learning': {'max_epoch': 2000, 'focus': 'attention_stabilization'},
            'quality_optimization': {'max_epoch': 3000, 'focus': 'quality_improvement'},
            'fine_tuning': {'max_epoch': 3500, 'focus': 'final_polishing'}
        }
        
        self.logger.info("‚úÖ Enhanced Tacotron Trainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        self.tensorboard_writer = None
        self.tensorboard_logdir = 'logs'  # –ú–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º
        if TENSORBOARD_AVAILABLE:
            try:
                # === –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ TensorBoard ===
                if os.path.exists(self.tensorboard_logdir):
                    for file in os.listdir(self.tensorboard_logdir):
                        if file.startswith('events.out.tfevents'):
                            os.remove(os.path.join(self.tensorboard_logdir, file))
                            self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥ TensorBoard: {file}")
                self.tensorboard_writer = SummaryWriter(self.tensorboard_logdir)
                self.logger.info(f"‚úÖ TensorBoard writer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.tensorboard_logdir}")
            except Exception as e:
                self.tensorboard_writer = None
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TensorBoard: {e}")
        else:
            self.logger.warning("TensorBoard –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è")
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ç—Ä–µ–Ω–µ—Ä–∞."""
        logger = logging.getLogger('EnhancedTacotronTrainer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π handler
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                '%(asctime)s - [Enhanced Tacotron] - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            
            # –§–∞–π–ª–æ–≤—ã–π handler
            file_handler = logging.FileHandler('enhanced_training.log')
            file_handler.setFormatter(console_formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def initialize_training(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""
        self.logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Smart Tuner
        if self.smart_tuner:
            original_hparams = vars(self.hparams).copy()
            optimized_hparams = self.smart_tuner.on_training_start(
                original_hparams, self.dataset_info
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            for key, value in optimized_hparams.items():
                if hasattr(self.hparams, key):
                    setattr(self.hparams, key, value)
                    
            self.logger.info("‚ú® –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ Smart Tuner")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model = Tacotron2(self.hparams).cuda()
        self.logger.info(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {sum(p.numel() for p in self.model.parameters())} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è loss —Ñ—É–Ω–∫—Ü–∏–∏ —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏
        self.criterion = Tacotron2Loss(self.hparams)
        self.logger.info("üéØ Enhanced loss function –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.hparams.learning_rate,
            weight_decay=getattr(self.hparams, 'weight_decay', 1e-6)
        )
        self.logger.info("‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Smart LR Adapter
        try:
            from smart_tuner.smart_lr_adapter import SmartLRAdapter, set_global_lr_adapter
            self.lr_adapter = SmartLRAdapter(
                optimizer=self.optimizer,
                patience=10,
                factor=0.5,
                min_lr=getattr(self.hparams, 'learning_rate_min', 1e-8),
                max_lr=self.hparams.learning_rate * 2,
                emergency_factor=0.1,
                grad_norm_threshold=1000.0,
                loss_nan_threshold=1e6
            )
            set_global_lr_adapter(self.lr_adapter)
            self.logger.info("‚úÖ Smart LR Adapter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            self.lr_adapter = None
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Smart LR Adapter: {e}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è scheduler
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, 
            T_0=getattr(self.hparams, 'scheduler_T_0', 1000),
            eta_min=getattr(self.hparams, 'min_learning_rate', 1e-6)
        )
        
        self.logger.info("üöÄ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.last_attention_diagonality = 0.0
        
        # === MLflow: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ ===
        if MLFLOW_AVAILABLE:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∞–∫—Ç–∏–≤–Ω—ã–π run
                active_run = mlflow.active_run()
                if active_run is not None:
                    self.logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π MLflow run: {active_run.info.run_id}")
                else:
                    experiment_name = f"tacotron2_training_{int(time.time())}"
                    mlflow.set_experiment(experiment_name)
                    mlflow.start_run(run_name=f"training_run_{int(time.time())}")
                    self.logger.info(f"‚úÖ MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {experiment_name}")
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MLflow: {e}")
        
        # üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AutoFixManager –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
        try:
            from smart_tuner.auto_fix_manager import AutoFixManager
            self.auto_fix_manager = AutoFixManager(
                model=self.model,
                optimizer=self.optimizer,
                hparams=self.hparams,
                telegram_monitor=self.telegram_monitor
            )
            self.logger.info("ü§ñ AutoFixManager –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω")
        except ImportError:
            self.auto_fix_manager = None
            self.logger.warning("‚ö†Ô∏è AutoFixManager –Ω–µ –Ω–∞–π–¥–µ–Ω - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ç–∫–ª—é—á–µ–Ω—ã")
    
    def get_current_training_phase(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è."""
        for phase_name, phase_info in self.training_phases.items():
            if self.current_epoch <= phase_info['max_epoch']:
                return phase_name
        return 'fine_tuning'
    
    def adjust_hyperparams_for_phase(self, phase: str):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è."""
        phase_configs = {
            'pre_alignment': {
                'guided_attention_weight': 10.0,
                'learning_rate_multiplier': 1.0,
                'teacher_forcing_ratio': 1.0
            },
            'alignment_learning': {
                'guided_attention_weight': 3.0,
                'learning_rate_multiplier': 0.8,
                'teacher_forcing_ratio': 0.9
            },
            'quality_optimization': {
                'guided_attention_weight': 1.0,
                'learning_rate_multiplier': 0.5,
                'teacher_forcing_ratio': 0.8
            },
            'fine_tuning': {
                'guided_attention_weight': 0.5,
                'learning_rate_multiplier': 0.3,
                'teacher_forcing_ratio': 0.7
            }
        }
        
        if phase in phase_configs:
            config = phase_configs[phase]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ loss —Ñ—É–Ω–∫—Ü–∏–π
            self.criterion.guide_loss_weight = config['guided_attention_weight']
            
            # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
            base_lr = self.hparams.learning_rate
            new_lr = base_lr * config['learning_rate_multiplier']
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = new_lr
            
            self.logger.info(f"üîÑ –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∞–∑—ã '{phase}': "
                           f"guided_weight={config['guided_attention_weight']}, "
                           f"lr_mult={config['learning_rate_multiplier']}")
    
    def train_step(self, batch):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è —Å enhanced –∫–∞—á–µ—Å—Ç–≤–æ–º."""
        self.model.train()
        self.optimizer.zero_grad()
        
        # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ batch (TextMelCollate –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)
        text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths, ctc_text, ctc_text_lengths, guide_mask = batch
        
        # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
        text_inputs = text_inputs.cuda()
        mel_targets = mel_targets.cuda() 
        gate_targets = gate_targets.cuda()
        
        # Forward pass —á–µ—Ä–µ–∑ parse_batch –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        x, y = self.model.parse_batch(batch)
        model_outputs = self.model(x)
        # –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
        if len(model_outputs) >= 4:
            mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model_outputs[:4]
        else:
            # Fallback –¥–ª—è —Å–ª—É—á–∞—è, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –º–µ–Ω—å—à–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            mel_outputs = model_outputs[0] if len(model_outputs) > 0 else None
            mel_outputs_postnet = model_outputs[1] if len(model_outputs) > 1 else None
            gate_outputs = model_outputs[2] if len(model_outputs) > 2 else None
            alignments = model_outputs[3] if len(model_outputs) > 3 else None
        
        # üîç –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –ö–ê–ß–ï–°–¢–í–ê –ò–ó –í–´–•–û–î–û–í –ú–û–î–ï–õ–ò
        attention_diagonality = 0.0
        gate_accuracy = 0.0
        
        # üîß –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê GUIDED ATTENTION
        if hasattr(self.criterion, 'guide_loss_weight'):
            # –ï—Å–ª–∏ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º guided attention weight
            if self.global_step > 0 and hasattr(self, 'last_attention_diagonality'):
                if self.last_attention_diagonality < 0.05:
                    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å - —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ
                    new_weight = min(self.criterion.guide_loss_weight * 3.0, 100.0)
                    self.criterion.guide_loss_weight = new_weight
                    self.logger.warning(f"üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {new_weight:.1f}")
                elif self.last_attention_diagonality < 0.1:
                    # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å - —Å–∏–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ
                    new_weight = min(self.criterion.guide_loss_weight * 2.5, 75.0)
                    self.criterion.guide_loss_weight = new_weight
                    self.logger.warning(f"üö® –°–∏–ª—å–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {new_weight:.1f}")
                elif self.last_attention_diagonality < 0.3:
                    # –ù–∏–∑–∫–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å - —É–º–µ—Ä–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ
                    new_weight = min(self.criterion.guide_loss_weight * 1.5, 50.0)
                    self.criterion.guide_loss_weight = new_weight
                    self.logger.info(f"üìà –£–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {new_weight:.1f}")
                elif self.last_attention_diagonality > 0.7:
                    # –•–æ—Ä–æ—à–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å - –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
                    new_weight = max(self.criterion.guide_loss_weight * 0.9, 1.0)
                    self.criterion.guide_loss_weight = new_weight
                    self.logger.info(f"üìâ –°–Ω–∏–∂–µ–Ω–∏–µ guided attention weight: {new_weight:.1f}")
        
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º attention_diagonality –∏–∑ attention –º–∞—Ç—Ä–∏—Ü—ã
            if alignments is not None:
                attention_matrix = alignments.detach().cpu().numpy()
                if attention_matrix.ndim == 3:  # [batch, time, mel_time]
                    # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ batch –æ—Ç–¥–µ–ª—å–Ω–æ
                    batch_diagonalities = []
                    for b in range(attention_matrix.shape[0]):
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º attention –º–∞—Ç—Ä–∏—Ü—É
                        attn = attention_matrix[b]
                        if attn.sum() > 0:
                            attn = attn / attn.sum(axis=1, keepdims=True)
                        
                        # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
                        min_dim = min(attn.shape[0], attn.shape[1])
                        diagonal_elements = []
                        for i in range(min_dim):
                            diagonal_elements.append(attn[i, i])
                        batch_diagonalities.append(np.mean(diagonal_elements) if diagonal_elements else 0.0)
                    
                    # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –ø–æ batch
                    attention_diagonality = np.mean(batch_diagonalities) if batch_diagonalities else 0.0
                else:
                    attention_diagonality = 0.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º gate_accuracy –∏–∑ gate outputs
            if gate_outputs is not None:
                # –í—ã—á–∏—Å–ª—è–µ–º accuracy –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                gate_pred = (gate_outputs > 0.5).float()
                gate_targets_binary = (gate_targets > 0.5).float()
                correct = (gate_pred == gate_targets_binary).float().mean()
                gate_accuracy = correct.item()
                
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            attention_diagonality = 0.0
            gate_accuracy = 0.0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
        self.last_attention_diagonality = attention_diagonality
        
        # –õ–æ–≥–∏—Ä—É–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if self.global_step % 100 == 0:
            self.logger.info(f"üìä Attention –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {attention_diagonality:.4f}")
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏
        loss_components = self.criterion(
            model_outputs, 
            (mel_targets, gate_targets),
            attention_weights=alignments,
            gate_outputs=gate_outputs
        )
        
        # Tacotron2Loss –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 4 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: mel_loss, gate_loss, guide_loss, emb_loss
        if len(loss_components) == 4:
            mel_loss, gate_loss, guide_loss, emb_loss = loss_components
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ loss –≤ –æ–¥–∏–Ω –æ–±—â–∏–π loss
            loss = mel_loss + gate_loss + guide_loss + emb_loss
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π
            loss_dict = {
                'mel_loss': mel_loss.item(),
                'gate_loss': gate_loss.item(),
                'guide_loss': guide_loss.item(),
                'emb_loss': emb_loss.item(),
                'total_loss': loss.item()
            }
        else:
            # Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö loss —Ñ—É–Ω–∫—Ü–∏–π
            loss = loss_components[0] if len(loss_components) > 0 else torch.tensor(0.0)
            loss_dict = {'total_loss': loss.item()}
        
        # Backward pass
        loss.backward()
        
        # –í—ã—á–∏—Å–ª—è–µ–º grad_norm –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        grad_norm = torch.nn.utils.clip_grad_norm_(
            self.model.parameters(), 
            getattr(self.hparams, 'grad_clip_thresh', 1.0)
        )
        
        # üîß –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú —á–µ—Ä–µ–∑ AutoFixManager
        if self.auto_fix_manager:
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                fix_metrics = {
                    'grad_norm': float(grad_norm),
                    'attention_diagonality': attention_diagonality,
                    'gate_accuracy': gate_accuracy,
                    'loss': float(loss.item()),
                    'mel_loss': loss_dict.get('mel_loss', 0),
                    'gate_loss': loss_dict.get('gate_loss', 0),
                    'guide_loss': loss_dict.get('guide_loss', 0)
                }
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                applied_fixes = self.auto_fix_manager.analyze_and_fix(
                    step=self.global_step,
                    metrics=fix_metrics,
                    loss=loss
                )
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                if applied_fixes:
                    self.logger.info(f"üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–æ {len(applied_fixes)} –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π")
                    for fix in applied_fixes:
                        if fix.success:
                            self.logger.info(f"‚úÖ {fix.description}")
                        else:
                            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å: {fix.description}")
                            
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ AutoFixManager: {e}")
        
        # üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (fallback)
        if grad_norm < 1e-8:
            self.logger.warning(f"‚ö†Ô∏è –ò—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {grad_norm:.2e}")
            # –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            try:
                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º loss —Å –±–æ–ª—å—à–∏–º –º–∞—Å—à—Ç–∞–±–æ–º
                scaled_loss = loss * 10.0
                scaled_loss.backward()
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    getattr(self.hparams, 'grad_clip_thresh', 1.0)
                )
                self.logger.info(f"üîÑ –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: {grad_norm:.2e}")
            except Exception as e:
                self.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã: {e}")
        
        self.optimizer.step()
        
        # üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Smart LR Adapter
        if self.lr_adapter:
            try:
                lr_changed = self.lr_adapter.step(
                    loss=float(loss.item()),
                    grad_norm=float(grad_norm),
                    step=self.global_step
                )
                if lr_changed:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    self.logger.info(f"üîÑ Smart LR –∞–¥–∞–ø—Ç–∞—Ü–∏—è: LR –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ {current_lr:.2e}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ Smart LR Adapter: {e}")
        
        self.scheduler.step()
        
        # üîß –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° INTEGRATION MANAGER (–ø–æ—Å–ª–µ backward)
        if self.integration_manager:
            try:
                # –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                integration_result = self.integration_manager.step(
                    step=self.global_step,
                    loss=float(loss.item()),
                    grad_norm=float(grad_norm),
                    model=self.model,
                    optimizer=self.optimizer
                )
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
                if integration_result.get('emergency_mode'):
                    self.logger.warning(f"üö® Smart Tuner –≤ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ: {integration_result.get('recommendations', [])}")
                    
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –≤ Integration Manager: {e}")
        
        # üîç DEBUG REPORTER - –¥–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        if self.debug_reporter:
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                debug_data = {
                    'step': self.global_step,
                    'epoch': self.current_epoch,
                    'loss': loss.item(),
                    'attention_diagonality': attention_diagonality,
                    'gate_accuracy': gate_accuracy,
                    'mel_outputs': mel_outputs_postnet.detach().cpu().numpy() if mel_outputs_postnet is not None else None,
                    'gate_outputs': gate_outputs.detach().cpu().numpy() if gate_outputs is not None else None,
                    'alignments': alignments.detach().cpu().numpy() if alignments is not None else None,
                }
                
                self.debug_reporter.collect_step_data(
                    step=self.global_step,
                    metrics=debug_data,
                    model=self.model,
                    y_pred=model_outputs,
                    loss_components=loss_dict,
                    hparams=self.hparams,
                    smart_tuner_decisions={}
                )
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Debug Reporter: {e}")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ Smart Tuner
        quality_analysis = {}
        if self.smart_tuner:
            try:
                quality_analysis = self.smart_tuner.on_batch_end(
                    self.current_epoch,
                    self.global_step,
                    loss_dict,
                    (mel_outputs_postnet, gate_outputs, alignments)
                )
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
        
        # üì± Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if self.telegram_monitor and self.global_step % 1000 == 0:
            try:
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
                enhanced_metrics = loss_dict.copy()
                enhanced_metrics.update({
                    'attention_diagonality': attention_diagonality,
                    'gate_accuracy': gate_accuracy,
                    'grad_norm': float(grad_norm),
                })
                
                self.telegram_monitor.send_training_update(
                    step=self.global_step,
                    metrics=enhanced_metrics,
                    attention_weights=alignments,
                    gate_outputs=gate_outputs
                )
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard ===
        if self.tensorboard_writer is not None:
            try:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                self.tensorboard_writer.add_scalar("train/loss", loss.item(), self.global_step)
                self.tensorboard_writer.add_scalar("train/attention_diagonality", attention_diagonality, self.global_step)
                self.tensorboard_writer.add_scalar("train/gate_accuracy", gate_accuracy, self.global_step)
                
                # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
                self.tensorboard_writer.add_scalar("train/mel_loss", loss_dict.get('mel_loss', 0), self.global_step)
                self.tensorboard_writer.add_scalar("train/gate_loss", loss_dict.get('gate_loss', 0), self.global_step)
                self.tensorboard_writer.add_scalar("train/guide_loss", loss_dict.get('guide_loss', 0), self.global_step)
                self.tensorboard_writer.add_scalar("train/emb_loss", loss_dict.get('emb_loss', 0), self.global_step)
                
                # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
                self.tensorboard_writer.add_scalar("train/grad_norm", float(grad_norm), self.global_step)
                self.tensorboard_writer.add_scalar("train/learning_rate", self.optimizer.param_groups[0]['lr'], self.global_step)
                
                # Guided attention weight
                if hasattr(self.criterion, 'guide_loss_weight'):
                    self.tensorboard_writer.add_scalar("train/guided_attention_weight", self.criterion.guide_loss_weight, self.global_step)
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                self.tensorboard_writer.flush()
                
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ TensorBoard: {e}")
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow ===
        if MLFLOW_AVAILABLE:
            try:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                mlflow.log_metric("train.loss", loss.item(), step=self.global_step)
                mlflow.log_metric("train.attention_diagonality", attention_diagonality, step=self.global_step)
                mlflow.log_metric("train.gate_accuracy", gate_accuracy, step=self.global_step)
                
                # –î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ loss –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
                mlflow.log_metric("train.mel_loss", loss_dict.get('mel_loss', 0), step=self.global_step)
                mlflow.log_metric("train.gate_loss", loss_dict.get('gate_loss', 0), step=self.global_step)
                mlflow.log_metric("train.guide_loss", loss_dict.get('guide_loss', 0), step=self.global_step)
                mlflow.log_metric("train.emb_loss", loss_dict.get('emb_loss', 0), step=self.global_step)
                
                # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
                mlflow.log_metric("train.grad_norm", float(grad_norm), step=self.global_step)
                mlflow.log_metric("train.learning_rate", self.optimizer.param_groups[0]['lr'], step=self.global_step)
                
                # Guided attention weight
                if hasattr(self.criterion, 'guide_loss_weight'):
                    mlflow.log_metric("train.guided_attention_weight", self.criterion.guide_loss_weight, step=self.global_step)
                
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLflow: {e}")
        
        self.global_step += 1
        
        return {
            'loss': loss.item(),
            'loss_breakdown': loss_dict,
            'quality_analysis': quality_analysis,
            'attention_diagonality': attention_diagonality,
            'gate_accuracy': gate_accuracy,
            'grad_norm': float(grad_norm)
        }
    
    def validate_step(self, val_loader):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞—á–µ—Å—Ç–≤–∞."""
        self.model.eval()
        val_losses = []
        quality_metrics = []
        
        with torch.no_grad():
            for batch in val_loader:
                text_inputs, text_lengths, mel_targets, gate_targets, mel_lengths, ctc_text, ctc_text_lengths, guide_mask = batch
                
                # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU
                text_inputs = text_inputs.cuda()
                mel_targets = mel_targets.cuda()
                gate_targets = gate_targets.cuda()
                
                # Forward pass —á–µ—Ä–µ–∑ parse_batch –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                x, y = self.model.parse_batch(batch)
                model_outputs = self.model(x)
                # –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
                if len(model_outputs) >= 4:
                    mel_outputs, mel_outputs_postnet, gate_outputs, alignments = model_outputs[:4]
                else:
                    # Fallback –¥–ª—è —Å–ª—É—á–∞—è, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –º–µ–Ω—å—à–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    mel_outputs = model_outputs[0] if len(model_outputs) > 0 else None
                    mel_outputs_postnet = model_outputs[1] if len(model_outputs) > 1 else None
                    gate_outputs = model_outputs[2] if len(model_outputs) > 2 else None
                    alignments = model_outputs[3] if len(model_outputs) > 3 else None
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
                loss_components = self.criterion(
                    model_outputs,
                    (mel_targets, gate_targets),
                    attention_weights=alignments,
                    gate_outputs=gate_outputs
                )
                
                # Tacotron2Loss –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 4 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞: mel_loss, gate_loss, guide_loss, emb_loss
                if len(loss_components) == 4:
                    mel_loss, gate_loss, guide_loss, emb_loss = loss_components
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ loss –≤ –æ–¥–∏–Ω –æ–±—â–∏–π loss
                    loss = mel_loss + gate_loss + guide_loss + emb_loss
                    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π
                    loss_dict = {
                        'mel_loss': mel_loss.item(),
                        'gate_loss': gate_loss.item(),
                        'guide_loss': guide_loss.item(),
                        'emb_loss': emb_loss.item(),
                        'total_loss': loss.item()
                    }
                else:
                    # Fallback –¥–ª—è –¥—Ä—É–≥–∏—Ö loss —Ñ—É–Ω–∫—Ü–∏–π
                    loss = loss_components[0] if len(loss_components) > 0 else torch.tensor(0.0)
                    loss_dict = {'total_loss': loss.item()}
                
                val_losses.append(loss.item())
                
                # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
                if self.smart_tuner and len(quality_metrics) < 5:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 –±–∞—Ç—á–µ–π
                    try:
                        quality_analysis = self.smart_tuner.on_batch_end(
                            self.current_epoch,
                            self.global_step,
                            loss_dict,
                            (mel_outputs_postnet, gate_outputs, alignments)
                        )
                        quality_metrics.append(quality_analysis.get('quality_score', 0.5))
                    except Exception as e:
                        self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        
        avg_val_loss = np.mean(val_losses)
        avg_quality_score = np.mean(quality_metrics) if quality_metrics else 0.5
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard ===
        if self.tensorboard_writer is not None:
            try:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                self.tensorboard_writer.add_scalar("val/loss", avg_val_loss, self.global_step)
                self.tensorboard_writer.add_scalar("val/quality_score", avg_quality_score, self.global_step)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                self.tensorboard_writer.add_scalar("val/epoch", self.current_epoch, self.global_step)
                self.tensorboard_writer.add_scalar("val/best_loss", self.best_validation_loss, self.global_step)
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                self.tensorboard_writer.flush()
                
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ TensorBoard (–≤–∞–ª–∏–¥–∞—Ü–∏—è): {e}")
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow ===
        if MLFLOW_AVAILABLE:
            try:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                mlflow.log_metric("val.loss", avg_val_loss, step=self.global_step)
                mlflow.log_metric("val.quality_score", avg_quality_score, step=self.global_step)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                mlflow.log_metric("val.epoch", self.current_epoch, step=self.global_step)
                mlflow.log_metric("val.best_loss", self.best_validation_loss, step=self.global_step)
                
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLflow (–≤–∞–ª–∏–¥–∞—Ü–∏—è): {e}")
        
        return {
            'val_loss': avg_val_loss,
            'quality_score': avg_quality_score
        }
    
    def train_epoch(self, train_loader, val_loader):
        """–¢—Ä–µ–Ω–∏—Ä—É–µ—Ç –æ–¥–Ω—É —ç–ø–æ—Ö—É —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∫–∞—á–µ—Å—Ç–≤–∞."""
        epoch_start_time = time.time()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è
        current_phase = self.get_current_training_phase()
        self.adjust_hyperparams_for_phase(current_phase)
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ Smart Tuner –æ –Ω–∞—á–∞–ª–µ —ç–ø–æ—Ö–∏
        current_hyperparams = {
            'learning_rate': self.optimizer.param_groups[0]['lr'],
            'guided_attention_weight': self.criterion.guide_loss_weight,
            'epoch': self.current_epoch,
            'phase': current_phase
        }
        
        if self.smart_tuner:
            try:
                updated_hyperparams = self.smart_tuner.on_epoch_start(
                    self.current_epoch, current_hyperparams
                )
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                if updated_hyperparams != current_hyperparams:
                    self.logger.info("üîß Smart Tuner –æ–±–Ω–æ–≤–∏–ª –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart Tuner –Ω–∞ —Å—Ç–∞—Ä—Ç–µ —ç–ø–æ—Ö–∏: {e}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_losses = []
        quality_issues_count = 0
        
        for batch_idx, batch in enumerate(train_loader):
            step_result = self.train_step(batch)
            train_losses.append(step_result['loss'])
            
            # –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
            quality_analysis = step_result.get('quality_analysis', {})
            quality_issues_count += len(quality_analysis.get('quality_issues', []))
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if batch_idx % 100 == 0:
                self.logger.info(
                    f"–≠–ø–æ—Ö–∞ {self.current_epoch}, –±–∞—Ç—á {batch_idx}: "
                    f"loss={step_result['loss']:.4f}, "
                    f"quality_score={quality_analysis.get('quality_score', 0):.3f}"
                )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_result = self.validate_step(val_loader)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏
        epoch_metrics = {
            'train_loss': np.mean(train_losses),
            'val_loss': val_result['val_loss'],
            'quality_score': val_result['quality_score'],
            'epoch_time': time.time() - epoch_start_time,
            'phase': current_phase,
            'quality_issues': quality_issues_count
        }
        
        if self.smart_tuner:
            try:
                decision = self.smart_tuner.on_epoch_end(
                    self.current_epoch, epoch_metrics, current_hyperparams
                )
                
                # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ—à–µ–Ω–∏–π Smart Tuner
                if decision and isinstance(decision, dict):
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ—à–µ–Ω–∏–π Smart Tuner
                    if decision.get('early_stop', False):
                        self.logger.info(f"üõë Smart Tuner —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫—É: {decision.get('reason')}")
                        return False  # –°–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è
                    
                    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    if decision.get('hyperparameter_updates'):
                        self.apply_hyperparameter_updates(decision['hyperparameter_updates'])
                        self.logger.info("üîß Smart Tuner –ø—Ä–∏–º–µ–Ω–∏–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                        
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ Smart Tuner –Ω–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —ç–ø–æ—Ö–∏: {e}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏
        self.training_metrics_history.append(epoch_metrics)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à—É—é validation loss
        if val_result['val_loss'] < self.best_validation_loss:
            self.best_validation_loss = val_result['val_loss']
            self.logger.info(f"üèÜ –ù–æ–≤—ã–π —Ä–µ–∫–æ—Ä–¥ validation loss: {self.best_validation_loss:.4f}")
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —ç–ø–æ—Ö–∏ –≤ TensorBoard ===
        if self.tensorboard_writer is not None:
            try:
                self.tensorboard_writer.add_scalar("epoch/train_loss", epoch_metrics['train_loss'], self.current_epoch)
                self.tensorboard_writer.add_scalar("epoch/val_loss", epoch_metrics['val_loss'], self.current_epoch)
                self.tensorboard_writer.add_scalar("epoch/quality_score", epoch_metrics['quality_score'], self.current_epoch)
                self.tensorboard_writer.add_scalar("epoch/quality_issues", quality_issues_count, self.current_epoch)
                self.tensorboard_writer.add_scalar("epoch/time", epoch_metrics['epoch_time'], self.current_epoch)
                self.tensorboard_writer.add_scalar("epoch/phase", 0 if current_phase == 'pre_alignment' else 1, self.current_epoch)
                self.tensorboard_writer.flush()
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–ø–æ—Ö–∏ –≤ TensorBoard: {e}")
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —ç–ø–æ—Ö–∏ –≤ MLflow ===
        if MLFLOW_AVAILABLE:
            try:
                mlflow.log_metric("epoch.train_loss", epoch_metrics['train_loss'], step=self.current_epoch)
                mlflow.log_metric("epoch.val_loss", epoch_metrics['val_loss'], step=self.current_epoch)
                mlflow.log_metric("epoch.quality_score", epoch_metrics['quality_score'], step=self.current_epoch)
                mlflow.log_metric("epoch.quality_issues", quality_issues_count, step=self.current_epoch)
                mlflow.log_metric("epoch.time", epoch_metrics['epoch_time'], step=self.current_epoch)
                mlflow.log_metric("epoch.phase", current_phase, step=self.current_epoch)
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —ç–ø–æ—Ö–∏ –≤ MLflow: {e}")
        
        # üéµ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞—É–¥–∏–æ –∫–∞–∂–¥—ã–µ 5000 —à–∞–≥–æ–≤
        if self.telegram_monitor and self.global_step % 5000 == 0:
            try:
                self.telegram_monitor.generate_and_send_test_audio(
                    step=self.global_step,
                    model=self.model,
                    hparams=self.hparams,
                    device=self.device
                )
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞—É–¥–∏–æ: {e}")
        
        # üì± –û–±—ã—á–Ω–æ–µ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤
        if self.telegram_monitor and self.global_step % 1000 == 0:
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
                telegram_metrics = {
                    'step': self.global_step,
                    'train_loss': train_loss,
                    'val_loss': getattr(self, 'last_val_loss', 0),
                    'attention_diagonality': attention_diagonality,
                    'gate_accuracy': gate_accuracy,
                    'quality_score': overall_quality,
                    'learning_rate': self.optimizer.param_groups[0]['lr'],
                    'phase': current_phase,
                    'epoch': getattr(self, 'current_epoch', 0)
                }
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ attention
                self.telegram_monitor.send_training_update(
                    step=self.global_step,
                    metrics=telegram_metrics,
                    alignments=alignments
                )
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–ø–æ—Ö–∏
        self.logger.info(
            f"üìä –≠–ø–æ—Ö–∞ {self.current_epoch} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ [{current_phase}]: "
            f"train_loss={epoch_metrics['train_loss']:.4f}, "
            f"val_loss={epoch_metrics['val_loss']:.4f}, "
            f"quality={epoch_metrics['quality_score']:.3f}, "
            f"–ø—Ä–æ–±–ª–µ–º={quality_issues_count}, "
            f"–≤—Ä–µ–º—è={epoch_metrics['epoch_time']:.1f}—Å"
        )
        
        return True  # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
    
    def apply_hyperparameter_updates(self, updates: Dict[str, Any]):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ—Ç Smart Tuner."""
        for param_name, new_value in updates.items():
            if param_name == 'learning_rate':
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = new_value
                self.logger.info(f"üîß –û–±–Ω–æ–≤–ª–µ–Ω learning_rate: {new_value}")
                
            elif param_name == 'guide_loss_weight':
                self.criterion.guide_loss_weight = new_value
                self.logger.info(f"üîß –û–±–Ω–æ–≤–ª–µ–Ω guide_loss_weight: {new_value}")
                
            elif hasattr(self.hparams, param_name):
                setattr(self.hparams, param_name, new_value)
                self.logger.info(f"üîß –û–±–Ω–æ–≤–ª–µ–Ω {param_name}: {new_value}")
    
    def save_checkpoint(self, filename: str, metrics: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç checkpoint –º–æ–¥–µ–ª–∏."""
        checkpoint = {
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_validation_loss': self.best_validation_loss,
            'hparams': vars(self.hparams),
            'metrics': metrics,
            'training_history': self.training_metrics_history
        }
        
        torch.save(checkpoint, filename)
        self.logger.info(f"üíæ Checkpoint —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
    
    def train(self, train_loader, val_loader, max_epochs: Optional[int] = None):
        """
        –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Smart Tuner.
        
        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö  
            max_epochs: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (None = –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        """
        self.logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º enhanced –æ–±—É—á–µ–Ω–∏–µ Tacotron2")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.initialize_training()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö
        if max_epochs is None:
            if self.smart_tuner and hasattr(self.smart_tuner, 'epoch_optimizer'):
                try:
                    analysis = self.smart_tuner.analyze_dataset_for_training(self.dataset_info)
                    max_epochs = analysis.get('optimal_epochs', 3000)
                    self.logger.info(f"üìä Smart Tuner —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç {max_epochs} —ç–ø–æ—Ö")
                except Exception as e:
                    self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
                    max_epochs = 3000
            else:
                max_epochs = 3000
        
        training_start_time = time.time()
        
        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
            for epoch in range(max_epochs):
                self.current_epoch = epoch
                
                # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —ç–ø–æ—Ö–∏
                should_continue = self.train_epoch(train_loader, val_loader)
                
                if not should_continue:
                    self.logger.info("üèÅ –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ Smart Tuner")
                    break
                
                # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if epoch % 100 == 0:
                    self.save_checkpoint(f'checkpoint_epoch_{epoch}.pth', 
                                       self.training_metrics_history[-1])
            
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
            total_training_time = (time.time() - training_start_time) / 3600  # –≤ —á–∞—Å–∞—Ö
            
            final_metrics = self.training_metrics_history[-1] if self.training_metrics_history else {}
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self.save_checkpoint('final_model.pth', final_metrics)
            
            # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ Smart Tuner –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            if self.smart_tuner:
                try:
                    training_summary = self.smart_tuner.on_training_complete(
                        final_metrics, self.current_epoch, total_training_time
                    )
                    self.logger.info(f"üìà Smart Tuner –∞–Ω–∞–ª–∏–∑: {training_summary}")
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            
            self.logger.info(
                f"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –≠–ø–æ—Ö: {self.current_epoch}, "
                f"–í—Ä–µ–º—è: {total_training_time:.1f}—á, "
                f"–õ—É—á—à–∏–π val_loss: {self.best_validation_loss:.4f}"
            )
            
        except KeyboardInterrupt:
            self.logger.info("‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            self.save_checkpoint('interrupted_model.pth', 
                               self.training_metrics_history[-1] if self.training_metrics_history else {})
        
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º checkpoint –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if self.training_metrics_history:
                self.save_checkpoint('error_model.pth', self.training_metrics_history[-1])
            raise
        
        finally:
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            if self.training_metrics_history:
                self._print_training_summary()
        
        # === –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ TensorBoard (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏) ===
        if self.tensorboard_writer is not None:
            try:
                self.tensorboard_writer.close()
                self.logger.info("‚úÖ TensorBoard writer –∑–∞–∫—Ä—ã—Ç")
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è TensorBoard: {e}")
        
        # === –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ MLflow (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏) ===
        if MLFLOW_AVAILABLE:
            try:
                mlflow.end_run()
                self.logger.info("‚úÖ MLflow run –∑–∞–≤–µ—Ä—à–µ–Ω")
            except Exception as e:
                self.logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è MLflow run: {e}")
    
    def _print_training_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –ø–æ –æ–±—É—á–µ–Ω–∏—é."""
        metrics = self.training_metrics_history
        
        if not metrics:
            return
        
        first_epoch = metrics[0]
        last_epoch = metrics[-1]
        
        train_loss_improvement = first_epoch['train_loss'] - last_epoch['train_loss']
        val_loss_improvement = first_epoch['val_loss'] - last_epoch['val_loss']
        quality_improvement = last_epoch.get('quality_score', 0) - first_epoch.get('quality_score', 0)
        
        self.logger.info("üìã –°–í–û–î–ö–ê –û–ë–£–ß–ï–ù–ò–Ø:")
        self.logger.info(f"   –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–æ: {len(metrics)}")
        self.logger.info(f"   –£–ª—É—á—à–µ–Ω–∏–µ train_loss: {train_loss_improvement:.4f}")
        self.logger.info(f"   –£–ª—É—á—à–µ–Ω–∏–µ val_loss: {val_loss_improvement:.4f}")
        self.logger.info(f"   –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞: {quality_improvement:.3f}")
        self.logger.info(f"   –õ—É—á—à–∏–π val_loss: {self.best_validation_loss:.4f}")
        
        # –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
        phases_used = set(m.get('phase', 'unknown') for m in metrics)
        self.logger.info(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–∑—ã: {', '.join(phases_used)}")
        
        # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏
        avg_epoch_time = np.mean([m.get('epoch_time', 0) for m in metrics])
        self.logger.info(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏: {avg_epoch_time:.1f}—Å")

    # === üî• –ú–ò–ì–†–ê–¶–ò–Ø –§–£–ù–ö–¶–ò–ô –ò–ó TRAIN.PY ===
    
    def reduce_tensor(self, tensor, n_gpus):
        """Reduce tensor across GPUs (–¥–ª—è distributed training)."""
        if n_gpus > 1:
            rt = tensor.clone()
            dist.all_reduce(rt, op=dist.ReduceOp.SUM)
            rt /= n_gpus
            return rt
        return tensor

    def init_distributed(self, hparams, n_gpus, rank, group_name):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è distributed training."""
        assert torch.cuda.is_available(), "Distributed mode requires CUDA."
        self.logger.info("Initializing Distributed")

        # Set cuda device so everything is done on the right GPU.
        torch.cuda.set_device(rank % torch.cuda.device_count())

        # Initialize distributed communication
        dist.init_process_group(
            backend=hparams.dist_backend,
            init_method=hparams.dist_url,
            world_size=n_gpus,
            rank=rank,
            group_name=group_name,
        )

        self.logger.info("Done initializing distributed")

    def load_model(self, hparams):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π distributed training."""
        model = Tacotron2(hparams).cuda()
        
        if hparams.distributed_run:
            from distributed import apply_gradient_allreduce
            model = apply_gradient_allreduce(model)

        return model

    def warm_start_model(self, checkpoint_path, model, ignore_layers, exclude=None):
        """Warm start –º–æ–¥–µ–ª–∏ –∏–∑ checkpoint."""
        assert os.path.isfile(checkpoint_path)
        self.logger.info(f"Warm starting model from checkpoint '{checkpoint_path}'")
        
        checkpoint_dict = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
        model_dict = checkpoint_dict["state_dict"]
        
        self.logger.info(f"ignoring layers: {ignore_layers}")
        if len(ignore_layers) > 0 or exclude:
            model_dict = {
                k: v
                for k, v in model_dict.items()
                if k not in ignore_layers and (not exclude or exclude not in k)
            }
        
        model.load_state_dict(model_dict, strict=False)
        return model

    def load_checkpoint(self, checkpoint_path, model, optimizer):
        """–ó–∞–≥—Ä—É–∑–∫–∞ checkpoint."""
        assert os.path.isfile(checkpoint_path)
        self.logger.info(f"Loading checkpoint '{checkpoint_path}'")
        
        checkpoint_dict = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
        model.load_state_dict(checkpoint_dict['state_dict'])
        optimizer.load_state_dict(checkpoint_dict['optimizer'])
        learning_rate = checkpoint_dict['learning_rate']
        iteration = checkpoint_dict['iteration']
        
        self.logger.info(f"Loaded checkpoint '{checkpoint_path}' (iteration {iteration})")
        return model, optimizer, learning_rate, iteration

    def save_checkpoint_legacy(self, model, optimizer, learning_rate, iteration, filepath):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ checkpoint –≤ legacy —Ñ–æ—Ä–º–∞—Ç–µ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)."""
        self.logger.info(f"Saving model and optimizer state at iteration {iteration} to {filepath}")
        torch.save({'iteration': iteration,
                   'state_dict': model.state_dict(),
                   'optimizer': optimizer.state_dict(),
                   'learning_rate': learning_rate}, filepath)

    def setup_mixed_precision(self, hparams):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ mixed precision (FP16/AMP)."""
        self.apex_available = False
        self.use_native_amp = False
        self.scaler = None

        if hparams.fp16_run:
            try:
                from apex import amp
                self.model, self.optimizer = amp.initialize(
                    self.model, self.optimizer, opt_level="O2"
                )
                self.apex_available = True
                self.logger.info("‚úÖ NVIDIA Apex —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è FP16 –æ–±—É—á–µ–Ω–∏—è")
            except ImportError:
                try:
                    from torch.amp import GradScaler, autocast
                    self.model = self.model.float()
                    self.scaler = GradScaler("cuda")
                    self.use_native_amp = True
                    self.logger.info("‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ torch.amp (PyTorch Native AMP)")
                except ImportError as e:
                    hparams.fp16_run = False
                    self.logger.warning(f"‚ùå Mixed precision –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}. FP16 –æ—Ç–∫–ª—é—á—ë–Ω.")

    def setup_loss_functions(self, hparams):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—Å–µ—Ö loss —Ñ—É–Ω–∫—Ü–∏–π."""
        # –û—Å–Ω–æ–≤–Ω–æ–π loss
        self.criterion = Tacotron2Loss(hparams)
        
        # MMI Loss
        self.mmi_loss = None
        if hparams.use_mmi:
            try:
                from mmi_loss import MMI_loss
                self.mmi_loss = MMI_loss(hparams.mmi_map, hparams.mmi_weight)
                self.logger.info("‚úÖ MMI loss –∑–∞–≥—Ä—É–∂–µ–Ω")
            except ImportError as e:
                self.logger.warning(f"‚ö†Ô∏è MMI loss –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

        # Guided Attention Loss
        self.guide_loss = None
        if hparams.use_guided_attn:
            try:
                from loss_function import GuidedAttentionLoss
                self.guide_loss = GuidedAttentionLoss(alpha=hparams.guided_attn_weight)
                self.logger.info("‚úÖ Guided Attention Loss –∑–∞–≥—Ä—É–∂–µ–Ω")
            except ImportError as e:
                self.logger.warning(f"‚ö†Ô∏è Guided Attention Loss –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

    def setup_smart_tuner_components(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—Å–µ—Ö Smart Tuner –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""
        # AdvancedQualityController
        try:
            from smart_tuner.advanced_quality_controller import AdvancedQualityController
            self.quality_ctrl = AdvancedQualityController()
            self.logger.info("ü§ñ AdvancedQualityController –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            self.quality_ctrl = None
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å AdvancedQualityController: {e}")

        # ParamScheduler
        try:
            from smart_tuner.param_scheduler import ParamScheduler
            self.sched_ctrl = ParamScheduler()
            self.logger.info("üìÖ ParamScheduler –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            self.sched_ctrl = None
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å ParamScheduler: {e}")

        # EarlyStopController
        try:
            from smart_tuner.early_stop_controller import EarlyStopController
            self.stop_ctrl = EarlyStopController()
            self.logger.info("üõë EarlyStopController –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            self.stop_ctrl = None
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å EarlyStopController: {e}")

        # Debug Reporter
        try:
            from debug_reporter import initialize_debug_reporter
            self.debug_reporter = initialize_debug_reporter(self.telegram_monitor)
            self.logger.info("üîç Debug Reporter –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            self.debug_reporter = None
            self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Debug Reporter: {e}")

    def calculate_global_mean(self, data_loader, global_mean_npy):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏."""
        if global_mean_npy and os.path.exists(global_mean_npy):
            self.logger.info(f"Loading global mean from {global_mean_npy}")
            return np.load(global_mean_npy)
        
        self.logger.info("Computing global mean...")
        global_mean = 0.0
        count = 0
        
        for batch in data_loader:
            mel = batch[1]  # mel spectrogram
            global_mean += mel.sum().item()
            count += mel.numel()
        
        global_mean /= count
        self.logger.info(f"Global mean computed: {global_mean}")
        
        if global_mean_npy:
            np.save(global_mean_npy, global_mean)
        
        return global_mean


def prepare_dataloaders(hparams):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç train –∏ val DataLoader —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π dynamic padding, bucket batching –∏ distributed.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç train_loader, val_loader.
    """
    from data_utils import TextMelLoader, TextMelCollate
    try:
        from training_utils.dynamic_padding import DynamicPaddingCollator
        from training_utils.bucket_batching import BucketBatchSampler
    except ImportError:
        DynamicPaddingCollator = None
        BucketBatchSampler = None

    trainset = TextMelLoader(hparams.training_files, hparams)
    valset = TextMelLoader(hparams.validation_files, hparams)

    use_bucket_batching = getattr(hparams, 'use_bucket_batching', True)
    use_dynamic_padding = getattr(hparams, 'use_dynamic_padding', True)

    if use_dynamic_padding and DynamicPaddingCollator is not None:
        collate_fn = DynamicPaddingCollator(pad_value=0.0, n_frames_per_step=hparams.n_frames_per_step)
    else:
        collate_fn = TextMelCollate(hparams.n_frames_per_step)

    if use_bucket_batching and BucketBatchSampler is not None:
        train_sampler = BucketBatchSampler(trainset, hparams.batch_size)
        shuffle = False
    else:
        if getattr(hparams, 'distributed_run', False):
            from torch.utils.data.distributed import DistributedSampler
            train_sampler = DistributedSampler(trainset)
            shuffle = False
        else:
            train_sampler = None
            shuffle = True

    from torch.utils.data import DataLoader
    if use_bucket_batching and BucketBatchSampler is not None:
        train_loader = DataLoader(
            trainset,
            num_workers=1,
            pin_memory=False,
            collate_fn=collate_fn,
            batch_sampler=train_sampler,
        )
    else:
        train_loader = DataLoader(
            trainset,
            num_workers=1,
            shuffle=shuffle,
            sampler=train_sampler,
            batch_size=hparams.batch_size,
            pin_memory=False,
            drop_last=True,
            collate_fn=collate_fn,
        )
    val_loader = DataLoader(
        valset,
        num_workers=1,
        shuffle=False,
        batch_size=hparams.batch_size,
        pin_memory=False,
        drop_last=False,
        collate_fn=collate_fn,
    )
    return train_loader, val_loader


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ enhanced –æ–±—É—á–µ–Ω–∏—è."""
    # –°–æ–∑–¥–∞–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    hparams = create_hparams()

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω–∞ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö)
    dataset_info = {
        'total_duration_minutes': 120,  # –ü—Ä–∏–º–µ—Ä: 2 —á–∞—Å–∞ –∞—É–¥–∏–æ
        'num_speakers': 1,
        'voice_complexity': 'moderate',  # simple, moderate, complex, very_complex
        'audio_quality': 'good',         # poor, fair, good, excellent
        'language': 'en'
    }

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataLoader'–æ–≤
    train_loader, val_loader = prepare_dataloaders(hparams)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = EnhancedTacotronTrainer(hparams, dataset_info)

    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    trainer.train(train_loader, val_loader)

    print("üöÄ Enhanced Tacotron2 Training System –∑–∞–≤–µ—Ä—à–∏–ª –æ–±—É—á–µ–Ω–∏–µ!")


if __name__ == "__main__":
    main() 