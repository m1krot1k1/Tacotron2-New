#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ EnhancedTacotronTrainer
–ü–µ—Ä–µ—Ö–æ–¥ —Å train.py –Ω–∞ enhanced_training_main.py –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
1. –ü–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ EnhancedTacotronTrainer
2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤—Å–µ—Ö —É–ª—É—á—à–µ–Ω–∏–π Smart Tuner
4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –º–∏–≥—Ä–∞—Ü–∏—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
"""

import os
import sys
import logging
import yaml
import torch
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import argparse

# –ò–º–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
from enhanced_training_main import EnhancedTacotronTrainer, prepare_dataloaders
from hparams import create_hparams
from model import Tacotron2

# –ò–º–ø–æ—Ä—Ç Smart Tuner –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    from smart_tuner.smart_tuner_integration import SmartTunerIntegration
    from smart_tuner.telegram_monitor import TelegramMonitor
    from smart_tuner.optimization_engine import OptimizationEngine
    SMART_TUNER_AVAILABLE = True
except ImportError:
    SMART_TUNER_AVAILABLE = False
    logging.warning("Smart Tuner –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")

class EnhancedTrainingIntegration:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ EnhancedTacotronTrainer
    """
    
    def __init__(self, config_path: str = "smart_tuner/config.yaml"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–æ–¥—É–ª—è
        
        Args:
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Smart Tuner
        """
        self.config_path = config_path
        self.logger = self._setup_logger()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        self.integration_status = {
            'enhanced_trainer_ready': False,
            'smart_tuner_integrated': False,
            'optimization_engine_ready': False,
            'telegram_monitor_ready': False
        }
        
        self.logger.info("üéØ Enhanced Training Integration –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger('EnhancedTrainingIntegration')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            self.logger.warning(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {self.config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            return {}
        except yaml.YAMLError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {}
    
    def validate_enhanced_trainer(self) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è EnhancedTacotronTrainer
        
        Returns:
            True –µ—Å–ª–∏ EnhancedTacotronTrainer –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            from enhanced_training_main import EnhancedTacotronTrainer
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            hparams = create_hparams()
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ç—Ä–µ–Ω–µ—Ä
            trainer = EnhancedTacotronTrainer(hparams)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            required_methods = [
                'initialize_training',
                'train_step', 
                'validate_step',
                'train_epoch',
                'train'
            ]
            
            for method_name in required_methods:
                if not hasattr(trainer, method_name):
                    self.logger.error(f"–ú–µ—Ç–æ–¥ {method_name} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ EnhancedTacotronTrainer")
                    return False
            
            self.integration_status['enhanced_trainer_ready'] = True
            self.logger.info("‚úÖ EnhancedTacotronTrainer –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ EnhancedTacotronTrainer: {e}")
            return False
    
    def integrate_smart_tuner(self) -> bool:
        """
        –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Smart Tuner —Å EnhancedTacotronTrainer
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞
        """
        if not SMART_TUNER_AVAILABLE:
            self.logger.warning("Smart Tuner –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é")
            return False
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Smart Tuner
            from smart_tuner.smart_tuner_integration import SmartTunerIntegration
            from smart_tuner.optimization_engine import OptimizationEngine
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            smart_tuner = SmartTunerIntegration()
            optimization_engine = OptimizationEngine(self.config_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            if hasattr(smart_tuner, 'on_training_start') and hasattr(smart_tuner, 'on_batch_end'):
                self.integration_status['smart_tuner_integrated'] = True
                self.logger.info("‚úÖ Smart Tuner –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                self.logger.error("Smart Tuner –Ω–µ –∏–º–µ–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç–æ–¥–æ–≤")
                return False
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Smart Tuner: {e}")
            return False
    
    def setup_optimization_engine(self) -> bool:
        """
        –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Optimization Engine
        
        Returns:
            True –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        if not SMART_TUNER_AVAILABLE:
            self.logger.warning("Smart Tuner –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Optimization Engine")
            return False
        
        try:
            from smart_tuner.optimization_engine import OptimizationEngine
            
            # –°–æ–∑–¥–∞–µ–º Optimization Engine
            optimization_engine = OptimizationEngine(self.config_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            if hasattr(optimization_engine, 'create_study_with_retry') and hasattr(optimization_engine, 'optimize'):
                self.integration_status['optimization_engine_ready'] = True
                self.logger.info("‚úÖ Optimization Engine –Ω–∞—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                self.logger.error("Optimization Engine –Ω–µ –∏–º–µ–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç–æ–¥–æ–≤")
                return False
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Optimization Engine: {e}")
            return False
    
    def setup_telegram_monitor(self) -> bool:
        """
        –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Telegram Monitor
        
        Returns:
            True –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        if not SMART_TUNER_AVAILABLE:
            self.logger.warning("Smart Tuner –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Telegram Monitor")
            return False
        
        try:
            from smart_tuner.telegram_monitor import TelegramMonitor
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Telegram
            telegram_config = self.config.get('telegram', {})
            bot_token = telegram_config.get('bot_token')
            chat_id = telegram_config.get('chat_id')
            enabled = telegram_config.get('enabled', False)
            
            if bot_token and chat_id and enabled:
                # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä
                monitor = TelegramMonitor(bot_token, chat_id)
                
                if hasattr(monitor, 'send_training_update'):
                    self.integration_status['telegram_monitor_ready'] = True
                    self.logger.info("‚úÖ Telegram Monitor –Ω–∞—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                    return True
                else:
                    self.logger.error("Telegram Monitor –Ω–µ –∏–º–µ–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç–æ–¥–æ–≤")
                    return False
            else:
                self.logger.warning("Telegram Monitor –æ—Ç–∫–ª—é—á–µ–Ω (–Ω–µ–ø–æ–ª–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)")
                return False
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Telegram Monitor: {e}")
            return False
    
    def migrate_configuration(self) -> Dict[str, Any]:
        """
        –ú–∏–≥—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å train.py –Ω–∞ EnhancedTacotronTrainer
        
        Returns:
            –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            hparams = create_hparams()
            
            # –°–æ–∑–¥–∞–µ–º –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            migrated_config = {
                'hparams': vars(hparams),
                'smart_tuner': {
                    'enabled': SMART_TUNER_AVAILABLE,
                    'config_path': self.config_path
                },
                'training': {
                    'max_epochs': getattr(hparams, 'epochs', 500000),
                    'validation_interval': getattr(hparams, 'validate_interval', 200),
                    'checkpoint_interval': getattr(hparams, 'iters_per_checkpoint', 1000)
                },
                'monitoring': {
                    'telegram_enabled': self.integration_status['telegram_monitor_ready'],
                    'tensorboard_enabled': True,
                    'mlflow_enabled': True
                }
            }
            
            self.logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞")
            return migrated_config
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {}
    
    def create_enhanced_trainer(self, hparams=None, dataset_info=None) -> Optional[EnhancedTacotronTrainer]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ EnhancedTacotronTrainer —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
        
        Args:
            hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            dataset_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            
        Returns:
            EnhancedTacotronTrainer –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            if not self.integration_status['enhanced_trainer_ready']:
                self.logger.error("EnhancedTacotronTrainer –Ω–µ –≥–æ—Ç–æ–≤")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã
            if hparams is None:
                hparams = create_hparams()
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å–ª–∏ –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞
            if dataset_info is None:
                dataset_info = {
                    'total_duration_minutes': 120,
                    'num_speakers': 1,
                    'voice_complexity': 'moderate',
                    'audio_quality': 'good',
                    'language': 'ru'
                }
            
            # –°–æ–∑–¥–∞–µ–º EnhancedTacotronTrainer
            trainer = EnhancedTacotronTrainer(hparams, dataset_info)
            
            self.logger.info("‚úÖ EnhancedTacotronTrainer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            return trainer
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è EnhancedTacotronTrainer: {e}")
            return None
    
    def run_full_integration_test(self) -> bool:
        """
        –ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        
        Returns:
            True –µ—Å–ª–∏ –≤—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã
        """
        self.logger.info("üß™ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏...")
        
        # –¢–µ—Å—Ç 1: –í–∞–ª–∏–¥–∞—Ü–∏—è EnhancedTacotronTrainer
        if not self.validate_enhanced_trainer():
            self.logger.error("‚ùå –¢–µ—Å—Ç 1 –ø—Ä–æ–≤–∞–ª–µ–Ω: EnhancedTacotronTrainer")
            return False
        
        # –¢–µ—Å—Ç 2: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Smart Tuner
        if not self.integrate_smart_tuner():
            self.logger.warning("‚ö†Ô∏è –¢–µ—Å—Ç 2: Smart Tuner –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –¢–µ—Å—Ç 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Optimization Engine
        if not self.setup_optimization_engine():
            self.logger.warning("‚ö†Ô∏è –¢–µ—Å—Ç 3: Optimization Engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –¢–µ—Å—Ç 4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Telegram Monitor
        if not self.setup_telegram_monitor():
            self.logger.warning("‚ö†Ô∏è –¢–µ—Å—Ç 4: Telegram Monitor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        
        # –¢–µ—Å—Ç 5: –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = self.create_enhanced_trainer()
        if trainer is None:
            self.logger.error("‚ùå –¢–µ—Å—Ç 5 –ø—Ä–æ–≤–∞–ª–µ–Ω: —Å–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞")
            return False
        
        # –¢–µ—Å—Ç 6: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ DataLoader'–æ–≤
        try:
            hparams = create_hparams()
            train_loader, val_loader = prepare_dataloaders(hparams)
            self.logger.info("‚úÖ –¢–µ—Å—Ç 6 –ø—Ä–æ–π–¥–µ–Ω: DataLoader'—ã –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã")
        except Exception as e:
            self.logger.error(f"‚ùå –¢–µ—Å—Ç 6 –ø—Ä–æ–≤–∞–ª–µ–Ω: {e}")
            return False
        
        self.logger.info("üéâ –í—Å–µ —Ç–µ—Å—Ç—ã –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        return True
    
    def get_integration_status(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        """
        return {
            'integration_status': self.integration_status,
            'smart_tuner_available': SMART_TUNER_AVAILABLE,
            'config_loaded': bool(self.config),
            'ready_for_training': self.integration_status['enhanced_trainer_ready']
        }


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    parser = argparse.ArgumentParser(description='Enhanced Training Integration')
    parser.add_argument('--config', type=str, default='smart_tuner/config.yaml',
                       help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Smart Tuner')
    parser.add_argument('--test', action='store_true',
                       help='–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏')
    parser.add_argument('--migrate', action='store_true',
                       help='–ú–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å
    integration = EnhancedTrainingIntegration(args.config)
    
    if args.test:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç
        success = integration.run_full_integration_test()
        if success:
            print("üéâ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        else:
            print("‚ùå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
            sys.exit(1)
    
    elif args.migrate:
        # –ú–∏–≥—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = integration.migrate_configuration()
        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞:")
        print(yaml.dump(config, default_flow_style=False))
    
    else:
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
        status = integration.get_integration_status()
        print("üìä –°—Ç–∞—Ç—É—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:")
        print(yaml.dump(status, default_flow_style=False))


if __name__ == "__main__":
    main() 