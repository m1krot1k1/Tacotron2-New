"""
üõ°Ô∏è TRAINING STABILIZATION SYSTEM - –°–∏—Å—Ç–µ–º–∞ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
================================================================

–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∏–∑ exported-assets:
1. üß† Intelligent Gradient Manager - —É–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
2. üìà Adaptive Learning Rate Scheduler - –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π LR –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫  
3. üìä Training Stability Monitor - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
4. üö® Emergency Stabilization System - —ç–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è

–í–µ—Ä—Å–∏—è: 1.0.0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
from dataclasses import dataclass
from collections import deque
import logging


class StabilityLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è"""
    STABLE = "stable"           # –°—Ç–∞–±–∏–ª—å–Ω–æ (loss_std < 1.0, grad_norm < 3.0)
    MODERATE = "moderate"       # –£–º–µ—Ä–µ–Ω–Ω–æ (loss_std < 2.0, grad_norm < 5.0)  
    UNSTABLE = "unstable"       # –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ (loss_std < 5.0, grad_norm < 10.0)
    CRITICAL = "critical"       # –ö—Ä–∏—Ç–∏—á–Ω–æ (loss_std >= 5.0, grad_norm >= 10.0)


@dataclass
class StabilityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è"""
    loss_std: float = 0.0
    gradient_norm: float = 0.0
    lr_volatility: float = 0.0
    attention_stability: float = 0.0
    convergence_trend: float = 0.0
    stability_level: StabilityLevel = StabilityLevel.STABLE


class IntelligentGradientManager(nn.Module):
    """
    üß† Intelligent Gradient Manager - —É–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
    
    –£—Å—Ç—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã:
    - Gradient explosion (–Ω–æ—Ä–º–∞ >10.0)
    - Gradient vanishing (–Ω–æ—Ä–º–∞ <0.1)
    - –ù–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–ª–æ—è–º
    """
    
    def __init__(self, 
                 target_norm: float = 2.0,
                 max_norm: float = 5.0,
                 min_norm: float = 0.1,
                 adaptation_rate: float = 0.05):
        super().__init__()
        
        self.target_norm = target_norm
        self.max_norm = max_norm
        self.min_norm = min_norm  
        self.adaptation_rate = adaptation_rate
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.gradient_history = deque(maxlen=50)
        self.clip_events = []
        self.current_scale = 1.0
        
        self.logger = logging.getLogger(__name__)
        
    def process_gradients(self, model: nn.Module, loss: torch.Tensor) -> Dict[str, float]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        
        Args:
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            loss: Loss –¥–ª—è backward pass
            
        Returns:
            Dict[str, float]: –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        """
        # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        if loss.requires_grad:
            loss.backward(retain_graph=True)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â—É—é –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        total_norm = 0.0
        param_count = 0
        
        for param in model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        total_norm = total_norm ** (1. / 2)
        self.gradient_history.append(total_norm)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
        metrics = self._apply_adaptive_clipping(model, total_norm)
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        self._apply_gradient_scaling(model, total_norm)
        
        return metrics
    
    def _apply_adaptive_clipping(self, model: nn.Module, current_norm: float) -> Dict[str, float]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏—è
        if len(self.gradient_history) >= 10:
            recent_norms = list(self.gradient_history)[-10:]
            adaptive_threshold = np.percentile(recent_norms, 75) * 1.5
            adaptive_threshold = min(adaptive_threshold, self.max_norm)
        else:
            adaptive_threshold = self.max_norm
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        if current_norm > adaptive_threshold:
            torch.nn.utils.clip_grad_norm_(model.parameters(), adaptive_threshold)
            
            self.clip_events.append({
                'step': len(self.gradient_history),
                'original_norm': current_norm,
                'clipped_norm': adaptive_threshold,
                'threshold': adaptive_threshold
            })
            
            clipped = True
            final_norm = adaptive_threshold
        else:
            clipped = False
            final_norm = current_norm
        
        return {
            'original_norm': current_norm,
            'final_norm': final_norm,
            'clipped': clipped,
            'adaptive_threshold': adaptive_threshold,
            'clip_ratio': len(self.clip_events) / max(len(self.gradient_history), 1)
        }
    
    def _apply_gradient_scaling(self, model: nn.Module, current_norm: float):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
        
        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –º–∞—Å—à—Ç–∞–± –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è target_norm
        if current_norm > 0:
            target_scale = self.target_norm / current_norm
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞—Å—à—Ç–∞–± –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            target_scale = torch.clamp(torch.tensor(target_scale), 0.1, 3.0).item()
            
            # –ü–ª–∞–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º –º–∞—Å—à—Ç–∞–±
            self.current_scale = (
                (1 - self.adaptation_rate) * self.current_scale + 
                self.adaptation_rate * target_scale
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º
            if abs(self.current_scale - 1.0) > 0.1:
                for param in model.parameters():
                    if param.grad is not None:
                        param.grad.data *= self.current_scale


class AdaptiveLearningRateScheduler:
    """
    üìà Adaptive Learning Rate Scheduler - —É–º–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate
    
    –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Ö–∞–æ—Ç–∏—á–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è LR (–∫–æ–ª–µ–±–∞–Ω–∏—è –≤ 40 —Ä–∞–∑ –∏–∑ exported-assets)
    """
    
    def __init__(self,
                 initial_lr: float = 1e-3,
                 min_lr: float = 1e-5,
                 max_lr: float = 1e-2,
                 patience: int = 10,
                 factor: float = 0.8):
        
        self.initial_lr = initial_lr
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.patience = patience
        self.factor = factor
        
        self.current_lr = initial_lr
        self.best_loss = float('inf')
        self.patience_counter = 0
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è —É–º–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.lr_history = []
        self.loss_history = deque(maxlen=100)
        
    def step(self, current_loss: float, stability_metrics: StabilityMetrics) -> float:
        """
        –£–º–Ω—ã–π —à–∞–≥ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ loss –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            current_loss: –¢–µ–∫—É—â–∞—è loss
            stability_metrics: –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            
        Returns:
            float: –ù–æ–≤—ã–π learning rate
        """
        self.loss_history.append(current_loss)
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        if stability_metrics.stability_level == StabilityLevel.CRITICAL:
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å - —Ä–µ–∑–∫–æ —Å–Ω–∏–∂–∞–µ–º LR
            new_lr = self.current_lr * 0.5
        elif stability_metrics.stability_level == StabilityLevel.UNSTABLE:
            # –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å - –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å–Ω–∏–∂–∞–µ–º LR
            new_lr = self.current_lr * 0.9
        else:
            # –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
            new_lr = self._standard_lr_update(current_loss)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        new_lr = np.clip(new_lr, self.min_lr, self.max_lr)
        
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º —Å–ª–∏—à–∫–æ–º —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (max 20% –∑–∞ —à–∞–≥)
        max_change = self.current_lr * 0.2
        if abs(new_lr - self.current_lr) > max_change:
            if new_lr > self.current_lr:
                new_lr = self.current_lr + max_change
            else:
                new_lr = self.current_lr - max_change
        
        self.current_lr = new_lr
        self.lr_history.append(new_lr)
        
        return new_lr
    
    def _standard_lr_update(self, current_loss: float) -> float:
        """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è LR"""
        
        if current_loss < self.best_loss * 0.99:  # –£–ª—É—á—à–µ–Ω–∏–µ –Ω–∞ 1%
            self.best_loss = current_loss
            self.patience_counter = 0
            return self.current_lr  # –ù–µ –∏–∑–º–µ–Ω—è–µ–º LR –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏
        else:
            self.patience_counter += 1
            
            if self.patience_counter >= self.patience:
                self.patience_counter = 0
                return self.current_lr * self.factor  # –°–Ω–∏–∂–∞–µ–º LR
            else:
                return self.current_lr  # –ñ–¥–µ–º


class TrainingStabilityMonitor:
    """
    üìä Training Stability Monitor - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è
    
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, window_size: int = 20):
        self.window_size = window_size
        
        # –ò—Å—Ç–æ—Ä–∏–∏ –º–µ—Ç—Ä–∏–∫
        self.loss_history = deque(maxlen=window_size)
        self.gradient_history = deque(maxlen=window_size)
        self.lr_history = deque(maxlen=window_size)
        self.attention_history = deque(maxlen=window_size)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stability_reports = []
        
    def update_metrics(self,
                      loss: float,
                      gradient_norm: float,
                      learning_rate: float,
                      attention_quality: float = 0.5) -> StabilityMetrics:
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            loss: –¢–µ–∫—É—â–∞—è loss
            gradient_norm: –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            learning_rate: –¢–µ–∫—É—â–∏–π learning rate
            attention_quality: –ö–∞—á–µ—Å—Ç–≤–æ attention
            
        Returns:
            StabilityMetrics: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        """
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏–∏
        self.loss_history.append(loss)
        self.gradient_history.append(gradient_norm)
        self.lr_history.append(learning_rate)
        self.attention_history.append(attention_quality)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        metrics = self._compute_stability_metrics()
        
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
        report = {
            'step': len(self.loss_history),
            'metrics': metrics,
            'recommendations': self._generate_recommendations(metrics)
        }
        self.stability_reports.append(report)
        
        return metrics
    
    def _compute_stability_metrics(self) -> StabilityMetrics:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
        
        if len(self.loss_history) < 5:
            return StabilityMetrics()  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        
        # Loss —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        loss_values = list(self.loss_history)
        loss_std = np.std(loss_values)
        
        # Gradient —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å  
        grad_values = list(self.gradient_history)
        avg_gradient_norm = np.mean(grad_values)
        
        # LR –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        lr_values = list(self.lr_history)
        lr_volatility = np.std(lr_values) / np.mean(lr_values) if len(lr_values) > 1 else 0.0
        
        # Attention —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
        att_values = list(self.attention_history)
        attention_stability = 1.0 - np.std(att_values) if len(att_values) > 1 else 1.0
        
        # –¢—Ä–µ–Ω–¥ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏
        if len(loss_values) >= 10:
            recent_loss = np.mean(loss_values[-5:])
            older_loss = np.mean(loss_values[-10:-5])
            convergence_trend = (older_loss - recent_loss) / older_loss
        else:
            convergence_trend = 0.0
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        stability_level = self._classify_stability_level(loss_std, avg_gradient_norm)
        
        return StabilityMetrics(
            loss_std=loss_std,
            gradient_norm=avg_gradient_norm,
            lr_volatility=lr_volatility,
            attention_stability=attention_stability,
            convergence_trend=convergence_trend,
            stability_level=stability_level
        )
    
    def _classify_stability_level(self, loss_std: float, grad_norm: float) -> StabilityLevel:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Ä–æ–≤–Ω—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
        
        if loss_std < 1.0 and grad_norm < 3.0:
            return StabilityLevel.STABLE
        elif loss_std < 2.0 and grad_norm < 5.0:
            return StabilityLevel.MODERATE
        elif loss_std < 5.0 and grad_norm < 10.0:
            return StabilityLevel.UNSTABLE
        else:
            return StabilityLevel.CRITICAL
    
    def _generate_recommendations(self, metrics: StabilityMetrics) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏"""
        
        recommendations = []
        
        if metrics.stability_level == StabilityLevel.CRITICAL:
            recommendations.append("üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å - –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å—Ç—Ä–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é")
            recommendations.append("üìâ –°–Ω–∏–∑–∏—Ç—å learning rate –≤ 2-3 —Ä–∞–∑–∞")
            recommendations.append("üõ°Ô∏è –í–∫–ª—é—á–∏—Ç—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
            
        elif metrics.stability_level == StabilityLevel.UNSTABLE:
            recommendations.append("‚ö†Ô∏è –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è - –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é")
            recommendations.append("üìà –°–Ω–∏–∑–∏—Ç—å learning rate –Ω–∞ 10-20%")
            recommendations.append("üîß –£–≤–µ–ª–∏—á–∏—Ç—å guided attention weight")
            
        if metrics.gradient_norm > 5.0:
            recommendations.append(f"üå™Ô∏è –í—ã—Å–æ–∫–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ ({metrics.gradient_norm:.2f}) - —É—Å–∏–ª–∏—Ç—å –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ")
            
        if metrics.lr_volatility > 0.3:
            recommendations.append(f"üìä –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å LR ({metrics.lr_volatility:.3f}) - —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫")
            
        return recommendations


class EmergencyStabilizationSystem:
    """
    üö® Emergency Stabilization System - —Å–∏—Å—Ç–µ–º–∞ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
    
    –ê–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self):
        self.active = False
        self.activation_count = 0
        self.stabilization_history = []
        
        self.logger = logging.getLogger(__name__)
        
    def check_emergency_conditions(self, metrics: StabilityMetrics) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        
        Args:
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            
        Returns:
            bool: –ù—É–∂–Ω–∞ –ª–∏ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è
        """
        emergency_conditions = [
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è —Ç—Ä–µ–±—É—é—Ç ALL —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
            (metrics.stability_level == StabilityLevel.CRITICAL and 
             (metrics.loss_std > 10.0 or metrics.gradient_norm > 15.0)),
            
            # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            metrics.loss_std > 20.0,  # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å loss
            metrics.gradient_norm > 25.0,  # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            metrics.lr_volatility > 0.8,  # –•–∞–æ—Å –≤ learning rate
            (metrics.attention_stability < 0.05 and metrics.gradient_norm > 10.0)  # –ö–æ–ª–ª–∞–ø—Å + –±–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        ]
        
        return any(emergency_conditions)
    
    def activate_emergency_stabilization(self, 
                                       model: nn.Module,
                                       optimizer: torch.optim.Optimizer,
                                       metrics: StabilityMetrics) -> Dict[str, Any]:
        """
        –ê–∫—Ç–∏–≤–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        
        Args:
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            
        Returns:
            Dict[str, Any]: –ü—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –º–µ—Ä—ã —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        """
        if self.active:
            return {'message': 'Emergency stabilization already active'}
        
        self.active = True
        self.activation_count += 1
        
        measures = {}
        
        # 1. –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ learning rate
        for param_group in optimizer.param_groups:
            old_lr = param_group['lr']
            param_group['lr'] = old_lr * 0.1  # –°–Ω–∏–∂–∞–µ–º –≤ 10 —Ä–∞–∑
            measures['lr_reduction'] = f"{old_lr:.2e} ‚Üí {param_group['lr']:.2e}"
        
        # 2. –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        measures['gradient_clipping'] = 1.0
        
        # 3. –í—Ä–µ–º–µ–Ω–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ momentum (–µ—Å–ª–∏ Adam)
        if hasattr(optimizer, 'param_groups'):
            for param_group in optimizer.param_groups:
                if 'betas' in param_group:
                    param_group['betas'] = (0.5, 0.999)  # –°–Ω–∏–∂–∞–µ–º momentum
                    measures['momentum_reduction'] = True
        
        # 4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        self.logger.warning(f"üö® EMERGENCY STABILIZATION ACTIVATED (#{self.activation_count})")
        self.logger.warning(f"   –ü—Ä–∏—á–∏–Ω–∞: {metrics.stability_level.value}")
        self.logger.warning(f"   Loss std: {metrics.loss_std:.3f}")
        self.logger.warning(f"   Gradient norm: {metrics.gradient_norm:.3f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.stabilization_history.append({
            'activation_count': self.activation_count,
            'metrics': metrics,
            'measures': measures
        })
        
        return measures
    
    def deactivate_emergency_stabilization(self,
                                         optimizer: torch.optim.Optimizer,
                                         original_lr: float) -> bool:
        """
        –î–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        
        Args:
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            original_lr: –ò—Å—Ö–æ–¥–Ω—ã–π learning rate
            
        Returns:
            bool: –£—Å–ø–µ—à–Ω–æ –¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞
        """
        if not self.active:
            return False
        
        # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        for param_group in optimizer.param_groups:
            param_group['lr'] = original_lr * 0.5  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º momentum
            if 'betas' in param_group:
                param_group['betas'] = (0.9, 0.999)
        
        self.active = False
        self.logger.info("‚úÖ Emergency stabilization deactivated")
        
        return True


class TrainingStabilizationSystem:
    """
    üõ°Ô∏è Training Stabilization System - –≥–ª–∞–≤–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –ø–æ–ª–Ω–æ–π —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self, hparams):
        self.hparams = hparams
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.gradient_manager = IntelligentGradientManager(
            target_norm=getattr(hparams, 'target_gradient_norm', 2.0),
            max_norm=getattr(hparams, 'max_gradient_norm', 5.0)
        )
        
        self.lr_scheduler = AdaptiveLearningRateScheduler(
            initial_lr=getattr(hparams, 'learning_rate', 1e-3),
            min_lr=getattr(hparams, 'min_learning_rate', 1e-5)
        )
        
        self.stability_monitor = TrainingStabilityMonitor(
            window_size=getattr(hparams, 'stability_window_size', 20)
        )
        
        self.emergency_system = EmergencyStabilizationSystem()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stabilization_stats = {
            'interventions': 0,
            'emergency_activations': 0,
            'stability_improvements': 0
        }
        
        self.logger = logging.getLogger(__name__)
        print("üõ°Ô∏è Training Stabilization System –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω - –ø–æ–ª–Ω–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏!")
    
    def stabilize_training_step(self,
                               model: nn.Module,
                               optimizer: torch.optim.Optimizer,
                               loss: torch.Tensor,
                               attention_quality: float = 0.5) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            model: –ú–æ–¥–µ–ª—å
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            loss: Loss –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            attention_quality: –ö–∞—á–µ—Å—Ç–≤–æ attention
            
        Returns:
            Dict[str, Any]: –û—Ç—á–µ—Ç –æ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
        """
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        gradient_metrics = self.gradient_manager.process_gradients(model, loss)
        
        # 2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        current_lr = optimizer.param_groups[0]['lr']
        stability_metrics = self.stability_monitor.update_metrics(
            loss.item(),
            gradient_metrics['final_norm'],
            current_lr,
            attention_quality
        )
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π
        if self.emergency_system.check_emergency_conditions(stability_metrics):
            emergency_measures = self.emergency_system.activate_emergency_stabilization(
                model, optimizer, stability_metrics
            )
            self.stabilization_stats['emergency_activations'] += 1
        else:
            emergency_measures = None
            
            # –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω—É—é —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é –µ—Å–ª–∏ –±—ã–ª–∞ –∞–∫—Ç–∏–≤–Ω–∞
            if self.emergency_system.active:
                self.emergency_system.deactivate_emergency_stabilization(optimizer, current_lr)
        
        # 4. –ê–¥–∞–ø—Ç–∞—Ü–∏—è learning rate
        new_lr = self.lr_scheduler.step(loss.item(), stability_metrics)
        if abs(new_lr - current_lr) > current_lr * 0.01:  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ >1%
            for param_group in optimizer.param_groups:
                param_group['lr'] = new_lr
            self.stabilization_stats['interventions'] += 1
        
        # 5. –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = {
            'stability_level': stability_metrics.stability_level.value,
            'gradient_metrics': gradient_metrics,
            'stability_metrics': stability_metrics,
            'lr_adjustment': {'old': current_lr, 'new': new_lr},
            'emergency_measures': emergency_measures,
            'recommendations': stability_metrics.stability_level != StabilityLevel.STABLE
        }
        
        return report
    
    def get_system_diagnostics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏"""
        
        return {
            'gradient_manager': {
                'current_scale': self.gradient_manager.current_scale,
                'clip_events_count': len(self.gradient_manager.clip_events),
                'recent_norms': list(self.gradient_manager.gradient_history)[-10:]
            },
            'lr_scheduler': {
                'current_lr': self.lr_scheduler.current_lr,
                'best_loss': self.lr_scheduler.best_loss,
                'patience_counter': self.lr_scheduler.patience_counter
            },
            'stability_monitor': {
                'reports_count': len(self.stability_monitor.stability_reports),
                'recent_stability': [r['metrics'].stability_level.value 
                                   for r in self.stability_monitor.stability_reports[-5:]]
            },
            'emergency_system': {
                'active': self.emergency_system.active,
                'activation_count': self.emergency_system.activation_count
            },
            'statistics': self.stabilization_stats
        }


def create_training_stabilization_system(hparams) -> TrainingStabilizationSystem:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
    
    Args:
        hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        
    Returns:
        TrainingStabilizationSystem: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
    """
    print("üõ°Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ Training Stabilization System...")
    
    system = TrainingStabilizationSystem(hparams)
    
    print("‚úÖ Training Stabilization System —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞!")
    print("üîß –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: Gradient Manager, LR Scheduler, Stability Monitor, Emergency System")
    
    return system


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Training Stabilization System...")
    
    class MockHParams:
        learning_rate = 1e-3
        target_gradient_norm = 2.0
        max_gradient_norm = 5.0
        min_learning_rate = 1e-5
        stability_window_size = 20
    
    hparams = MockHParams()
    system = create_training_stabilization_system(hparams)
    
    print(f"‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è") 