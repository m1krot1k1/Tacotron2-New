#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Learning Rate Adapter –¥–ª—è Tacotron2-New
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–æ–±–ª–µ–º NaN/Inf –≤ Loss –∏ –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ learning rate
- –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Ä–µ–∂–∏–º—ã –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
"""

import torch
import math
import logging
import numpy as np
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

@dataclass
class LRChangeEvent:
    """–°–æ–±—ã—Ç–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è learning rate."""
    step: int
    old_lr: float
    new_lr: float
    reason: str
    trigger_value: Optional[float] = None
    emergency: bool = False

class SmartLRAdapter:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–¥–∞–ø—Ç–µ—Ä learning rate —Å —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–º–∏ —Ä–µ–∂–∏–º–∞–º–∏.
    
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã:
    - NaN/Inf –≤ loss
    - –í–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    - –ü–ª–æ—Ö–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å
    - –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    """
    
    def __init__(self, optimizer: torch.optim.Optimizer, 
                 patience: int = 10, factor: float = 0.5, 
                 min_lr: float = 1e-8, max_lr: float = 1e-3,
                 emergency_factor: float = 0.1,
                 grad_norm_threshold: float = 1000.0,
                 loss_nan_threshold: float = 1e6):
        
        self.optimizer = optimizer
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.emergency_factor = emergency_factor
        self.grad_norm_threshold = grad_norm_threshold
        self.loss_nan_threshold = loss_nan_threshold
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞
        self.bad_epochs = 0
        self.best_loss = float('inf')
        self.emergency_mode = False
        self.emergency_mode_steps = 0
        self.recovery_attempts = 0
        self.max_recovery_attempts = 5
        
        # –ò—Å—Ç–æ—Ä–∏—è
        self.lr_history = []
        self.change_events = []
        self.loss_history = []
        self.grad_norm_history = []
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self._record_initial_lr()
    
    def _record_initial_lr(self):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –Ω–∞—á–∞–ª—å–Ω—ã–π learning rate."""
        initial_lr = self.optimizer.param_groups[0]['lr']
        self.lr_history.append(initial_lr)
        self.logger.info(f"üöÄ Smart LR Adapter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å LR: {initial_lr:.2e}")
    
    def step(self, loss: float, grad_norm: Optional[float] = None, 
             step: int = 0) -> bool:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ learning rate.
        
        Args:
            loss: –¢–µ–∫—É—â–∏–π loss
            grad_norm: –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ LR –±—ã–ª –∏–∑–º–µ–Ω–µ–Ω
        """
        current_loss = loss  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        self.loss_history.append(current_loss)
        if grad_norm is not None:
            self.grad_norm_history.append(grad_norm)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.loss_history) > 1000:
            self.loss_history.pop(0)
        if len(self.grad_norm_history) > 1000:
            self.grad_norm_history.pop(0)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏
        if self._check_emergency_conditions(current_loss, grad_norm):
            return self._handle_emergency(current_loss, grad_norm, step)
        
        # –û–±—ã—á–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
        return self._handle_normal_adaptation(current_loss, step)
    
    def _check_emergency_conditions(self, current_loss: float, 
                                  grad_norm: Optional[float]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ª–æ–≤–∏—è –¥–ª—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞."""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf –≤ loss
        if math.isnan(current_loss) or math.isinf(current_loss) or current_loss > self.loss_nan_threshold:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        if grad_norm is not None:
            if math.isnan(grad_norm) or math.isinf(grad_norm) or grad_norm > self.grad_norm_threshold:
                return True
        
        return False
    
    def _handle_emergency(self, current_loss: float, grad_norm: Optional[float], 
                         step: int) -> bool:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–∏—Ç—É–∞—Ü–∏–∏."""
        self.emergency_mode = True
        self.emergency_mode_steps += 1
        self.recovery_attempts += 1
        
        current_lr = self.optimizer.param_groups[0]['lr']
        
        # –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ LR
        if self.recovery_attempts <= self.max_recovery_attempts:
            new_lr = max(current_lr * self.emergency_factor, self.min_lr)
        else:
            # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ–ø—ã—Ç–æ–∫ - —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –∫ –º–∏–Ω–∏–º—É–º—É
            new_lr = self.min_lr
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        self._apply_lr_change(current_lr, new_lr, step, "EMERGENCY", 
                            trigger_value=current_loss, emergency=True)
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        reason = "NaN/Inf loss" if math.isnan(current_loss) or math.isinf(current_loss) else "Gradient explosion"
        self.logger.warning(f"üö® –≠–ö–°–¢–†–ï–ù–ù–´–ô —Ä–µ–∂–∏–º: {reason} - LR {current_lr:.2e} ‚Üí {new_lr:.2e}")
        
        return True
    
    def _handle_normal_adaptation(self, current_loss: float, step: int) -> bool:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–±—ã—á–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é LR."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.bad_epochs = 0
            
            # –í—ã—Ö–æ–¥ –∏–∑ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
            if self.emergency_mode:
                self._exit_emergency_mode()
        else:
            self.bad_epochs += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∏—è LR
        if self.bad_epochs >= self.patience:
            return self._reduce_lr(step, "PATIENCE")
        
        return False
    
    def _reduce_lr(self, step: int, reason: str) -> bool:
        """–°–Ω–∏–∂–∞–µ—Ç learning rate."""
        current_lr = self.optimizer.param_groups[0]['lr']
        new_lr = max(current_lr * self.factor, self.min_lr)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        self._apply_lr_change(current_lr, new_lr, step, reason)
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫
        self.bad_epochs = 0
        
        self.logger.info(f"üìâ –°–Ω–∏–∂–µ–Ω–∏–µ LR: {current_lr:.2e} ‚Üí {new_lr:.2e} (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
        return True
    
    def _apply_lr_change(self, old_lr: float, new_lr: float, step: int, 
                        reason: str, trigger_value: Optional[float] = None, 
                        emergency: bool = False):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–µ learning rate."""
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ –≥—Ä—É–ø–ø—ã –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = new_lr
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.lr_history.append(new_lr)
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–±—ã—Ç–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        event = LRChangeEvent(
            step=step,
            old_lr=old_lr,
            new_lr=new_lr,
            reason=reason,
            trigger_value=trigger_value,
            emergency=emergency
        )
        self.change_events.append(event)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏ —Å–æ–±—ã—Ç–∏–π
        if len(self.change_events) > 100:
            self.change_events.pop(0)
    
    def _exit_emergency_mode(self):
        """–í—ã—Ö–æ–¥ –∏–∑ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞."""
        if self.emergency_mode:
            self.emergency_mode = False
            self.emergency_mode_steps = 0
            self.recovery_attempts = 0
            self.logger.info("‚úÖ –í—ã—Ö–æ–¥ –∏–∑ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ LR")
    
    def increase_lr(self, factor: float = 1.5, step: int = 0, reason: str = "MANUAL"):
        """–£–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç learning rate."""
        current_lr = self.optimizer.param_groups[0]['lr']
        new_lr = min(current_lr * factor, self.max_lr)
        
        self._apply_lr_change(current_lr, new_lr, step, reason)
        self.logger.info(f"üìà –£–≤–µ–ª–∏—á–µ–Ω–∏–µ LR: {current_lr:.2e} ‚Üí {new_lr:.2e} (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
    
    def set_lr(self, new_lr: float, step: int = 0, reason: str = "MANUAL"):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ learning rate."""
        current_lr = self.optimizer.param_groups[0]['lr']
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
        new_lr = max(min(new_lr, self.max_lr), self.min_lr)
        
        self._apply_lr_change(current_lr, new_lr, step, reason)
        self.logger.info(f"üéØ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ LR: {current_lr:.2e} ‚Üí {new_lr:.2e} (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
    
    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞–¥–∞–ø—Ç–µ—Ä–∞."""
        if not self.lr_history:
            return {
                'current_lr': 0.0,
                'total_changes': 0,
                'emergency_changes': 0,
                'emergency_mode': False,
                'recovery_attempts': 0,
                'avg_loss': 0.0,
                'avg_grad_norm': 0.0
            }
        
        current_lr = self.lr_history[-1]
        emergency_changes = len([e for e in self.change_events if e.emergency])
        
        return {
            'current_lr': current_lr,
            'total_changes': len(self.change_events),
            'emergency_changes': emergency_changes,
            'emergency_mode': self.emergency_mode,
            'emergency_mode_steps': self.emergency_mode_steps,
            'recovery_attempts': self.recovery_attempts,
            'bad_epochs': self.bad_epochs,
            'best_loss': self.best_loss,
            'avg_loss': np.mean(self.loss_history) if self.loss_history else 0.0,
            'avg_grad_norm': np.mean(self.grad_norm_history) if self.grad_norm_history else 0.0,
            'lr_history': self.lr_history[-10:],  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–Ω–∞—á–µ–Ω–∏–π
            'recent_changes': self.change_events[-5:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –∏–∑–º–µ–Ω–µ–Ω–∏–π
        }
    
    def get_recommendations(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        stats = self.get_statistics()
        recommendations = []
        
        if stats['emergency_changes'] > 0:
            recommendations.append("üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è LR - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
        
        if stats['emergency_mode']:
            recommendations.append("üõ°Ô∏è –ê–∫—Ç–∏–≤–µ–Ω —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º LR - –æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º")
        
        if stats['recovery_attempts'] >= self.max_recovery_attempts:
            recommendations.append("‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É")
        
        if stats['avg_grad_norm'] > 10.0:
            recommendations.append("üìà –í—ã—Å–æ–∫–∞—è —Å—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ - —É–≤–µ–ª–∏—á–∏—Ç—å batch size")
        
        if stats['current_lr'] <= self.min_lr:
            recommendations.append("üí° LR –¥–æ—Å—Ç–∏–≥ –º–∏–Ω–∏–º—É–º–∞ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏")
        
        return recommendations
    
    def reset(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞."""
        self.bad_epochs = 0
        self.best_loss = float('inf')
        self.emergency_mode = False
        self.emergency_mode_steps = 0
        self.recovery_attempts = 0
        self.logger.info("üîÑ Smart LR Adapter —Å–±—Ä–æ—à–µ–Ω")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
_global_lr_adapter = None

def get_global_lr_adapter() -> Optional[SmartLRAdapter]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LR –∞–¥–∞–ø—Ç–µ—Ä–∞."""
    return _global_lr_adapter

def set_global_lr_adapter(adapter: SmartLRAdapter):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä LR –∞–¥–∞–ø—Ç–µ—Ä–∞."""
    global _global_lr_adapter
    _global_lr_adapter = adapter 