"""
Early Stop Controller –¥–ª—è Smart Tuner V2
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞ –æ–±—É—á–µ–Ω–∏—è
"""

import yaml
import logging
import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import json
from datetime import datetime
import pandas as pd
import os
import sys

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - (EarlyStopController) - %(message)s')

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
def handle_exception(exc_type, exc_value, exc_traceback):
    if issubclass(exc_type, KeyboardInterrupt):
        sys.__excepthook__(exc_type, exc_value, exc_traceback)
        return
    logging.error("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:", exc_info=(exc_type, exc_value, exc_traceback))

import sys
sys.excepthook = handle_exception

class ProactiveController:
    """–ü—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ä—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, config):
        self.config = config
        self.metrics_history = []
        self.interventions_history = []
        
    def analyze_training_health(self, metrics: Dict) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∑–¥–æ—Ä–æ–≤—å—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –º–µ—Ä"""
        health_report = {
            "status": "healthy",
            "warnings": [],
            "interventions": [],
            "severity": "low"
        }
        
        if not self.metrics_history:
            return health_report
            
        recent_metrics = self.metrics_history[-10:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–Ω–∞—á–µ–Ω–∏–π
        
        # 1. –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏ loss
        train_losses = [m.get('train_loss', 0) for m in recent_metrics if 'train_loss' in m]
        if len(train_losses) >= 5:
            recent_change = abs(train_losses[-1] - train_losses[-5])
            if recent_change < 0.001:  # –û—á–µ–Ω—å –º–∞–ª–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
                health_report["warnings"].append("‚ö†Ô∏è –°—Ç–∞–≥–Ω–∞—Ü–∏—è train_loss")
                health_report["interventions"].append({
                    "type": "learning_rate_boost",
                    "action": "–£–≤–µ–ª–∏—á–∏—Ç—å learning rate –Ω–∞ 20%",
                    "reason": "–ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ –ø–ª–∞—Ç–æ"
                })
                health_report["severity"] = "medium"
        
        # 2. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è train/val loss
        if 'train_loss' in metrics and 'val_loss' in metrics:
            gap = metrics['val_loss'] - metrics['train_loss']
            if gap > 0.5:  # –ë–æ–ª—å—à–æ–π —Ä–∞–∑—Ä—ã–≤
                health_report["warnings"].append("üö® –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
                health_report["interventions"].append({
                    "type": "regularization_boost",
                    "action": "–£–≤–µ–ª–∏—á–∏—Ç—å dropout –¥–æ 0.3",
                    "reason": "–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
                })
                health_report["interventions"].append({
                    "type": "learning_rate_reduce",
                    "action": "–°–Ω–∏–∑–∏—Ç—å learning rate –Ω–∞ 30%",
                    "reason": "–ó–∞–º–µ–¥–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"
                })
                health_report["severity"] = "high"
        
        # 3. –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        if len(train_losses) >= 5:
            volatility = np.std(train_losses[-5:])
            if volatility > 0.1:  # –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
                health_report["warnings"].append("üìà –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
                health_report["interventions"].append({
                    "type": "gradient_clipping",
                    "action": "–í–∫–ª—é—á–∏—Ç—å gradient clipping (max_norm=1.0)",
                    "reason": "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"
                })
                health_report["interventions"].append({
                    "type": "batch_size_increase",
                    "action": "–£–≤–µ–ª–∏—á–∏—Ç—å batch size –Ω–∞ 50%",
                    "reason": "–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"
                })
        
        # 4. –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if len(train_losses) >= 10:
            early_avg = np.mean(train_losses[:5])
            recent_avg = np.mean(train_losses[-5:])
            improvement_rate = (early_avg - recent_avg) / early_avg
            
            if improvement_rate < 0.01:  # –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
                health_report["warnings"].append("üêå –ú–µ–¥–ª–µ–Ω–Ω–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å")
                health_report["interventions"].append({
                    "type": "optimizer_change",
                    "action": "–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ AdamW —Å weight_decay=0.01",
                    "reason": "–£—Å–∫–æ—Ä–µ–Ω–∏–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏"
                })
        
        return health_report
    
    def apply_intervention(self, intervention: Dict) -> Dict[str, Any]:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–π –º–µ—Ä—ã"""
        intervention_type = intervention["type"]
        timestamp = datetime.now().isoformat()
        
        # –ó–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.interventions_history.append({
            "timestamp": timestamp,
            "intervention": intervention,
            "applied": True
        })
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        new_params = {}
        
        if intervention_type == "learning_rate_boost":
            current_lr = self.get_current_lr()
            new_params["learning_rate"] = current_lr * 1.2
            
        elif intervention_type == "learning_rate_reduce":
            current_lr = self.get_current_lr()
            new_params["learning_rate"] = current_lr * 0.7
            
        elif intervention_type == "regularization_boost":
            new_params["dropout"] = 0.3
            new_params["weight_decay"] = 0.01
            
        elif intervention_type == "gradient_clipping":
            new_params["gradient_clip_val"] = 1.0
            new_params["gradient_clip_algorithm"] = "norm"
            
        elif intervention_type == "batch_size_increase":
            current_batch = self.get_current_batch_size()
            new_params["batch_size"] = int(current_batch * 1.5)
            
        elif intervention_type == "optimizer_change":
            new_params["optimizer"] = "AdamW"
            new_params["weight_decay"] = 0.01
            new_params["betas"] = [0.9, 0.999]
        
        return {
            "status": "applied",
            "new_params": new_params,
            "intervention": intervention,
            "timestamp": timestamp
        }
    
    def get_current_lr(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ learning rate"""
        if self.metrics_history:
            return self.metrics_history[-1].get('learning_rate', 0.001)
        return 0.001
    
    def get_current_batch_size(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ batch size"""
        # –ú–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–ª–∏ –º–µ—Ç—Ä–∏–∫
        return self.config.get('hyperparameter_search_space', {}).get('batch_size', {}).get('default', 32)

class EarlyStopController:
    """
    –ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
    """
    
    def __init__(self, config_path: str = "smart_tuner/config.yaml"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞
        
        Args:
            config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.logger = self._setup_logger()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        self.best_metrics = {}
        self.patience_counters = {}
        self.metric_history = {}
        self.should_stop = False
        self.stop_reasons = []
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Å—Ç–∞–Ω–æ–≤–∞
        self._initialize_stop_criteria()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        self.proactive = ProactiveController(self.config)
        
        self.logger.info("EarlyStopController –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–º–∏ –º–µ—Ä–∞–º–∏")
        
    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML —Ñ–∞–π–ª–∞"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            self.logger.error(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {self.config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            raise
        except yaml.YAMLError as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ YAML: {e}")
            raise
            
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞"""
        logger = logging.getLogger('EarlyStopController')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
        
    def _initialize_stop_criteria(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Å—Ç–∞–Ω–æ–≤–∞"""
        early_stop_config = self.config.get('early_stopping', {})
        
        for criterion_name, criterion_config in early_stop_config.items():
            if criterion_config.get('enabled', False):
                metric_name = criterion_config.get('metric', 'val_loss')
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –º–µ—Ç—Ä–∏–∫
                if metric_name not in self.metric_history:
                    self.metric_history[metric_name] = []
                    
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª—É—á—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                if criterion_config.get('mode', 'min') == 'min':
                    self.best_metrics[metric_name] = float('inf')
                else:
                    self.best_metrics[metric_name] = float('-inf')
                    
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤ —Ç–µ—Ä–ø–µ–Ω–∏—è
                self.patience_counters[metric_name] = 0
                
                self.logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∞: {criterion_name}")
                
    def update_metrics(self, metrics: Dict[str, float], step: int) -> bool:
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –æ—Å—Ç–∞–Ω–æ–≤–∞
        
        Args:
            metrics: –°–ª–æ–≤–∞—Ä—å —Å —Ç–µ–∫—É—â–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            True, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
        """
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –º–µ—Ç—Ä–∏–∫
        for metric_name, value in metrics.items():
            if metric_name not in self.metric_history:
                self.metric_history[metric_name] = []
            self.metric_history[metric_name].append({
                'step': step,
                'value': value,
                'timestamp': datetime.now().isoformat()
            })
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –æ—Å—Ç–∞–Ω–æ–≤–∞
        early_stop_config = self.config.get('early_stopping', {})
        
        for criterion_name, criterion_config in early_stop_config.items():
            if criterion_config.get('enabled', False):
                should_stop_by_criterion = self._check_criterion(
                    criterion_name, criterion_config, metrics, step
                )
                
                if should_stop_by_criterion:
                    self.should_stop = True
                    self.stop_reasons.append(f"{criterion_name} (—à–∞–≥ {step})")
                    
        return self.should_stop
        
    def _check_criterion(self, criterion_name: str, config: Dict[str, Any], 
                        metrics: Dict[str, float], step: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∞
        
        Args:
            criterion_name: –ò–º—è –∫—Ä–∏—Ç–µ—Ä–∏—è
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è
            metrics: –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            step: –ù–æ–º–µ—Ä —à–∞–≥–∞
            
        Returns:
            True, –µ—Å–ª–∏ –∫—Ä–∏—Ç–µ—Ä–∏–π —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç
        """
        criterion_type = config.get('type', 'patience')
        
        if criterion_type == 'patience':
            return self._check_patience_criterion(config, metrics)
        elif criterion_type == 'threshold':
            return self._check_threshold_criterion(config, metrics)
        elif criterion_type == 'gradient':
            return self._check_gradient_criterion(config, metrics)
        elif criterion_type == 'plateau':
            return self._check_plateau_criterion(config, metrics)
        elif criterion_type == 'overfitting':
            return self._check_overfitting_criterion(config, metrics)
        elif criterion_type == 'loss_divergence':
            return self._check_divergence_criterion(config, metrics)
        else:
            self.logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫—Ä–∏—Ç–µ—Ä–∏—è: {criterion_type}")
            return False
            
    def _check_patience_criterion(self, config: Dict[str, Any], 
                                metrics: Dict[str, float]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è —Ç–µ—Ä–ø–µ–Ω–∏—è"""
        metric_name = config.get('metric', 'val_loss')
        patience = config.get('patience', 10)
        min_delta = config.get('min_delta', 0.0)
        mode = config.get('mode', 'min')
        
        if metric_name not in metrics:
            return False
            
        current_value = metrics[metric_name]
        best_value = self.best_metrics.get(metric_name)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è
        improved = False
        if mode == 'min':
            if current_value < best_value - min_delta:
                improved = True
                self.best_metrics[metric_name] = current_value
        else:  # mode == 'max'
            if current_value > best_value + min_delta:
                improved = True
                self.best_metrics[metric_name] = current_value
                
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ —Ç–µ—Ä–ø–µ–Ω–∏—è
        if improved:
            self.patience_counters[metric_name] = 0
        else:
            self.patience_counters[metric_name] += 1
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è —Ç–µ—Ä–ø–µ–Ω–∏—è
        if self.patience_counters[metric_name] >= patience:
            self.logger.info(
                f"–ö—Ä–∏—Ç–µ—Ä–∏–π —Ç–µ—Ä–ø–µ–Ω–∏—è —Å—Ä–∞–±–æ—Ç–∞–ª –¥–ª—è {metric_name}: "
                f"{self.patience_counters[metric_name]} >= {patience}"
            )
            return True
            
        return False
        
    def _check_threshold_criterion(self, config: Dict[str, Any], 
                                 metrics: Dict[str, float]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è"""
        metric_name = config.get('metric', 'val_loss')
        threshold = config.get('threshold', 0.0)
        mode = config.get('mode', 'min')
        
        if metric_name not in metrics:
            return False
            
        current_value = metrics[metric_name]
        
        if mode == 'min' and current_value <= threshold:
            self.logger.info(f"–ü–æ—Ä–æ–≥–æ–≤—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π —Å—Ä–∞–±–æ—Ç–∞–ª: {metric_name} <= {threshold}")
            return True
        elif mode == 'max' and current_value >= threshold:
            self.logger.info(f"–ü–æ—Ä–æ–≥–æ–≤—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π —Å—Ä–∞–±–æ—Ç–∞–ª: {metric_name} >= {threshold}")
            return True
            
        return False
        
    def _check_gradient_criterion(self, config: Dict[str, Any], 
                                metrics: Dict[str, float]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (—Å–∫–æ—Ä–æ—Å—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏)"""
        metric_name = config.get('metric', 'val_loss')
        window_size = config.get('window_size', 5)
        gradient_threshold = config.get('gradient_threshold', 1e-4)
        
        if metric_name not in self.metric_history:
            return False
            
        history = self.metric_history[metric_name]
        if len(history) < window_size:
            return False
            
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –æ–∫–Ω–æ
        recent_values = [h['value'] for h in history[-window_size:]]
        gradient = np.mean(np.diff(recent_values))
        
        if abs(gradient) < gradient_threshold:
            self.logger.info(
                f"–ö—Ä–∏—Ç–µ—Ä–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ —Å—Ä–∞–±–æ—Ç–∞–ª: |{gradient:.6f}| < {gradient_threshold}"
            )
            return True
            
        return False
        
    def _check_plateau_criterion(self, config: Dict[str, Any], 
                               metrics: Dict[str, float]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è –ø–ª–∞—Ç–æ"""
        metric_name = config.get('metric', 'val_loss')
        window_size = config.get('window_size', 10)
        plateau_threshold = config.get('plateau_threshold', 0.01)
        
        if metric_name not in self.metric_history:
            return False
            
        history = self.metric_history[metric_name]
        if len(history) < window_size:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏
        recent_values = [h['value'] for h in history[-window_size:]]
        std_dev = np.std(recent_values)
        mean_value = np.mean(recent_values)
        
        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
        cv = std_dev / abs(mean_value) if mean_value != 0 else float('inf')
        
        if cv < plateau_threshold:
            self.logger.info(
                f"–ö—Ä–∏—Ç–µ—Ä–∏–π –ø–ª–∞—Ç–æ —Å—Ä–∞–±–æ—Ç–∞–ª: CV = {cv:.6f} < {plateau_threshold}"
            )
            return True
            
        return False
        
    def _check_overfitting_criterion(self, config: Dict[str, Any], 
                                   metrics: Dict[str, float]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        train_metric = config.get('train_metric', 'train_loss')
        val_metric = config.get('val_metric', 'val_loss')
        overfitting_threshold = config.get('overfitting_threshold', 0.1)
        window_size = config.get('window_size', 5)
        
        if train_metric not in metrics or val_metric not in metrics:
            return False
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É train –∏ val –º–µ—Ç—Ä–∏–∫–∞–º–∏
        train_value = metrics[train_metric]
        val_value = metrics[val_metric]
        
        if train_value <= 0:
            return False
            
        overfitting_ratio = (val_value - train_value) / train_value
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–Ω–¥–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        if (train_metric in self.metric_history and 
            val_metric in self.metric_history):
            
            train_history = self.metric_history[train_metric]
            val_history = self.metric_history[val_metric]
            
            if len(train_history) >= window_size and len(val_history) >= window_size:
                recent_train = [h['value'] for h in train_history[-window_size:]]
                recent_val = [h['value'] for h in val_history[-window_size:]]
                
                # –¢—Ä–µ–Ω–¥: train loss —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è, val loss —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è
                train_trend = np.mean(np.diff(recent_train))
                val_trend = np.mean(np.diff(recent_val))
                
                if train_trend < 0 and val_trend > 0 and overfitting_ratio > overfitting_threshold:
                    self.logger.info(
                        f"–ö—Ä–∏—Ç–µ—Ä–∏–π –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —Å—Ä–∞–±–æ—Ç–∞–ª: "
                        f"ratio = {overfitting_ratio:.4f} > {overfitting_threshold}"
                    )
                    return True
                    
        return False
        
    def _check_divergence_criterion(self, config: Dict[str, Any], 
                                  metrics: Dict[str, float]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è loss"""
        metric_name = config.get('metric', 'train_loss')
        divergence_threshold = config.get('divergence_threshold', 10.0)
        
        if metric_name not in metrics:
            return False
            
        current_value = metrics[metric_name]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏–ª–∏ Inf
        if np.isnan(current_value) or np.isinf(current_value):
            self.logger.info(f"–ö—Ä–∏—Ç–µ—Ä–∏–π —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è —Å—Ä–∞–±–æ—Ç–∞–ª: {metric_name} = {current_value}")
            return True
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞
        if current_value > divergence_threshold:
            self.logger.info(
                f"–ö—Ä–∏—Ç–µ—Ä–∏–π —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è —Å—Ä–∞–±–æ—Ç–∞–ª: "
                f"{current_value} > {divergence_threshold}"
            )
            return True
            
        return False
        
    def get_best_metrics(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç—Ä–∏–∫"""
        return self.best_metrics.copy()
        
    def get_stop_reasons(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω –æ—Å—Ç–∞–Ω–æ–≤–∞"""
        return self.stop_reasons.copy()
        
    def reset(self):
        """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞"""
        self.should_stop = False
        self.stop_reasons = []
        self.best_metrics = {}
        self.patience_counters = {}
        self.metric_history = {}
        
        # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self._initialize_stop_criteria()
        
        self.logger.info("–°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞ —Å–±—Ä–æ—à–µ–Ω–æ")
        
        # –°–±—Ä–æ—Å –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        self.proactive.metrics_history = []
        self.proactive.interventions_history = []
        
    def save_state(self, output_path: str = "smart_tuner/early_stop_state.json"):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        
        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        """
        state = {
            'should_stop': self.should_stop,
            'stop_reasons': self.stop_reasons,
            'best_metrics': self.best_metrics,
            'patience_counters': self.patience_counters,
            'metric_history': self.metric_history,
            'interventions_history': self.proactive.interventions_history
        }
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(state, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_path}")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
            
    def load_state(self, input_path: str = "smart_tuner/early_stop_state.json"):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        
        Args:
            input_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ—Å—Ç–æ—è–Ω–∏—è
        """
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                state = json.load(f)
                
            self.should_stop = state.get('should_stop', False)
            self.stop_reasons = state.get('stop_reasons', [])
            self.best_metrics = state.get('best_metrics', {})
            self.patience_counters = state.get('patience_counters', {})
            self.metric_history = state.get('metric_history', {})
            self.proactive.interventions_history = state.get('interventions_history', [])
            
            self.logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {input_path}")
            
        except FileNotFoundError:
            self.logger.warning(f"–§–∞–π–ª —Å–æ—Å—Ç–æ—è–Ω–∏—è {input_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
            
    def get_status_report(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏
        """
        report = {
            'should_stop': self.should_stop,
            'stop_reasons': self.stop_reasons,
            'best_metrics': self.best_metrics,
            'patience_counters': self.patience_counters,
            'active_criteria': [],
            'proactive_interventions': len(self.proactive.interventions_history)
        }
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö
        early_stop_config = self.config.get('early_stopping', {})
        for criterion_name, criterion_config in early_stop_config.items():
            if criterion_config.get('enabled', False):
                report['active_criteria'].append({
                    'name': criterion_name,
                    'type': criterion_config.get('type', 'patience'),
                    'metric': criterion_config.get('metric', 'val_loss'),
                    'config': criterion_config
                })
                
        return report 

    def should_stop_training(self, current_metrics: Dict[str, float]) -> Tuple[bool, str, Dict]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–ª–µ–¥—É–µ—Ç –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–º–∏ –º–µ—Ä–∞–º–∏.
        
        Args:
            current_metrics: –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            Tuple[bool, str, Dict]: (should_stop, reason, details)
        """
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –º–µ—Ç—Ä–∏–∫ –¥–ª—è –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        timestamped_metrics = {
            **current_metrics,
            'timestamp': datetime.now().isoformat(),
            'epoch': len(self.proactive.metrics_history) + 1
        }
        self.proactive.metrics_history.append(timestamped_metrics)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–µ –º–µ—Ä—ã
        health_report = self.proactive.analyze_training_health(current_metrics)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, –ø—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ä—ã
        if health_report["severity"] == "high" and health_report["interventions"]:
            self.logger.warning(f"üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã: {health_report['warnings']}")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–≤—É—é –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏—é
            intervention_result = self.proactive.apply_intervention(health_report["interventions"][0])
            self.logger.info(f"üõ°Ô∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∞ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–∞—è –º–µ—Ä–∞: {intervention_result}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –æ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–µ, –Ω–æ –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
            return False, "proactive_intervention", {
                "health_report": health_report,
                "intervention": intervention_result,
                "continue_training": True
            }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Å—Ç–∞–Ω–æ–≤–∞ —á–µ—Ä–µ–∑ update_metrics
        step = len(self.proactive.metrics_history)
        should_stop = self.update_metrics(current_metrics, step)
        
        if should_stop:
            reason = f"early_stopping: {', '.join(self.stop_reasons)}"
            details = {
                "stop_reasons": self.stop_reasons,
                "best_metrics": self.best_metrics,
                "patience_counters": self.patience_counters
            }
            self.logger.info(f"üõë –†–µ—à–µ–Ω–∏–µ –æ–± –æ—Å—Ç–∞–Ω–æ–≤–∫–µ: {reason}")
            return True, reason, details
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è, –Ω–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ
        if health_report["warnings"]:
            self.logger.info(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {health_report['warnings']}")
        
        return False, "continue", {"health_report": health_report} 