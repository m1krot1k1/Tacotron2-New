#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AutoFixManager - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è Tacotron2

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
2. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ attention alignment
3. –ê–¥–∞–ø—Ç–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Smart Tuner –∏ Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
"""

import torch
import torch.nn as nn
import numpy as np
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import time

@dataclass
class FixAction:
    """–î–µ–π—Å—Ç–≤–∏–µ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø—Ä–æ–±–ª–µ–º—ã"""
    name: str
    description: str
    priority: int  # 1-10, –≥–¥–µ 10 - –∫—Ä–∏—Ç–∏—á–Ω–æ
    applied: bool = False
    success: bool = False
    timestamp: float = 0.0

class AutoFixManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º
    """
    
    def __init__(self, model: nn.Module, optimizer: torch.optim.Optimizer, 
                 hparams, telegram_monitor=None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        
        Args:
            model: PyTorch –º–æ–¥–µ–ª—å
            optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
            hparams: –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            telegram_monitor: Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
        """
        self.model = model
        self.optimizer = optimizer
        self.hparams = hparams
        self.telegram_monitor = telegram_monitor
        self.logger = logging.getLogger('AutoFixManager')
        
        # –ò—Å—Ç–æ—Ä–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        self.fix_history = []
        self.last_fix_time = 0
        self.fix_cooldown = 60  # 1 –º–∏–Ω—É—Ç–∞ –º–µ–∂–¥—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        self.thresholds = {
            'gradient_vanishing': 1e-8,
            'gradient_explosion': 100.0,
            'attention_diagonality_critical': 0.1,
            'attention_diagonality_warning': 0.3,
            'gate_accuracy_critical': 0.3,
            'loss_critical': 50.0,
            'nan_detected': True
        }
        
        # –°—á–µ—Ç—á–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º
        self.problem_counters = {
            'gradient_vanishing': 0,
            'gradient_explosion': 0,
            'attention_problems': 0,
            'gate_problems': 0,
            'nan_problems': 0
        }
        
        self.logger.info("ü§ñ AutoFixManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def analyze_and_fix(self, step: int, metrics: Dict[str, Any], 
                       loss: Optional[torch.Tensor] = None) -> List[FixAction]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        
        Args:
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            metrics: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
            loss: –¢–µ–Ω–∑–æ—Ä loss (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        """
        current_time = time.time()
        if current_time - self.last_fix_time < self.fix_cooldown:
            return []
        
        applied_fixes = []
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            grad_norm = metrics.get('grad_norm', 0.0)
            if grad_norm < self.thresholds['gradient_vanishing']:
                fixes = self._fix_gradient_vanishing(step, grad_norm, loss)
                applied_fixes.extend(fixes)
                self.problem_counters['gradient_vanishing'] += 1
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            elif grad_norm > self.thresholds['gradient_explosion']:
                fixes = self._fix_gradient_explosion(step, grad_norm)
                applied_fixes.extend(fixes)
                self.problem_counters['gradient_explosion'] += 1
            
            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å attention
            attention_diag = metrics.get('attention_diagonality', 1.0)
            if attention_diag < self.thresholds['attention_diagonality_critical']:
                fixes = self._fix_attention_problems(step, attention_diag)
                applied_fixes.extend(fixes)
                self.problem_counters['attention_problems'] += 1
            
            # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å gate
            gate_accuracy = metrics.get('gate_accuracy', 1.0)
            if gate_accuracy < self.thresholds['gate_accuracy_critical']:
                fixes = self._fix_gate_problems(step, gate_accuracy)
                applied_fixes.extend(fixes)
                self.problem_counters['gate_problems'] += 1
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ NaN/Inf
            if loss is not None and (torch.isnan(loss) or torch.isinf(loss)):
                fixes = self._fix_nan_problems(step, loss)
                applied_fixes.extend(fixes)
                self.problem_counters['nan_problems'] += 1
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            for fix in applied_fixes:
                fix.timestamp = current_time
                fix.applied = True
                self._apply_fix(fix)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            if applied_fixes:
                self.last_fix_time = current_time
                self.fix_history.extend(applied_fixes)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
                self._send_fix_notification(step, applied_fixes)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ analyze_and_fix: {e}")
        
        return applied_fixes
    
    def _fix_gradient_vanishing(self, step: int, grad_norm: float, 
                               loss: Optional[torch.Tensor]) -> List[FixAction]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
        fixes = []
        
        # 1. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ loss
        if loss is not None:
            fix = FixAction(
                name="loss_scaling",
                description=f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ loss –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (grad_norm={grad_norm:.2e})",
                priority=9
            )
            fixes.append(fix)
        
        # 2. –°–Ω–∏–∂–µ–Ω–∏–µ learning rate
        current_lr = self.optimizer.param_groups[0]['lr']
        new_lr = max(current_lr * 0.1, 1e-8)
        
        fix = FixAction(
            name="lr_reduction",
            description=f"–°–Ω–∏–∂–µ–Ω–∏–µ learning rate: {current_lr:.2e} ‚Üí {new_lr:.2e}",
            priority=8
        )
        fixes.append(fix)
        
        # 3. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight
        if hasattr(self.hparams, 'guide_loss_weight'):
            current_weight = getattr(self.hparams, 'guide_loss_weight', 1.0)
            new_weight = min(current_weight * 5.0, 100.0)
            
            fix = FixAction(
                name="guided_attention_boost",
                description=f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight: {current_weight} ‚Üí {new_weight}",
                priority=7
            )
            fixes.append(fix)
        
        return fixes
    
    def _fix_gradient_explosion(self, step: int, grad_norm: float) -> List[FixAction]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤"""
        fixes = []
        
        # 1. –£—Å–∏–ª–µ–Ω–∏–µ gradient clipping
        current_clip = getattr(self.hparams, 'grad_clip_thresh', 1.0)
        new_clip = max(current_clip * 0.1, 0.01)
        
        fix = FixAction(
            name="gradient_clipping",
            description=f"–£—Å–∏–ª–µ–Ω–∏–µ gradient clipping: {current_clip} ‚Üí {new_clip}",
            priority=9
        )
        fixes.append(fix)
        
        # 2. –°–Ω–∏–∂–µ–Ω–∏–µ learning rate
        current_lr = self.optimizer.param_groups[0]['lr']
        new_lr = max(current_lr * 0.5, 1e-8)
        
        fix = FixAction(
            name="lr_reduction",
            description=f"–°–Ω–∏–∂–µ–Ω–∏–µ learning rate: {current_lr:.2e} ‚Üí {new_lr:.2e}",
            priority=8
        )
        fixes.append(fix)
        
        return fixes
    
    def _fix_attention_problems(self, step: int, attention_diag: float) -> List[FixAction]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å attention alignment"""
        fixes = []
        
        # 1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention weight
        if hasattr(self.hparams, 'guide_loss_weight'):
            current_weight = getattr(self.hparams, 'guide_loss_weight', 1.0)
            new_weight = min(current_weight * 10.0, 200.0)
            
            fix = FixAction(
                name="guided_attention_critical_boost",
                description=f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ guided attention: {current_weight} ‚Üí {new_weight}",
                priority=10
            )
            fixes.append(fix)
        
        # 2. –°–Ω–∏–∂–µ–Ω–∏–µ dropout –¥–ª—è attention
        if hasattr(self.hparams, 'p_attention_dropout'):
            current_dropout = getattr(self.hparams, 'p_attention_dropout', 0.1)
            new_dropout = max(current_dropout * 0.1, 0.001)
            
            fix = FixAction(
                name="attention_dropout_reduction",
                description=f"–°–Ω–∏–∂–µ–Ω–∏–µ attention dropout: {current_dropout} ‚Üí {new_dropout}",
                priority=7
            )
            fixes.append(fix)
        
        return fixes
    
    def _fix_gate_problems(self, step: int, gate_accuracy: float) -> List[FixAction]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å gate accuracy"""
        fixes = []
        
        # 1. –°–Ω–∏–∂–µ–Ω–∏–µ gate threshold
        if hasattr(self.hparams, 'gate_threshold'):
            current_threshold = getattr(self.hparams, 'gate_threshold', 0.5)
            new_threshold = max(current_threshold * 0.5, 0.1)
            
            fix = FixAction(
                name="gate_threshold_reduction",
                description=f"–°–Ω–∏–∂–µ–Ω–∏–µ gate threshold: {current_threshold} ‚Üí {new_threshold}",
                priority=6
            )
            fixes.append(fix)
        
        # 2. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤–µ—Å–∞ gate loss
        if hasattr(self.hparams, 'gate_loss_weight'):
            current_weight = getattr(self.hparams, 'gate_loss_weight', 1.0)
            new_weight = min(current_weight * 2.0, 10.0)
            
            fix = FixAction(
                name="gate_loss_boost",
                description=f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –≤–µ—Å–∞ gate loss: {current_weight} ‚Üí {new_weight}",
                priority=5
            )
            fixes.append(fix)
        
        return fixes
    
    def _fix_nan_problems(self, step: int, loss: torch.Tensor) -> List[FixAction]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ NaN/Inf –ø—Ä–æ–±–ª–µ–º"""
        fixes = []
        
        # 1. –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ learning rate
        current_lr = self.optimizer.param_groups[0]['lr']
        new_lr = max(current_lr * 0.01, 1e-10)
        
        fix = FixAction(
            name="emergency_lr_reduction",
            description=f"–≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ LR: {current_lr:.2e} ‚Üí {new_lr:.2e}",
            priority=10
        )
        fixes.append(fix)
        
        # 2. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è guided attention
        if hasattr(self.hparams, 'use_guided_attn'):
            fix = FixAction(
                name="force_guided_attention",
                description="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è guided attention",
                priority=9
            )
            fixes.append(fix)
        
        # 3. –û—Ç–∫–ª—é—á–µ–Ω–∏–µ fp16
        if hasattr(self.hparams, 'fp16_run') and self.hparams.fp16_run:
            fix = FixAction(
                name="disable_fp16",
                description="–û—Ç–∫–ª—é—á–µ–Ω–∏–µ fp16 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏",
                priority=8
            )
            fixes.append(fix)
        
        return fixes
    
    def _apply_fix(self, fix: FixAction):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"""
        try:
            if fix.name == "lr_reduction":
                current_lr = self.optimizer.param_groups[0]['lr']
                new_lr = max(current_lr * 0.1, 1e-8)
                for group in self.optimizer.param_groups:
                    group['lr'] = new_lr
                fix.success = True
                
            elif fix.name == "gradient_clipping":
                new_clip = max(getattr(self.hparams, 'grad_clip_thresh', 1.0) * 0.1, 0.01)
                setattr(self.hparams, 'grad_clip_thresh', new_clip)
                fix.success = True
                
            elif fix.name == "guided_attention_boost":
                if hasattr(self.hparams, 'guide_loss_weight'):
                    current_weight = getattr(self.hparams, 'guide_loss_weight', 1.0)
                    new_weight = min(current_weight * 5.0, 100.0)
                    setattr(self.hparams, 'guide_loss_weight', new_weight)
                    fix.success = True
                    
            elif fix.name == "guided_attention_critical_boost":
                if hasattr(self.hparams, 'guide_loss_weight'):
                    current_weight = getattr(self.hparams, 'guide_loss_weight', 1.0)
                    new_weight = min(current_weight * 10.0, 200.0)
                    setattr(self.hparams, 'guide_loss_weight', new_weight)
                    fix.success = True
                    
            elif fix.name == "attention_dropout_reduction":
                if hasattr(self.hparams, 'p_attention_dropout'):
                    current_dropout = getattr(self.hparams, 'p_attention_dropout', 0.1)
                    new_dropout = max(current_dropout * 0.1, 0.001)
                    setattr(self.hparams, 'p_attention_dropout', new_dropout)
                    fix.success = True
                    
            elif fix.name == "gate_threshold_reduction":
                if hasattr(self.hparams, 'gate_threshold'):
                    current_threshold = getattr(self.hparams, 'gate_threshold', 0.5)
                    new_threshold = max(current_threshold * 0.5, 0.1)
                    setattr(self.hparams, 'gate_threshold', new_threshold)
                    fix.success = True
                    
            elif fix.name == "emergency_lr_reduction":
                current_lr = self.optimizer.param_groups[0]['lr']
                new_lr = max(current_lr * 0.01, 1e-10)
                for group in self.optimizer.param_groups:
                    group['lr'] = new_lr
                fix.success = True
                
            elif fix.name == "force_guided_attention":
                setattr(self.hparams, 'use_guided_attn', True)
                setattr(self.hparams, 'guide_loss_weight', 100.0)
                fix.success = True
                
            elif fix.name == "disable_fp16":
                setattr(self.hparams, 'fp16_run', False)
                fix.success = True
            
            if fix.success:
                self.logger.info(f"‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {fix.description}")
            else:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {fix.name}")
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è {fix.name}: {e}")
            fix.success = False
    
    def _send_fix_notification(self, step: int, fixes: List[FixAction]):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö"""
        if not self.telegram_monitor:
            return
        
        try:
            message = f"üîß **–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø**\n\n"
            message += f"üìç **–®–∞–≥:** {step}\n"
            message += f"üïê **–í—Ä–µ–º—è:** {time.strftime('%H:%M:%S')}\n\n"
            
            message += f"üìã **–ü—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**\n"
            for fix in fixes:
                status = "‚úÖ" if fix.success else "‚ùå"
                message += f"{status} {fix.description}\n"
            
            message += f"\nü§ñ **–°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥...**"
            
            self.telegram_monitor.send_message(message)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
    
    def get_fix_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π"""
        return {
            'total_fixes': len(self.fix_history),
            'successful_fixes': len([f for f in self.fix_history if f.success]),
            'problem_counters': self.problem_counters.copy(),
            'recent_fixes': self.fix_history[-10:] if self.fix_history else []
        }
    
    def reset_counters(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—á–µ—Ç—á–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º"""
        for key in self.problem_counters:
            self.problem_counters[key] = 0 