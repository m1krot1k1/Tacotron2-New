import logging
import subprocess
import os
import time
import requests
import sys
from datetime import datetime
import mlflow
from smart_tuner.alert_manager import AlertManager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - (TrainerWrapper) - %(message)s'
)

class TrainerWrapper:
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è train.py.
    """
    def __init__(self, config: dict):
        try:
            self.config = config
            logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TrainerWrapper —Å config —Ç–∏–ø–∞: {type(config)}")
            
            self.python_executable = self.config.get('training', {}).get('python_executable', 'python')
            self.train_script = self.config.get('training', {}).get('script_path', 'train.py')
            self.output_dir = self.config.get('output_dir', 'output')
            self.current_process = None
            self.current_run_id = None
            self.alert_manager = AlertManager(config)
            self.ensure_mlflow_running()
            logging.info("TrainerWrapper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π.")
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TrainerWrapper: {e}")
            raise

    def _construct_hparams_string(self, hparams_override):
        """–ö–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤."""
        if not hparams_override:
            return ""
        
        if isinstance(hparams_override, str):
            return hparams_override
        
        if hasattr(hparams_override, 'values') and callable(getattr(hparams_override, 'values')):
            hparams_dict = hparams_override.values()
        elif isinstance(hparams_override, dict):
            hparams_dict = hparams_override
        else:
            raise TypeError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–ª—è hparams_override: {type(hparams_override)}")

        return ",".join([f"{key}={value}" for key, value in hparams_dict.items()])

    def is_mlflow_running(self, port=5000):
        try:
            requests.get(f"http://localhost:{port}", timeout=3)
            return True
        except requests.ConnectionError:
            return False

    def ensure_mlflow_running(self):
        # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ MLflow
        if not self.is_mlflow_running():
            logging.warning("MLflow UI –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø—É—Å—Ç–∏—Ç–µ –µ–≥–æ —á–µ—Ä–µ–∑ install.sh.")
        else:
            logging.info("‚úÖ MLflow —É–∂–µ –∑–∞–ø—É—â–µ–Ω")

    def start_training(self, hparams_override=None, checkpoint_path=None, run_name_prefix="proactive_run"):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç train.py —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å, MLflow run_id –∏ –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º.
        """
        run_name = f"{run_name_prefix}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
        
        output_dir = os.path.join(self.config.get('output_dir', 'output'), run_name)
        log_dir = os.path.join(output_dir, "logs")
        checkpoint_dir = os.path.join(output_dir, "checkpoint")

        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        mlflow.set_tracking_uri(self.config.get('mlflow', {}).get('tracking_uri', 'file:./mlruns'))
        mlflow.set_experiment(self.config.get('experiment_name', 'tacotron2_production'))
        
        active_run = mlflow.start_run(run_name=run_name)
        self.current_run_id = active_run.info.run_id
        logging.info(f"–°–æ–∑–¥–∞–Ω MLflow run: {run_name} —Å ID: {self.current_run_id}")

        command = [
            self.python_executable, self.train_script,
            '--output_directory', output_dir,
            '--log_directory', log_dir,
        ]

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ hparams –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
        hparams_dict = {}
        if hasattr(hparams_override, 'values') and callable(getattr(hparams_override, 'values')):
            hparams_dict = hparams_override.values()
        elif isinstance(hparams_override, dict):
            hparams_dict = hparams_override.copy()

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ distributed_run –≤—ã–∫–ª—é—á–µ–Ω
        hparams_dict['distributed_run'] = 'False'
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è HParams.parse()
        hparams_parts = []
        for key, value in hparams_dict.items():
            if isinstance(value, list):
                # –°–ø–∏—Å–∫–∏ –ø–µ—Ä–µ–¥–∞–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ name=[val1,val2,val3]
                formatted_list = "[" + ",".join(str(v) for v in value) + "]"
                hparams_parts.append(f"{key}={formatted_list}")
            else:
                hparams_parts.append(f"{key}={value}")
        
        hparams_str = ",".join(hparams_parts)
        
        command.extend(["--hparams", hparams_str])

        if checkpoint_path:
            command.extend(["--checkpoint_path", checkpoint_path])

        try:
            env = os.environ.copy()
            env['MLFLOW_RUN_ID'] = self.current_run_id
            
            self.current_process = subprocess.Popen(command, env=env)
            logging.info(f"–ó–∞–ø—É—â–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å PID: {self.current_process.pid}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
            return self.current_process, self.current_run_id, output_dir, log_dir

        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
            mlflow.end_run()
            return None, None, None, None

    def stop_training(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è."""
        if self.current_process and self.current_process.poll() is None:
            logging.info(f"–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å PID: {self.current_process.pid}")
            self.current_process.terminate()
            try:
                self.current_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                logging.warning("–ü—Ä–æ—Ü–µ—Å—Å –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è, —É–±–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ.")
                self.current_process.kill()
            logging.info("–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
        else:
            logging.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
            
        # –ó–∞–≤–µ—Ä—à–∞–µ–º MLflow run –µ—Å–ª–∏ –æ–Ω –µ—â–µ –∞–∫—Ç–∏–≤–µ–Ω
        if self.current_run_id:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã–π run
                active_run = mlflow.active_run()
                if active_run and active_run.info.run_id == self.current_run_id:
                    mlflow.end_run()
                    logging.info(f"MLflow run {self.current_run_id} –∑–∞–≤–µ—Ä—à–µ–Ω.")
            except Exception as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≤–µ—Ä—à–∏—Ç—å MLflow run: {e}")
            finally:
                self.current_run_id = None

    def train_with_params(self, hyperparams: dict):
        """
        –§—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna: –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–∞ —Å –Ω–∞—à–∏–º —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–º EarlyStopController.
        """
        from smart_tuner.early_stop_controller import EarlyStopController
        from smart_tuner.log_watcher import LogWatcher
        from smart_tuner.metrics_store import MetricsStore
        
        logging.info(f"üß™ –ó–∞–ø—É—Å–∫ Optuna trial —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {hyperparams}")
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è —ç—Ç–æ–≥–æ trial
        trial_name = f"optuna_trial_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        process, run_id, output_dir, log_dir = self.start_training(
            hparams_override=hyperparams,
            checkpoint_path=None,  # Optuna trials –Ω–∞—á–∏–Ω–∞—é—Ç —Å –Ω—É–ª—è
            run_name_prefix="optuna_trial"
        )
        
        if not process or not run_id:
            logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è Optuna trial")
            return None
            
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—à —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
        controller = EarlyStopController()
        metrics_store = MetricsStore()
        log_watcher = LogWatcher(
            metrics_store=metrics_store,
            tracking_uri=self.config.get('mlflow', {}).get('tracking_uri', 'mlruns')
        )
        log_watcher.set_run_id(run_id)
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        final_metrics = None
        check_interval = 30  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è Optuna
        
        try:
            while process.poll() is None:
                # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                log_watcher.check_for_new_metrics()
                raw_metrics = metrics_store.get_latest_metrics()
                
                if raw_metrics:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –Ω–∞—à–µ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
                    converted_metrics = self._convert_metrics_for_optuna(raw_metrics)
                    if converted_metrics:
                        controller.add_metrics(converted_metrics)
                        final_metrics = converted_metrics  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å trial
                        decision = controller.decide_next_step(hyperparams)
                        if decision.get('action') == 'stop':
                            logging.info(f"EarlyStopController —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å trial: {decision.get('reason')}")
                            self.stop_training()
                            break
                
                time.sleep(check_interval)
                
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
            if process.poll() is None:
                process.wait(timeout=60)
                
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Optuna trial: {e}")
            self.stop_training()
            return None
        finally:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if process.poll() is None:
                self.stop_training()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Optuna
        if final_metrics:
            logging.info(f"‚úÖ Trial –∑–∞–≤–µ—Ä—à–µ–Ω. –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}")
            return final_metrics
        else:
            logging.warning("‚ùå Trial –∑–∞–≤–µ—Ä—à–µ–Ω –±–µ–∑ –º–µ—Ç—Ä–∏–∫")
            return {"val_loss": float('inf')}  # –ë–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –Ω–µ—É–¥–∞—á–Ω–æ–≥–æ trial
    
    def _convert_metrics_for_optuna(self, raw_metrics: dict) -> dict:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ MetricsStore –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è EarlyStopController.
        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ç–æ–¥—É –≤ SmartTunerMain, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è TrainerWrapper.
        """
        if not raw_metrics:
            return None
            
        metrics_mapping = {
            'training.loss': 'train_loss',
            'validation.loss': 'val_loss', 
            'grad_norm': 'grad_norm'
        }
        
        converted = {}
        for mlflow_name, advisor_name in metrics_mapping.items():
            if mlflow_name in raw_metrics:
                value = raw_metrics[mlflow_name]
                if isinstance(value, list) and len(value) > 0:
                    converted[advisor_name] = float(value[-1])
                elif isinstance(value, (int, float)):
                    converted[advisor_name] = float(value)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç—Ä–∏–∫
        required_metrics = ['train_loss', 'val_loss', 'grad_norm']
        if all(metric in converted for metric in required_metrics):
            return converted
        else:
            missing = [m for m in required_metrics if m not in converted]
            logging.debug(f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è EarlyStopController: {missing}")
            return None 