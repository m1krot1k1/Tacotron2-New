import logging
import subprocess
import os
import time
import requests
import sys
from datetime import datetime
import mlflow
from smart_tuner.alert_manager import AlertManager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - (TrainerWrapper) - %(message)s'
)

class TrainerWrapper:
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è train.py.
    """
    def __init__(self, config: dict):
        try:
            self.config = config
            logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TrainerWrapper —Å config —Ç–∏–ø–∞: {type(config)}")
            
            self.python_executable = self.config.get('training', {}).get('python_executable', 'python')
            self.train_script = self.config.get('training', {}).get('script_path', 'train.py')
            self.output_dir = self.config.get('output_dir', 'output')
            self.current_process = None
            self.current_run_id = None
            self.alert_manager = AlertManager(config)
            self.ensure_mlflow_running()
            logging.info("TrainerWrapper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π.")
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TrainerWrapper: {e}")
            raise

    def _construct_hparams_string(self, hparams_override):
        """–ö–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤."""
        if not hparams_override:
            return ""
        
        if isinstance(hparams_override, str):
            return hparams_override
        
        if hasattr(hparams_override, 'values') and callable(getattr(hparams_override, 'values')):
            hparams_dict = hparams_override.values()
        elif isinstance(hparams_override, dict):
            hparams_dict = hparams_override
        else:
            raise TypeError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–ª—è hparams_override: {type(hparams_override)}")

        return ",".join([f"{key}={value}" for key, value in hparams_dict.items()])

    def is_mlflow_running(self, port=5000):
        try:
            requests.get(f"http://localhost:{port}", timeout=3)
            return True
        except requests.ConnectionError:
            return False

    def ensure_mlflow_running(self):
        # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ MLflow
        if not self.is_mlflow_running():
            logging.warning("MLflow UI –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø—É—Å—Ç–∏—Ç–µ –µ–≥–æ —á–µ—Ä–µ–∑ install.sh.")
        else:
            logging.info("‚úÖ MLflow —É–∂–µ –∑–∞–ø—É—â–µ–Ω")

    def start_training(self, hparams_override=None, checkpoint_path=None, run_name_prefix="proactive_run"):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç train.py —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å, MLflow run_id –∏ –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º.
        """
        run_name = f"{run_name_prefix}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
        
        output_dir = os.path.join(self.config.get('output_dir', 'output'), run_name)
        log_dir = os.path.join(output_dir, "logs")
        checkpoint_dir = os.path.join(output_dir, "checkpoint")

        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π run –µ—Å–ª–∏ –æ–Ω –∞–∫—Ç–∏–≤–µ–Ω
        if mlflow.active_run():
            mlflow.end_run()
            
        mlflow.set_tracking_uri(self.config.get('mlflow', {}).get('tracking_uri', 'file:./mlruns'))
        mlflow.set_experiment(self.config.get('experiment_name', 'tacotron2_production'))
        
        active_run = mlflow.start_run(run_name=run_name)
        self.current_run_id = active_run.info.run_id
        logging.info(f"–°–æ–∑–¥–∞–Ω MLflow run: {run_name} —Å ID: {self.current_run_id}")

        command = [
            self.python_executable, self.train_script,
            '--output_directory', output_dir,
            '--log_directory', log_dir,
        ]

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ hparams –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
        hparams_dict = {}
        if isinstance(hparams_override, dict):
            hparams_dict = hparams_override.copy()
        elif hasattr(hparams_override, 'values') and callable(getattr(hparams_override, 'values')):
            # –ï—Å–ª–∏ —ç—Ç–æ HParams object, –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            try:
                hparams_dict = vars(hparams_override)
            except:
                hparams_dict = {}
        else:
            hparams_dict = {}

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ distributed_run –≤—ã–∫–ª—é—á–µ–Ω
        hparams_dict['distributed_run'] = 'False'
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è HParams.parse()
        hparams_parts = []
        for key, value in hparams_dict.items():
            if isinstance(value, list):
                # –°–ø–∏—Å–∫–∏ –ø–µ—Ä–µ–¥–∞–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ name=[val1,val2,val3]
                formatted_list = "[" + ",".join(str(v) for v in value) + "]"
                hparams_parts.append(f"{key}={formatted_list}")
            else:
                hparams_parts.append(f"{key}={value}")
        
        hparams_str = ",".join(hparams_parts)
        
        command.extend(["--hparams", hparams_str])

        if checkpoint_path:
            command.extend(["--checkpoint_path", checkpoint_path])

        try:
            env = os.environ.copy()
            env['MLFLOW_RUN_ID'] = self.current_run_id
            
            self.current_process = subprocess.Popen(command, env=env)
            logging.info(f"–ó–∞–ø—É—â–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å PID: {self.current_process.pid}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
            return self.current_process, self.current_run_id, output_dir, log_dir

        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
            mlflow.end_run()
            return None, None, None, None

    def stop_training(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è."""
        if self.current_process and self.current_process.poll() is None:
            logging.info(f"–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å PID: {self.current_process.pid}")
            self.current_process.terminate()
            try:
                self.current_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                logging.warning("–ü—Ä–æ—Ü–µ—Å—Å –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è, —É–±–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ.")
                self.current_process.kill()
            logging.info("–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
        else:
            logging.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
            
        # –ó–∞–≤–µ—Ä—à–∞–µ–º MLflow run –µ—Å–ª–∏ –æ–Ω –µ—â–µ –∞–∫—Ç–∏–≤–µ–Ω
        if self.current_run_id:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã–π run
                active_run = mlflow.active_run()
                if active_run and active_run.info.run_id == self.current_run_id:
                    mlflow.end_run()
                    logging.info(f"MLflow run {self.current_run_id} –∑–∞–≤–µ—Ä—à–µ–Ω.")
            except Exception as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≤–µ—Ä—à–∏—Ç—å MLflow run: {e}")
            finally:
                self.current_run_id = None

    def train_with_params(self, hyperparams: dict, trial=None, **kwargs):
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –¥–ª—è Optuna, —Ç–∞–∫ –∏ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
        """
        logging.info(f"üß™ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {hyperparams}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è
        name_prefix = "trial" if trial else "single_training"
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        process, run_id, output_dir, log_dir = self.start_training(
            hparams_override=hyperparams,
            checkpoint_path=None,
            run_name_prefix=name_prefix
        )
        
        if not process or not run_id:
            logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è")
            return {"val_loss": float('inf'), "error": "failed_to_start"}
            
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        # ------------------------------
        # ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        training_cfg = self.config.get('training', {})
        check_interval = training_cfg.get('metrics_poll_interval', 30)  # —Å–µ–∫

        final_metrics = {}
        last_step = 0

        # –ö–ª–∏–µ–Ω—Ç –¥–ª—è –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ MLflow
        from mlflow.tracking import MlflowClient
        client = MlflowClient()

        def _fetch_latest_mlflow_metrics(rid: str):
            """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –º–µ—Ç—Ä–∏–∫ –∏–∑ MLflow."""
            try:
                metric_keys = [
                    "training.loss", "validation.loss", "grad_norm",
                    "attention_alignment_score", "gate_accuracy", "mel_quality_score",
                    "training.step", "validation.step"
                ]
                latest = {}
                for key in metric_keys:
                    try:
                        hist = client.get_metric_history(rid, key)
                    except Exception:
                        hist = []
                    if hist:
                        # MLflow –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ—Ä—è–¥–æ–∫, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ step
                        last_m = max(hist, key=lambda m: m.step)
                        latest[key] = (last_m.value, last_m.step)
                if not latest:
                    return None
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∫ Optimizer/EarlyStopController
                converted = {}
                mapping = {
                    "training.loss": "train_loss",
                    "validation.loss": "val_loss",
                    "grad_norm": "grad_norm",
                    "attention_alignment_score": "attention_alignment_score",
                    "gate_accuracy": "gate_accuracy",
                    "mel_quality_score": "mel_quality_score",
                }
                for ml_name, (val, step) in latest.items():
                    if ml_name in mapping:
                        converted[mapping[ml_name]] = val
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º step –¥–ª—è train/val
                    if ml_name in ("validation.step", "training.step"):
                        converted["step"] = step
                return converted
            except Exception as e:
                logging.debug(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∏–∑ MLflow: {e}")
                return None
        
        try:
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
            while process.poll() is None:
                time.sleep(check_interval)

                real_metrics = _fetch_latest_mlflow_metrics(run_id)
                if not real_metrics:
                    logging.debug("–ù–µ—Ç –Ω–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –≤ MLflow (–ø–æ–∫–∞). –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–∂–∏–¥–∞–Ω–∏–µ...")
                    continue

                current_step = real_metrics.get("step", last_step)

                # –û—Ç—á–∏—Ç—ã–≤–∞–µ–º—Å—è –≤ Optuna –µ—Å–ª–∏ —ç—Ç–æ trial
                if trial and current_step != last_step:
                    composite_score = self._calculate_composite_score(real_metrics)
                    try:
                        from smart_tuner.optimization_engine import OptimizationEngine
                        opt_engine = OptimizationEngine()
                        opt_engine.report_intermediate_value(trial, current_step, composite_score, real_metrics)
                    except Exception as e:
                        logging.warning(f"–û—à–∏–±–∫–∞ –æ—Ç—á–µ—Ç–∞ –≤ Optuna: {e}")

                final_metrics = real_metrics
                last_step = current_step
                
                # ------------------------------
                # üõë DEMO-MODE BREAK (—É–¥–∞–ª–µ–Ω–æ)
                # –†–∞–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞—Å–∏–ª—å–Ω–æ –ø—Ä–µ—Ä—ã–≤–∞–ª–æ—Å—å –ø–æ—Å–ª–µ 200 –ø—Å–µ–≤–¥–æ-—à–∞–≥–æ–≤,
                # —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–∞–ø–æ–≤ (~2.5 –º–∏–Ω).
                # –¢–µ–ø–µ—Ä—å —ç—Ç–∞ –ª–æ–≥–∏–∫–∞ –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º—ã–π –ø—Ä–µ–¥–µ–ª,
                # –∫–æ—Ç–æ—Ä—ã–π –ø–æ-—É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–∫–ª—é—á—ë–Ω. –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≥–∏–±–∫–æ—Å—Ç—å ‚Äî
                # –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –¥–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è train.py
                # –ª–∏–±–æ –¥–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è EarlyStopController.
                max_demo_steps = self.config.get('training', {}).get('max_demo_steps')
                if max_demo_steps is not None and current_step > max_demo_steps:
                    logging.info(
                        f"‚èπ Demo-mode: –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {max_demo_steps} —à–∞–≥–æ–≤, –ø—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ"
                    )
                    break
                
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
            if process.poll() is None:
                logging.info("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –ø–æ —Ç–∞–π–º–∞—É—Ç—É")
                self.stop_training()
                
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            self.stop_training()
            return {"val_loss": float('inf'), "error": str(e)}
        finally:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if process.poll() is None:
                self.stop_training()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if final_metrics:
            logging.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}")
            return final_metrics
        else:
            logging.warning("‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –±–µ–∑ –º–µ—Ç—Ä–∏–∫")
            return {"val_loss": float('inf'), "error": "no_metrics"}

    def _calculate_composite_score(self, metrics: dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –¥–ª—è Optuna"""
        val_loss = metrics.get('val_loss', 10.0)
        attention_score = metrics.get('attention_alignment_score', 0.0)
        gate_accuracy = metrics.get('gate_accuracy', 0.0)
        mel_quality = metrics.get('mel_quality_score', 0.0)
        
        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        composite = (
            val_loss * 0.4 +
            (1 - attention_score) * 0.3 +
            (1 - gate_accuracy) * 0.2 +
            (1 - mel_quality) * 0.1
        )
        
        return composite
    
    def _convert_metrics_for_optuna(self, raw_metrics: dict) -> dict:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ MetricsStore –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è EarlyStopController.
        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ç–æ–¥—É –≤ SmartTunerMain, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è TrainerWrapper.
        """
        if not raw_metrics:
            return None
            
        metrics_mapping = {
            'training.loss': 'train_loss',
            'validation.loss': 'val_loss', 
            'grad_norm': 'grad_norm'
        }
        
        converted = {}
        for mlflow_name, advisor_name in metrics_mapping.items():
            if mlflow_name in raw_metrics:
                value = raw_metrics[mlflow_name]
                if isinstance(value, list) and len(value) > 0:
                    converted[advisor_name] = float(value[-1])
                elif isinstance(value, (int, float)):
                    converted[advisor_name] = float(value)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç—Ä–∏–∫
        required_metrics = ['train_loss', 'val_loss', 'grad_norm']
        if all(metric in converted for metric in required_metrics):
            return converted
        else:
            missing = [m for m in required_metrics if m not in converted]
            logging.debug(f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è EarlyStopController: {missing}")
            return None 