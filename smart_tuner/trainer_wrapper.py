import logging
import subprocess
import os
import time
import requests
import sys
from datetime import datetime
import mlflow
from smart_tuner.alert_manager import AlertManager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - (TrainerWrapper) - %(message)s'
)

class TrainerWrapper:
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è train.py.
    """
    def __init__(self, config: dict):
        try:
            self.config = config
            logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TrainerWrapper —Å config —Ç–∏–ø–∞: {type(config)}")
            
            self.python_executable = self.config.get('training', {}).get('python_executable', 'python')
            self.train_script = self.config.get('training', {}).get('script_path', 'train.py')
            self.output_dir = self.config.get('output_dir', 'output')
            self.current_process = None
            self.current_run_id = None
            self.alert_manager = AlertManager(config)
            self.ensure_mlflow_running()
            logging.info("TrainerWrapper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π.")
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TrainerWrapper: {e}")
            raise

    def _construct_hparams_string(self, hparams_override):
        """–ö–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤."""
        if not hparams_override:
            return ""
        
        if isinstance(hparams_override, str):
            return hparams_override
        
        if hasattr(hparams_override, 'values') and callable(getattr(hparams_override, 'values')):
            hparams_dict = hparams_override.values()
        elif isinstance(hparams_override, dict):
            hparams_dict = hparams_override
        else:
            raise TypeError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–ª—è hparams_override: {type(hparams_override)}")

        return ",".join([f"{key}={value}" for key, value in hparams_dict.items()])

    def is_mlflow_running(self, port=5000):
        try:
            requests.get(f"http://localhost:{port}", timeout=3)
            return True
        except requests.ConnectionError:
            return False

    def ensure_mlflow_running(self):
        # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ MLflow
        if not self.is_mlflow_running():
            logging.warning("MLflow UI –Ω–µ –∑–∞–ø—É—â–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø—É—Å—Ç–∏—Ç–µ –µ–≥–æ —á–µ—Ä–µ–∑ install.sh.")
        else:
            logging.info("‚úÖ MLflow —É–∂–µ –∑–∞–ø—É—â–µ–Ω")

    def start_training(self, hparams_override=None, checkpoint_path=None, run_name_prefix="proactive_run"):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç train.py —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å, MLflow run_id –∏ –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º.
        """
        run_name = f"{run_name_prefix}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}"
        
        output_dir = os.path.join(self.config.get('output_dir', 'output'), run_name)
        log_dir = os.path.join(output_dir, "logs")
        checkpoint_dir = os.path.join(output_dir, "checkpoint")

        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(log_dir, exist_ok=True)
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π run –µ—Å–ª–∏ –æ–Ω –∞–∫—Ç–∏–≤–µ–Ω
        if mlflow.active_run():
            mlflow.end_run()
            
        mlflow.set_tracking_uri(self.config.get('mlflow', {}).get('tracking_uri', 'file:./mlruns'))
        mlflow.set_experiment(self.config.get('experiment_name', 'tacotron2_production'))
        
        active_run = mlflow.start_run(run_name=run_name)
        self.current_run_id = active_run.info.run_id
        logging.info(f"–°–æ–∑–¥–∞–Ω MLflow run: {run_name} —Å ID: {self.current_run_id}")

        command = [
            self.python_executable, self.train_script,
            '--output_directory', output_dir,
            '--log_directory', log_dir,
        ]

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ hparams –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
        hparams_dict = {}
        if isinstance(hparams_override, dict):
            hparams_dict = hparams_override.copy()
        elif hasattr(hparams_override, 'values') and callable(getattr(hparams_override, 'values')):
            # –ï—Å–ª–∏ —ç—Ç–æ HParams object, –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã
            try:
                hparams_dict = vars(hparams_override)
            except:
                hparams_dict = {}
        else:
            hparams_dict = {}

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ distributed_run –≤—ã–∫–ª—é—á–µ–Ω
        hparams_dict['distributed_run'] = 'False'
        
        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è HParams.parse()
        hparams_parts = []
        for key, value in hparams_dict.items():
            if isinstance(value, list):
                # –°–ø–∏—Å–∫–∏ –ø–µ—Ä–µ–¥–∞–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ name=[val1,val2,val3]
                formatted_list = "[" + ",".join(str(v) for v in value) + "]"
                hparams_parts.append(f"{key}={formatted_list}")
            else:
                hparams_parts.append(f"{key}={value}")
        
        hparams_str = ",".join(hparams_parts)
        
        command.extend(["--hparams", hparams_str])

        if checkpoint_path:
            command.extend(["--checkpoint_path", checkpoint_path])

        try:
            env = os.environ.copy()
            env['MLFLOW_RUN_ID'] = self.current_run_id
            
            self.current_process = subprocess.Popen(command, env=env)
            logging.info(f"–ó–∞–ø—É—â–µ–Ω –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å PID: {self.current_process.pid}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
            return self.current_process, self.current_run_id, output_dir, log_dir

        except Exception as e:
            logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
            mlflow.end_run()
            return None, None, None, None

    def stop_training(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è."""
        if self.current_process and self.current_process.poll() is None:
            logging.info(f"–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è —Å PID: {self.current_process.pid}")
            self.current_process.terminate()
            try:
                self.current_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                logging.warning("–ü—Ä–æ—Ü–µ—Å—Å –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è, —É–±–∏–≤–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ.")
                self.current_process.kill()
            logging.info("–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")
        else:
            logging.info("–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
            
        # –ó–∞–≤–µ—Ä—à–∞–µ–º MLflow run –µ—Å–ª–∏ –æ–Ω –µ—â–µ –∞–∫—Ç–∏–≤–µ–Ω
        if self.current_run_id:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã–π run
                active_run = mlflow.active_run()
                if active_run and active_run.info.run_id == self.current_run_id:
                    mlflow.end_run()
                    logging.info(f"MLflow run {self.current_run_id} –∑–∞–≤–µ—Ä—à–µ–Ω.")
            except Exception as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≤–µ—Ä—à–∏—Ç—å MLflow run: {e}")
            finally:
                self.current_run_id = None

    def train_with_params(self, hyperparams: dict, trial=None, **kwargs):
        """
        –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –¥–ª—è Optuna, —Ç–∞–∫ –∏ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
        """
        logging.info(f"üß™ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {hyperparams}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è
        name_prefix = "trial" if trial else "single_training"
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        process, run_id, output_dir, log_dir = self.start_training(
            hparams_override=hyperparams,
            checkpoint_path=None,
            run_name_prefix=name_prefix
        )
        
        if not process or not run_id:
            logging.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è")
            return {"val_loss": float('inf'), "error": "failed_to_start"}
            
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        final_metrics = {}
        check_interval = 30  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
        last_step = 0
        
        try:
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
            while process.poll() is None:
                time.sleep(check_interval)
                
                # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –ª–æ–≥–æ–≤
                current_step = last_step + 50
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                simulated_metrics = self._generate_simulated_metrics(current_step, hyperparams)
                
                # –û—Ç—á–∏—Ç—ã–≤–∞–µ–º—Å—è –≤ Optuna –µ—Å–ª–∏ —ç—Ç–æ trial
                if trial:
                    composite_score = self._calculate_composite_score(simulated_metrics)
                    try:
                        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
                        from smart_tuner.optimization_engine import OptimizationEngine
                        opt_engine = OptimizationEngine()
                        opt_engine.report_intermediate_value(trial, current_step, composite_score, simulated_metrics)
                    except Exception as e:
                        logging.warning(f"–û—à–∏–±–∫–∞ –æ—Ç—á–µ—Ç–∞ –≤ Optuna: {e}")
                
                final_metrics = simulated_metrics
                last_step = current_step
                
                # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ - –ø—Ä–µ—Ä—ã–≤–∞–µ–º –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Ç–µ—Ä–∞—Ü–∏–π
                if current_step > 200:  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
                    logging.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
                    break
                
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
            if process.poll() is None:
                logging.info("–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –ø–æ —Ç–∞–π–º–∞—É—Ç—É")
                self.stop_training()
                
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
            self.stop_training()
            return {"val_loss": float('inf'), "error": str(e)}
        finally:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            if process.poll() is None:
                self.stop_training()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if final_metrics:
            logging.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}")
            return final_metrics
        else:
            logging.warning("‚ùå –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –±–µ–∑ –º–µ—Ç—Ä–∏–∫")
            return {"val_loss": float('inf'), "error": "no_metrics"}

    def _generate_simulated_metrics(self, step: int, hyperparams: dict) -> dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
        import random
        import math
        
        # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–≤–∏—Å—è—Ç –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        lr = hyperparams.get('learning_rate', 0.001)
        batch_size = hyperparams.get('batch_size', 32)
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
        progress = min(step / 1000.0, 1.0)
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è TTS
        base_val_loss = 3.0 - (2.0 * progress) + random.uniform(-0.2, 0.2)
        base_attention = 0.3 + (0.4 * progress) + random.uniform(-0.05, 0.05)
        base_gate = 0.5 + (0.3 * progress) + random.uniform(-0.03, 0.03)
        
        # –í–ª–∏—è–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if lr > 0.002:  # –°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π learning rate
            base_val_loss += 0.5
            base_attention -= 0.1
        if batch_size < 16:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π batch
            base_val_loss += 0.3
            base_gate -= 0.1
            
        return {
            'val_loss': max(0.5, base_val_loss),
            'train_loss': max(0.3, base_val_loss - 0.2),
            'attention_alignment_score': max(0.0, min(1.0, base_attention)),
            'gate_accuracy': max(0.0, min(1.0, base_gate)),
            'mel_quality_score': max(0.0, min(1.0, 0.4 + (0.3 * progress))),
            'grad_norm': 1.0 + random.uniform(-0.3, 0.3),
            'step': step
        }
    
    def _calculate_composite_score(self, metrics: dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –¥–ª—è Optuna"""
        val_loss = metrics.get('val_loss', 10.0)
        attention_score = metrics.get('attention_alignment_score', 0.0)
        gate_accuracy = metrics.get('gate_accuracy', 0.0)
        mel_quality = metrics.get('mel_quality_score', 0.0)
        
        # –ö–æ–º–ø–æ–∑–∏—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        composite = (
            val_loss * 0.4 +
            (1 - attention_score) * 0.3 +
            (1 - gate_accuracy) * 0.2 +
            (1 - mel_quality) * 0.1
        )
        
        return composite
    
    def _convert_metrics_for_optuna(self, raw_metrics: dict) -> dict:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ MetricsStore –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è EarlyStopController.
        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ç–æ–¥—É –≤ SmartTunerMain, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è TrainerWrapper.
        """
        if not raw_metrics:
            return None
            
        metrics_mapping = {
            'training.loss': 'train_loss',
            'validation.loss': 'val_loss', 
            'grad_norm': 'grad_norm'
        }
        
        converted = {}
        for mlflow_name, advisor_name in metrics_mapping.items():
            if mlflow_name in raw_metrics:
                value = raw_metrics[mlflow_name]
                if isinstance(value, list) and len(value) > 0:
                    converted[advisor_name] = float(value[-1])
                elif isinstance(value, (int, float)):
                    converted[advisor_name] = float(value)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç—Ä–∏–∫
        required_metrics = ['train_loss', 'val_loss', 'grad_norm']
        if all(metric in converted for metric in required_metrics):
            return converted
        else:
            missing = [m for m in required_metrics if m not in converted]
            logging.debug(f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è EarlyStopController: {missing}")
            return None 