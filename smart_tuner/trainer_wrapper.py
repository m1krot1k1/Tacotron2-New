import logging
import os
from typing import Dict, Any, Optional
import torch
import optuna


class TrainerWrapper:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è Tacotron2.
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å Optuna, MLflow –∏ TensorBoard.
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.last_checkpoint_path: Optional[str] = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logging.info("TrainerWrapper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")

    def train_with_params(
        self, 
        params: Dict[str, Any], 
        trial: Optional[optuna.Trial] = None, 
        writer: Optional[Any] = None,  # SummaryWriter
        **kwargs  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> Optional[Dict[str, float]]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —Å–µ–∞–Ω—Å –æ–±—É—á–µ–Ω–∏—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –≠—Ç–æ –æ—Å–Ω–æ–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è Smart Tuner.
        """
        logging.info(f"üß™ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {params}")
        
        # –°–æ–∑–¥–∞–µ–º hparams –Ω–∞ –ª–µ—Ç—É –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        from hparams import create_hparams
        hparams = create_hparams()
        for key, value in params.items():
            if hasattr(hparams, key):
                setattr(hparams, key, value)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è
        return self._train_core(hparams, trial, self.last_checkpoint_path, writer)

    def _train_core(
        self, 
        hparams: Any, 
        trial: Optional[optuna.Trial] = None, 
        checkpoint_path: Optional[str] = None,
        writer: Optional[Any] = None  # SummaryWriter
    ) -> Optional[Dict[str, float]]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è –∏–∑ train.py.
        """
        try:
            logging.info("–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é train –∏–∑ train.py...")
            
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ü–£–¢–ò: –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ loss_function.py –≤ sys.path
            import sys
            import os
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ loss_function
            current_dir = os.path.dirname(os.path.abspath(__file__))
            parent_dir = os.path.dirname(current_dir)
            if parent_dir not in sys.path:
                sys.path.insert(0, parent_dir)
                logging.info(f"–î–æ–±–∞–≤–ª–µ–Ω –≤ sys.path: {parent_dir}")
            
            from train import train as core_train_func
            logging.info("–§—É–Ω–∫—Ü–∏—è train —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞.")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
            output_directory = os.path.join(
                self.config.get('training', {}).get('base_output_dir', 'output'),
                f"trial_{trial.number}" if trial else "single_run"
            )
            log_directory = os.path.join(output_directory, "logs")
            os.makedirs(log_directory, exist_ok=True)
            
            logging.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
            logging.info(f"   - epochs: {hparams.epochs}")
            logging.info(f"   - batch_size: {hparams.batch_size}")
            logging.info(f"   - learning_rate: {hparams.learning_rate}")
            logging.info(f"   - output_directory: {output_directory}")

            # üì± –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Telegram Monitor
            telegram_monitor = None
            try:
                from smart_tuner.telegram_monitor import TelegramMonitor
                telegram_monitor = TelegramMonitor()
                logging.info("üì± Telegram Monitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Telegram Monitor: {e}")

            final_metrics = core_train_func(
                output_directory=output_directory,
                log_directory=log_directory,
                checkpoint_path=checkpoint_path,
                warm_start=False,
                ignore_mmi_layers=False,
                ignore_gst_layers=False,
                ignore_tsgst_layers=False,
                n_gpus=1,
                rank=0,
                group_name="",
                hparams=hparams,
                # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–æ Smart Tuner
                smart_tuner_trial=trial,
                smart_tuner_logger=self._setup_logger(output_directory),
                tensorboard_writer=writer,
                telegram_monitor=telegram_monitor
            )
            
            logging.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –ø–æ–ª—É—á–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}")
            
            if final_metrics and final_metrics.get('checkpoint_path'):
                self.last_checkpoint_path = final_metrics['checkpoint_path']
            
            return final_metrics
        
        except Exception as e:
            import traceback
            logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —è–¥—Ä–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
            logging.error(f"–ü–æ–ª–Ω—ã–π traceback: {traceback.format_exc()}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
            try:
                return {
                    "validation_loss": float('inf'),
                    "iteration": 0,
                    "checkpoint_path": None,
                    "error": str(e)
                }
            except:
                return None

    def _setup_logger(self, output_directory: str) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Python –ª–æ–≥–≥–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞."""
        logger = logging.getLogger(f"smart_tuner_train_{os.path.basename(output_directory)}")
        logger.setLevel(logging.INFO)
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Ñ–∞–π–ª–∞
        log_file = os.path.join(output_directory, "smart_tuner_train.log")
        file_handler = logging.FileHandler(log_file)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        logger.info(f"–õ–æ–≥–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤ {log_file}")
        return logger
        
    def cleanup(self):
        self.last_checkpoint_path = None
        logging.info("TrainerWrapper –æ—á–∏—â–µ–Ω.") 