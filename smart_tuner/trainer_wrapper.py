import logging
import os
import sys
import time
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, Optional, Tuple
import optuna

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# === MLflow: –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
try:
    import mlflow
    import mlflow.pytorch
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logging.warning("MLflow –Ω–µ –Ω–∞–π–¥–µ–Ω, –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è")

# === TensorBoard: –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    TENSORBOARD_AVAILABLE = False
    logging.warning("TensorBoard –Ω–µ –Ω–∞–π–¥–µ–Ω, –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å—Å—è")

# –ò–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
try:
    from training_utils.dynamic_padding import DynamicPaddingCollator
    from training_utils.bucket_batching import BucketBatchSampler
    UTILS_AVAILABLE = True
except ImportError:
    UTILS_AVAILABLE = False
    logging.warning("–£—Ç–∏–ª–∏—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

# –ò–º–ø–æ—Ä—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
try:
    from debug_reporter import initialize_debug_reporter, get_debug_reporter
    DEBUG_REPORTER_AVAILABLE = True
except ImportError:
    DEBUG_REPORTER_AVAILABLE = False
    logging.warning("Debug Reporter –Ω–µ –Ω–∞–π–¥–µ–Ω")

class EnhancedTrainerWrapper:
    """
    üöÄ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è Tacotron2.
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑ enhanced_training_main.py:
    - –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (10+ –º–µ—Ç—Ä–∏–∫)
    - –§–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    - –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ (bucket batching, dynamic padding)
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    - Smart Tuner –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.last_checkpoint_path: Optional[str] = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        self.current_epoch = 0
        self.global_step = 0
        self.best_validation_loss = float('inf')
        self.training_metrics_history = []
        
        # –§–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è (–∫–∞–∫ –≤ enhanced_training_main.py)
        self.training_phases = {
            'pre_alignment': {'max_epoch': 500, 'focus': 'attention_learning'},
            'alignment_learning': {'max_epoch': 2000, 'focus': 'attention_stabilization'},
            'quality_optimization': {'max_epoch': 3000, 'focus': 'quality_improvement'},
            'fine_tuning': {'max_epoch': 3500, 'focus': 'final_polishing'}
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.tensorboard_writer = None
        self.tensorboard_logdir = 'logs'
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.debug_reporter = None
        self.telegram_monitor = None
        
        logging.info("üöÄ Enhanced TrainerWrapper –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏")

    def train_with_params(
        self, 
        params: Dict[str, Any], 
        trial: Optional[optuna.Trial] = None, 
        writer: Optional[Any] = None,  # SummaryWriter
        **kwargs  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    ) -> Optional[Dict[str, float]]:
        """
        üöÄ –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —Å–µ–∞–Ω—Å –æ–±—É—á–µ–Ω–∏—è —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –¢–µ–ø–µ—Ä—å —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∏–∑ enhanced_training_main.py
        """
        logging.info(f"üß™ –ó–∞–ø—É—Å–∫ ENHANCED –æ–±—É—á–µ–Ω–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏: {params}")
        
        # –°–æ–∑–¥–∞–µ–º hparams –Ω–∞ –ª–µ—Ç—É –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
        from hparams import create_hparams
        hparams = create_hparams()
        for key, value in params.items():
            if hasattr(hparams, key):
                setattr(hparams, key, value)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self._setup_enhanced_logging(trial)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è
        return self._train_core_enhanced(hparams, trial, self.last_checkpoint_path, writer)

    def _setup_enhanced_logging(self, trial: Optional[optuna.Trial] = None):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ –≤ enhanced_training_main.py"""
        
        # === –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ TensorBoard ===
        if TENSORBOARD_AVAILABLE and os.path.exists(self.tensorboard_logdir):
            try:
                for file in os.listdir(self.tensorboard_logdir):
                    if file.startswith('events.out.tfevents'):
                        os.remove(os.path.join(self.tensorboard_logdir, file))
                        logging.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –ª–æ–≥ TensorBoard: {file}")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤: {e}")
        
        # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TensorBoard ===
        if TENSORBOARD_AVAILABLE:
            try:
                self.tensorboard_writer = SummaryWriter(self.tensorboard_logdir)
                logging.info(f"‚úÖ TensorBoard writer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.tensorboard_logdir}")
            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TensorBoard: {e}")
                self.tensorboard_writer = None
        
        # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MLflow ===
        if MLFLOW_AVAILABLE:
            try:
                # –ó–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π run –µ—Å–ª–∏ –æ–Ω –∞–∫—Ç–∏–≤–µ–Ω
                try:
                    mlflow.end_run()
                except:
                    pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –µ—Å–ª–∏ run –Ω–µ –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω
                
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
                experiment_name = f"tacotron2_enhanced_{int(time.time())}"
                mlflow.set_experiment(experiment_name)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π run
                run_name = f"enhanced_run_{trial.number if trial else 'single'}_{int(time.time())}"
                mlflow.start_run(run_name=run_name)
                logging.info(f"‚úÖ MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {experiment_name}")
            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MLflow: {e}")
        
        # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Debug Reporter ===
        if DEBUG_REPORTER_AVAILABLE:
            try:
                self.debug_reporter = initialize_debug_reporter(self.telegram_monitor)
                logging.info("üîç Debug Reporter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Debug Reporter: {e}")
        
        # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram Monitor ===
        try:
            from smart_tuner.telegram_monitor import TelegramMonitor
            import yaml
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
            config_path = "smart_tuner/config.yaml"
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
            except Exception as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥: {e}")
                config = {}
            
            telegram_config = config.get('telegram', {})
            bot_token = telegram_config.get('bot_token')
            chat_id = telegram_config.get('chat_id')
            enabled = telegram_config.get('enabled', False)
            
            if bot_token and chat_id and enabled:
                self.telegram_monitor = TelegramMonitor()
                logging.info("üì± Telegram Monitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            else:
                self.telegram_monitor = None
                logging.warning("üì± Telegram Monitor –æ—Ç–∫–ª—é—á–µ–Ω (–Ω–µ–ø–æ–ª–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)")
        except Exception as e:
            self.telegram_monitor = None
            logging.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Telegram Monitor: {e}")

    def _train_core_enhanced(
        self, 
        hparams: Any, 
        trial: Optional[optuna.Trial] = None, 
        checkpoint_path: Optional[str] = None,
        writer: Optional[Any] = None  # SummaryWriter
    ) -> Optional[Dict[str, float]]:
        """
        üöÄ –í—ã–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç train.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
        """
        try:
            logging.info("üöÄ –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ENHANCED —Ñ—É–Ω–∫—Ü–∏—é –æ–±—É—á–µ–Ω–∏—è –∏–∑ train.py...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º train.py —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
            from train import train as core_train_func
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
            output_directory = os.path.join(
                self.config.get('training', {}).get('base_output_dir', 'output'),
                f"enhanced_trial_{trial.number}" if trial else "enhanced_single_run"
            )
            log_directory = os.path.join(output_directory, "logs")
            os.makedirs(log_directory, exist_ok=True)
            
            logging.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º ENHANCED –æ–±—É—á–µ–Ω–∏–µ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
            logging.info(f"   - epochs: {hparams.epochs}")
            logging.info(f"   - batch_size: {hparams.batch_size}")
            logging.info(f"   - learning_rate: {hparams.learning_rate}")
            logging.info(f"   - output_directory: {output_directory}")

            # üì± –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Telegram Monitor
            telegram_monitor = None
            try:
                from smart_tuner.telegram_monitor import TelegramMonitor
                telegram_monitor = TelegramMonitor()
                logging.info("üì± Telegram Monitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Telegram Monitor: {e}")

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
            final_metrics = core_train_func(
                output_directory=output_directory,
                log_directory=log_directory,
                checkpoint_path=checkpoint_path,
                warm_start=False,
                ignore_mmi_layers=False,
                ignore_gst_layers=False,
                ignore_tsgst_layers=False,
                n_gpus=1,
                rank=0,
                group_name="",
                hparams=hparams,
                # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–æ Smart Tuner
                smart_tuner_trial=trial,
                smart_tuner_logger=self._setup_logger(output_directory),
                tensorboard_writer=writer,
                telegram_monitor=telegram_monitor
            )
            
            logging.info(f"‚úÖ ENHANCED –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –ø–æ–ª—É—á–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏: {final_metrics}")
            
            if final_metrics and final_metrics.get('checkpoint_path'):
                self.last_checkpoint_path = final_metrics['checkpoint_path']
            
            # === –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ ===
            self._log_enhanced_metrics(final_metrics, trial)
            
            return final_metrics
        
        except Exception as e:
            import traceback
            logging.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ ENHANCED –æ–±—É—á–µ–Ω–∏–∏: {e}")
            logging.error(f"–ü–æ–ª–Ω—ã–π traceback: {traceback.format_exc()}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
            try:
                return {
                    "validation_loss": float('inf'),
                    "iteration": 0,
                    "checkpoint_path": None,
                    "error": str(e)
                }
            except:
                return None

    def _log_enhanced_metrics(self, metrics: Dict[str, Any], trial: Optional[optuna.Trial] = None):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞–∫ –≤ enhanced_training_main.py"""
        
        if not metrics:
            return
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard ===
        if self.tensorboard_writer is not None:
            try:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                if 'validation_loss' in metrics:
                    self.tensorboard_writer.add_scalar("val/loss", metrics['validation_loss'], self.global_step)
                if 'training_loss' in metrics:
                    self.tensorboard_writer.add_scalar("train/loss", metrics['training_loss'], self.global_step)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
                if 'attention_alignment_score' in metrics:
                    self.tensorboard_writer.add_scalar("quality/attention_alignment", metrics['attention_alignment_score'], self.global_step)
                if 'gate_accuracy' in metrics:
                    self.tensorboard_writer.add_scalar("quality/gate_accuracy", metrics['gate_accuracy'], self.global_step)
                if 'mel_quality_score' in metrics:
                    self.tensorboard_writer.add_scalar("quality/mel_quality", metrics['mel_quality_score'], self.global_step)
                
                # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                if 'learning_rate' in metrics:
                    self.tensorboard_writer.add_scalar("hyperparams/learning_rate", metrics['learning_rate'], self.global_step)
                if 'batch_size' in metrics:
                    self.tensorboard_writer.add_scalar("hyperparams/batch_size", metrics['batch_size'], self.global_step)
                
                self.tensorboard_writer.flush()
                logging.info("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ TensorBoard")
            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ TensorBoard: {e}")
        
        # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow ===
        if MLFLOW_AVAILABLE:
            try:
                for key, value in metrics.items():
                    if isinstance(value, (int, float)) and not np.isnan(value):
                        mlflow.log_metric(f"enhanced.{key}", value, step=self.global_step)
                logging.info("‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã –≤ MLflow")
            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ MLflow: {e}")

    def get_current_training_phase(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è"""
        for phase, config in self.training_phases.items():
            if self.current_epoch <= config['max_epoch']:
                return phase
        return 'fine_tuning'

    def adjust_hyperparams_for_phase(self, phase: str, hparams: Any):
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è"""
        if phase == 'pre_alignment':
            # –°–Ω–∏–∂–∞–µ–º learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            hparams.learning_rate *= 0.5
            hparams.guided_attn_weight = 100.0
        elif phase == 'alignment_learning':
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º guided attention
            hparams.guided_attn_weight = 200.0
        elif phase == 'quality_optimization':
            # –§–æ–∫—É—Å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ
            hparams.learning_rate *= 0.8
        elif phase == 'fine_tuning':
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ª–∏—Ä–æ–≤–∫–∞
            hparams.learning_rate *= 0.5
        
        logging.info(f"üîÑ –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∞–∑—ã '{phase}'")

    def _setup_logger(self, output_directory: str) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Python –ª–æ–≥–≥–µ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—É—Å–∫–∞."""
        logger = logging.getLogger(f"enhanced_smart_tuner_{os.path.basename(output_directory)}")
        logger.setLevel(logging.INFO)
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è —Ñ–∞–π–ª–∞
        log_file = os.path.join(output_directory, "enhanced_smart_tuner.log")
        file_handler = logging.FileHandler(log_file)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        logger.info(f"–õ–æ–≥–∏ –¥–ª—è ENHANCED –∑–∞–ø—É—Å–∫–∞ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤ {log_file}")
        return logger
        
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.last_checkpoint_path = None
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º TensorBoard
        if self.tensorboard_writer is not None:
            try:
                self.tensorboard_writer.close()
                logging.info("‚úÖ TensorBoard writer –∑–∞–∫—Ä—ã—Ç")
            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è TensorBoard: {e}")
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º MLflow
        if MLFLOW_AVAILABLE:
            try:
                mlflow.end_run()
                logging.info("‚úÖ MLflow run –∑–∞–≤–µ—Ä—à–µ–Ω")
            except Exception as e:
                logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è MLflow run: {e}")
        
        logging.info("üöÄ Enhanced TrainerWrapper –æ—á–∏—â–µ–Ω.")


# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
class TrainerWrapper(EnhancedTrainerWrapper):
    """
    –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—Ç–∞—Ä—ã–º TrainerWrapper
    """
    pass 