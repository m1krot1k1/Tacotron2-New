#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SafeDDCLoss - –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ DDC Loss –¥–ª—è Tacotron2-New
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–æ–±–ª–µ–º "DDC loss: —Ä–∞–∑–º–µ—Ä—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç"

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ —Ç–µ–Ω–∑–æ—Ä–æ–≤
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å–æ–∫
- –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Smart Tuner
"""

import torch
import torch.nn.functional as F
import logging
from typing import Optional, Tuple, Dict, Any
from smart_tuner.smart_truncation_ddc import SmartTruncationDDC
from smart_tuner.memory_efficient_ddc import MemoryEfficientDDC

class SafeDDCLoss:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π DDC Loss —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤.
    
    –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã:
    - –†–∞–∑–º–µ—Ä—ã —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç
    - –ü–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –æ–±—Ä–µ–∑–∞–Ω–∏–∏
    - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å–æ–∫
    """
    
    def __init__(self, weight=1.0, use_masking=True, log_warnings=True, mode='safe'):
        self.weight = weight
        self.use_masking = use_masking
        self.log_warnings = log_warnings
        self.mode = mode  # 'safe', 'smart_truncation', 'memory_efficient'
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_calls = 0
        self.size_mismatches = 0
        self.masking_applied = 0
        self.errors = 0
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logger = logging.getLogger(__name__)
        
        self.smart_trunc = SmartTruncationDDC() if mode == 'smart_truncation' else None
        self.memory_efficient = MemoryEfficientDDC() if mode == 'memory_efficient' else None
        
    def __call__(self, pred: torch.Tensor, target: torch.Tensor, 
                 input_lengths: Optional[torch.Tensor] = None, 
                 target_lengths: Optional[torch.Tensor] = None,
                 step: int = 0) -> torch.Tensor:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ DDC loss —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤.
        
        Args:
            pred: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ mel —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã (B, n_mels, T_pred)
            target: –¶–µ–ª–µ–≤—ã–µ mel —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã (B, n_mels, T_target)
            input_lengths: –î–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            target_lengths: –î–ª–∏–Ω—ã —Ü–µ–ª–µ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            DDC loss tensor
        """
        self.total_calls += 1
        
        try:
            if self.mode == 'smart_truncation' and self.smart_trunc is not None:
                return self.smart_trunc(pred, target)
            if self.mode == 'memory_efficient' and self.memory_efficient is not None:
                return self.memory_efficient(pred, target)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if pred is None or target is None:
                self.logger.warning("‚ùå DDC Loss: pred –∏–ª–∏ target —Ä–∞–≤–Ω—ã None")
                return torch.tensor(0.0, requires_grad=True, device=pred.device if pred is not None else 'cpu')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            if pred.dim() != 3 or target.dim() != 3:
                self.logger.warning(f"‚ùå DDC Loss: –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ pred={pred.shape}, target={target.shape}")
                return torch.tensor(0.0, requires_grad=True, device=pred.device)
            
            batch_size = pred.size(0)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
            min_seq_len = min(pred.size(-1), target.size(-1))
            min_feat_len = min(pred.size(1), target.size(1))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—Ä–µ–∑–∞–Ω–∏—è
            needs_trimming = (pred.size(-1) != min_seq_len or 
                            target.size(-1) != min_seq_len or
                            pred.size(1) != min_feat_len or 
                            target.size(1) != min_feat_len)
            
            if needs_trimming:
                self.size_mismatches += 1
                if self.log_warnings:
                    self.logger.info(f"üîß DDC Loss: –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–æ {min_feat_len}x{min_seq_len} "
                                   f"(–±—ã–ª–æ pred={pred.shape}, target={target.shape})")
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ
            pred_safe = pred[:, :min_feat_len, :min_seq_len]
            target_safe = target[:, :min_feat_len, :min_seq_len]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å–æ–∫
            if self.use_masking and input_lengths is not None and target_lengths is not None:
                mask = self._create_advanced_mask(batch_size, min_seq_len, min_feat_len,
                                                input_lengths, target_lengths, pred_safe.device)
                pred_safe = pred_safe * mask
                target_safe = target_safe * mask
                self.masking_applied += 1
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ NaN/Inf
            loss = F.mse_loss(pred_safe, target_safe, reduction='mean')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å loss
            if torch.isnan(loss) or torch.isinf(loss):
                self.logger.error(f"‚ùå DDC Loss: NaN/Inf –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ loss={loss}")
                self.errors += 1
                return torch.tensor(0.0, requires_grad=True, device=pred.device)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å
            weighted_loss = loss * self.weight
            
            return weighted_loss
            
        except Exception as e:
            self.errors += 1
            self.logger.error(f"‚ùå DDC Loss error: {e}")
            return torch.tensor(0.0, requires_grad=True, device=pred.device if pred is not None else 'cpu')
    
    def _create_advanced_mask(self, batch_size: int, seq_len: int, feat_len: int,
                            input_lengths: torch.Tensor, target_lengths: torch.Tensor,
                            device: torch.device) -> torch.Tensor:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –º–∞—Å–∫—É –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss.
        
        –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –£—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±–∞ —Ç–∏–ø–∞ –¥–ª–∏–Ω (input –∏ target)
        - –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ batch —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        - –ó–∞—â–∏—Ç–∞ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã
        """
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É —Ä–∞–∑–º–µ—Ä–æ–º (batch_size, feat_len, seq_len)
        mask = torch.ones(batch_size, feat_len, seq_len, device=device)
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –¥–ª–∏–Ω—ã –∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
        if input_lengths.dim() == 0:
            input_lengths = input_lengths.unsqueeze(0)
        if target_lengths.dim() == 0:
            target_lengths = target_lengths.unsqueeze(0)
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –¥–æ batch_size –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if input_lengths.size(0) == 1 and batch_size > 1:
            input_lengths = input_lengths.expand(batch_size)
        if target_lengths.size(0) == 1 and batch_size > 1:
            target_lengths = target_lengths.expand(batch_size)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ batch
        for i in range(min(batch_size, input_lengths.size(0), target_lengths.size(0))):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–ª–∏–¥–Ω—É—é –¥–ª–∏–Ω—É –∫–∞–∫ –º–∏–Ω–∏–º—É–º –∏–∑ –¥–≤—É—Ö
            valid_len = min(
                int(input_lengths[i].item()) if input_lengths[i] is not None else seq_len,
                int(target_lengths[i].item()) if target_lengths[i] is not None else seq_len,
                seq_len
            )
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
            if valid_len < seq_len:
                mask[i, :, valid_len:] = 0.0
        
        return mask
    
    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã SafeDDCLoss."""
        if self.total_calls == 0:
            return {
                'total_calls': 0,
                'size_mismatches': 0,
                'masking_applied': 0,
                'errors': 0,
                'mismatch_rate': 0.0,
                'error_rate': 0.0
            }
        
        return {
            'total_calls': self.total_calls,
            'size_mismatches': self.size_mismatches,
            'masking_applied': self.masking_applied,
            'errors': self.errors,
            'mismatch_rate': self.size_mismatches / self.total_calls,
            'error_rate': self.errors / self.total_calls
        }
    
    def get_recommendations(self) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        stats = self.get_statistics()
        recommendations = []
        
        if stats['mismatch_rate'] > 0.5:
            recommendations.append("‚ö†Ô∏è –ß–∞—Å—Ç—ã–µ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤ DDC - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏")
        
        if stats['error_rate'] > 0.1:
            recommendations.append("üö® –í—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫ DDC - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        
        if stats['masking_applied'] == 0 and self.use_masking:
            recommendations.append("üí° –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ DDC –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–µ—Ä–µ–¥–∞—á—É lengths")
        
        return recommendations
    
    def reset_statistics(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        self.total_calls = 0
        self.size_mismatches = 0
        self.masking_applied = 0
        self.errors = 0
        self.logger.info("üîÑ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ SafeDDCLoss —Å–±—Ä–æ—à–µ–Ω–∞")


class AdaptiveDDCLoss(SafeDDCLoss):
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π DDC Loss —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –≤–µ—Å–∞.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞
    - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    - –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ —Ñ–∞–∑–µ –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self, initial_weight=1.0, min_weight=0.1, max_weight=5.0,
                 adaptation_rate=0.01, quality_threshold=0.7):
        super().__init__(weight=initial_weight)
        
        self.initial_weight = initial_weight
        self.min_weight = min_weight
        self.max_weight = max_weight
        self.adaptation_rate = adaptation_rate
        self.quality_threshold = quality_threshold
        
        # –ò—Å—Ç–æ—Ä–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.quality_history = []
        self.weight_history = []
        
    def adapt_weight(self, quality_score: float, step: int):
        """
        –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –≤–µ—Å DDC loss –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞.
        
        Args:
            quality_score: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (0-1)
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
        """
        self.quality_history.append(quality_score)
        if len(self.quality_history) > 100:
            self.quality_history.pop(0)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —à–∞–≥–∏
        if len(self.quality_history) >= 10:
            recent_quality = sum(self.quality_history[-10:]) / 10
            
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –≤–µ—Å
            if recent_quality < self.quality_threshold:
                # –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å DDC
                new_weight = min(self.weight * (1 + self.adaptation_rate), self.max_weight)
            else:
                # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - —É–º–µ–Ω—å—à–∞–µ–º –≤–µ—Å DDC
                new_weight = max(self.weight * (1 - self.adaptation_rate), self.min_weight)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            if abs(new_weight - self.weight) > 0.01:
                old_weight = self.weight
                self.weight = new_weight
                self.weight_history.append({
                    'step': step,
                    'old_weight': old_weight,
                    'new_weight': new_weight,
                    'quality': recent_quality
                })
                
                self.logger.info(f"üîÑ DDC weight –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω: {old_weight:.3f} ‚Üí {new_weight:.3f} "
                               f"(quality: {recent_quality:.3f})")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
_global_ddc_loss = None

def get_global_ddc_loss() -> SafeDDCLoss:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä DDC loss."""
    global _global_ddc_loss
    if _global_ddc_loss is None:
        _global_ddc_loss = SafeDDCLoss()
    return _global_ddc_loss

def set_global_ddc_loss(ddc_loss: SafeDDCLoss):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä DDC loss."""
    global _global_ddc_loss
    _global_ddc_loss = ddc_loss 