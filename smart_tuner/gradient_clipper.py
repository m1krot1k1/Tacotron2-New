#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π Gradient Clipper –¥–ª—è Tacotron2-New
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–æ–±–ª–µ–º –≤–∑—Ä—ã–≤–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (Grad Norm 100k-400k)
"""

import torch
import torch.nn.utils as utils
import numpy as np
import logging
from typing import Tuple, Optional

class AdaptiveGradientClipper:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π gradient clipper —Å –∏—Å—Ç–æ—Ä–∏–µ–π –∏ —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–º–∏ —Ä–µ–∂–∏–º–∞–º–∏.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    - –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö
    - –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Smart Tuner
    """
    
    def __init__(self, max_norm=1.0, adaptive=True, history_size=1000, 
                 emergency_threshold=1000.0, percentile=95):
        self.max_norm = max_norm
        self.adaptive = adaptive
        self.history_size = history_size
        self.emergency_threshold = emergency_threshold
        self.percentile = percentile
        
        # –ò—Å—Ç–æ—Ä–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        self.grad_history = []
        self.clip_history = []
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_clips = 0
        self.emergency_clips = 0
        self.max_grad_norm_seen = 0.0
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.logger = logging.getLogger(__name__)
        
        # –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º
        self.emergency_mode = False
        self.emergency_mode_steps = 0
        
    def clip_gradients(self, model: torch.nn.Module, step: int = 0) -> Tuple[bool, float, float]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º.
        
        Args:
            model: –ú–æ–¥–µ–ª—å PyTorch
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            (was_clipped, current_norm, clip_threshold)
        """
        # –í—ã—á–∏—Å–ª—è–µ–º —Ç–µ–∫—É—â—É—é –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        total_norm = self._calculate_gradient_norm(model)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.max_grad_norm_seen = max(self.max_grad_norm_seen, total_norm)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.grad_history.append(total_norm)
        if len(self.grad_history) > self.history_size:
            self.grad_history.pop(0)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –æ–±—Ä–µ–∑–∞–Ω–∏—è
        clip_threshold = self._calculate_adaptive_threshold(total_norm, step)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—Ä–µ–∑–∞–Ω–∏—è
        should_clip = total_norm > clip_threshold
        
        if should_clip:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–µ–∑–∞–Ω–∏–µ
            utils.clip_grad_norm_(model.parameters(), clip_threshold)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.total_clips += 1
            self.clip_history.append({
                'step': step,
                'original_norm': total_norm,
                'clip_threshold': clip_threshold,
                'emergency': total_norm > self.emergency_threshold
            })
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º
            if total_norm > self.emergency_threshold:
                self.emergency_clips += 1
                self.emergency_mode = True
                self.emergency_mode_steps += 1
                self.logger.warning(f"üö® –≠–ö–°–¢–†–ï–ù–ù–´–ô —Ä–µ–∂–∏–º: Grad Norm {total_norm:.2f} > {self.emergency_threshold}")
            else:
                self.logger.info(f"‚úÇÔ∏è –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ–±—Ä–µ–∑–∞–Ω—ã: {total_norm:.2f} ‚Üí {clip_threshold:.2f}")
        
        # –í—ã—Ö–æ–¥ –∏–∑ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        if self.emergency_mode and total_norm < self.max_norm * 0.5:
            self.emergency_mode = False
            self.emergency_mode_steps = 0
            self.logger.info("‚úÖ –í—ã—Ö–æ–¥ –∏–∑ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ gradient clipping")
        
        return should_clip, total_norm, clip_threshold
    
    def _calculate_gradient_norm(self, model: torch.nn.Module) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â—É—é –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏."""
        total_norm = 0.0
        param_count = 0
        
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        if param_count == 0:
            return 0.0
        
        total_norm = total_norm ** (1. / 2)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
        if np.isnan(total_norm) or np.isinf(total_norm):
            self.logger.error(f"‚ùå NaN/Inf –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö: {total_norm}")
            return float('inf')
        
        return total_norm
    
    def _calculate_adaptive_threshold(self, current_norm: float, step: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ–±—Ä–µ–∑–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤."""
        if not self.adaptive or len(self.grad_history) < 10:
            return self.max_norm
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        recent_history = self.grad_history[-100:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 —à–∞–≥–æ–≤
        if len(recent_history) < 5:
            return self.max_norm
        
        try:
            adaptive_threshold = np.percentile(recent_history, self.percentile)
            
            # –í —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥
            if self.emergency_mode:
                adaptive_threshold = min(adaptive_threshold * 0.5, self.max_norm * 0.5)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏
            final_threshold = min(max(adaptive_threshold, self.max_norm * 0.1), self.max_norm)
            
            return final_threshold
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞: {e}")
            return self.max_norm
    
    def get_statistics(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã clipper."""
        if not self.grad_history:
            return {
                'total_clips': 0,
                'emergency_clips': 0,
                'max_grad_norm': 0.0,
                'avg_grad_norm': 0.0,
                'emergency_mode': False,
                'clip_rate': 0.0
            }
        
        avg_norm = np.mean(self.grad_history)
        clip_rate = self.total_clips / len(self.grad_history) if self.grad_history else 0.0
        
        return {
            'total_clips': self.total_clips,
            'emergency_clips': self.emergency_clips,
            'max_grad_norm': self.max_grad_norm_seen,
            'avg_grad_norm': avg_norm,
            'emergency_mode': self.emergency_mode,
            'emergency_mode_steps': self.emergency_mode_steps,
            'clip_rate': clip_rate,
            'recent_clips': len([c for c in self.clip_history if c['step'] > max(0, len(self.grad_history) - 100)])
        }
    
    def reset_emergency_mode(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º."""
        self.emergency_mode = False
        self.emergency_mode_steps = 0
        self.logger.info("üîÑ –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º gradient clipping —Å–±—Ä–æ—à–µ–Ω")
    
    def get_recommendations(self) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        stats = self.get_statistics()
        recommendations = []
        
        if stats['emergency_clips'] > 0:
            recommendations.append("üö® –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ –æ–±—Ä–µ–∑–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ - —Å–Ω–∏–∑–∏—Ç—å learning rate")
        
        if stats['clip_rate'] > 0.3:
            recommendations.append("‚ö†Ô∏è –ß–∞—Å—Ç—ã–µ –æ–±—Ä–µ–∑–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏")
        
        if stats['avg_grad_norm'] > 10.0:
            recommendations.append("üìà –í—ã—Å–æ–∫–∞—è —Å—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ - —É–≤–µ–ª–∏—á–∏—Ç—å batch size")
        
        if stats['emergency_mode']:
            recommendations.append("üõ°Ô∏è –ê–∫—Ç–∏–≤–µ–Ω —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º - –æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º")
        
        return recommendations


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Smart Tuner
_global_clipper = None

def get_global_clipper() -> AdaptiveGradientClipper:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä clipper."""
    global _global_clipper
    if _global_clipper is None:
        _global_clipper = AdaptiveGradientClipper()
    return _global_clipper

def set_global_clipper(clipper: AdaptiveGradientClipper):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä clipper."""
    global _global_clipper
    _global_clipper = clipper 