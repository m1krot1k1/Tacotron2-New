# Конфигурация Smart Tuner V2
experiment_name: "tacotron2_production"
hparams_path: "hparams.py"
dataset_path: "data/dataset"
checkpoint_path: "data/checkpoint"
output_dir: "output"

# Настройки обучения
training:
  script_path: "train.py"
  python_executable: "python"
  base_command: "python train.py"
  continue_from_checkpoint: true
  full_training: true
  
# Пространство поиска гиперпараметров
search_space:
  learning_rate:
    type: "float"
    min: 0.0001
    max: 0.01
    log: true
    default: 0.001
    
  batch_size:
    type: "categorical"
    choices: [16, 32, 64]
    default: 32
    
  epochs:
    type: "int"
    min: 50
    max: 200
    default: 100
    
  warmup_steps:
    type: "int"
    min: 500
    max: 2000
    default: 1000

# Дублируем для совместимости с OptimizationEngine
hyperparameter_search_space:
  learning_rate:
    type: "float"
    min: 0.0001
    max: 0.01
    log: true
    default: 0.001
    
  batch_size:
    type: "categorical"
    choices: [16, 32, 64]
    default: 32
    
  epochs:
    type: "int"
    min: 50
    max: 200
    default: 100
    
  warmup_steps:
    type: "int"
    min: 500
    max: 2000
    default: 1000

# Настройки оптимизации
optimization:
  direction: "minimize"
  objective_metric: "val_loss"
  n_trials: 20
  overfitting_penalty: 0.1
  continue_training: true
  full_epochs_per_trial: 50

# Настройки планирования параметров
parameter_scheduling:
  learning_rate:
    enabled: true
    strategy: "cosine"
    start_value: 0.001
    end_value: 0.0001
    total_steps: 10000
    
parameter_scheduling_config:
  update_frequency: 10

# Настройки раннего останова с проактивными мерами
early_stopping:
  patience_criterion:
    enabled: true
    type: "patience"
    metric: "val_loss"
    patience: 10
    min_delta: 0.001
    mode: "min"
    
  overfitting_criterion:
    enabled: true
    type: "overfitting"
    train_metric: "train_loss"
    val_metric: "val_loss"
    overfitting_threshold: 0.2
    window_size: 5
    
  divergence_criterion:
    enabled: true
    type: "loss_divergence"
    metric: "train_loss"
    divergence_threshold: 10.0

# Проактивные меры предотвращения проблем
proactive_measures:
  enabled: true
  
  # 1. Стагнация (Validation Loss на плато)
  stagnation_detection:
    enabled: true
    patience: 15  # после скольки шагов без улучшения считать стагнацией
    min_delta: 0.005 # минимальное улучшение, которое не считается стагнацией
    action:
      learning_rate_multiplier: 1.5 # Увеличить LR в 1.5 раза

  # 2. Переобучение (Validation Loss растет, а Train Loss падает)
  overfitting_detection:
    enabled: true
    patience: 5 # после скольки шагов с признаками переобучения вмешиваться
    threshold: 0.1 # если val_loss > train_loss на 10%
    action:
      learning_rate_multiplier: 0.7 # Уменьшить LR на 30%
      # В hparams.py нужно добавить параметр "dropout_rate"
      dropout_rate_increase: 0.1 # Увеличить dropout

  # 3. Нестабильность обучения (взрыв градиентов)
  instability_detection:
    enabled: true
    grad_norm_threshold: 50.0 # порог для grad_norm
    action:
      enable_gradient_clipping: true # Включить/увеличить grad_clip_thresh
      gradient_clip_thresh: 1.0
      batch_size_multiplier: 1.5 # Увеличить batch_size в 1.5 раза (если позволяет память)
      
# Настройки Telegram уведомлений
telegram:
  enabled: false
  bot_token: ""

# Настройки реестра моделей
model_registry:
  path: "smart_tuner/models"
  max_models: 5
  best_model_name: "best_model.pt"
  primary_metric: "val_loss"
  minimize_metric: true

# Настройки финального обучения
final_training:
  epochs: 200

# Настройки MLflow
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "tacotron2_optimization"

# Порты для компонентов Smart Tuner V2 (последовательно 5000-5010)
ports:
  mlflow: 5000              # Эксперименты и метрики  
  tensorboard: 5001         # Графики обучения
  optimization_engine: 5002 # Optuna Dashboard (перенесен на 5002)
  streamlit: 5003           # TTS Demo
  log_watcher: 5004         # Мониторинг логов в реальном времени  
  metrics_store: 5005       # API для метрик и статистики
  param_scheduler: 5006     # Планировщик гиперпараметров
  early_stop_controller: 5007  # Контроллер раннего останова
  alert_manager: 5008       # Управление уведомлениями
  model_registry: 5009      # Реестр моделей

# Настройки ресурсов
resources:
  checkpointing:
    path: "output" 