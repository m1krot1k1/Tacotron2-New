# Конфигурация Smart Tuner V2
experiment_name: "tacotron2_production"
dataset_path: "data/dataset"
checkpoint_dir: "data/checkpoint"
output_dir: "output"

# Настройки обучения
training:
  script_path: "train.py"
  python_executable: "python"
  base_command: "python train.py"
  continue_from_checkpoint: true
  full_training: true
  
# Пространство поиска гиперпараметров
search_space:
  learning_rate:
    type: "float"
    min: 0.0001
    max: 0.01
    log: true
    default: 0.001
    
  batch_size:
    type: "categorical"
    choices: [16, 32, 64]
    default: 32
    
  epochs:
    type: "int"
    min: 50
    max: 200
    default: 100
    
  warmup_steps:
    type: "int"
    min: 500
    max: 2000
    default: 1000

# Дублируем для совместимости с OptimizationEngine
hyperparameter_search_space:
  learning_rate:
    type: "float"
    min: 0.0001
    max: 0.01
    log: true
    default: 0.001
    
  batch_size:
    type: "categorical"
    choices: [16, 32, 64]
    default: 32
    
  epochs:
    type: "int"
    min: 50
    max: 200
    default: 100
    
  warmup_steps:
    type: "int"
    min: 500
    max: 2000
    default: 1000

# Настройки оптимизации
optimization:
  direction: "minimize"
  objective_metric: "val_loss"
  n_trials: 20
  overfitting_penalty: 0.1
  continue_training: true
  full_epochs_per_trial: 50

# Настройки планирования параметров
parameter_scheduling:
  learning_rate:
    enabled: true
    strategy: "cosine"
    start_value: 0.001
    end_value: 0.0001
    total_steps: 10000
    
parameter_scheduling_config:
  update_frequency: 10

# Настройки раннего останова с проактивными мерами
early_stopping:
  patience_criterion:
    enabled: true
    type: "patience"
    metric: "val_loss"
    patience: 10
    min_delta: 0.001
    mode: "min"
    
  overfitting_criterion:
    enabled: true
    type: "overfitting"
    train_metric: "train_loss"
    val_metric: "val_loss"
    overfitting_threshold: 0.2
    window_size: 5
    
  divergence_criterion:
    enabled: true
    type: "loss_divergence"
    metric: "train_loss"
    divergence_threshold: 10.0

# Проактивные меры предотвращения проблем
proactive_measures:
  enabled: true
  stagnation_threshold: 0.001      # Порог стагнации loss
  overfitting_gap_threshold: 0.5   # Порог разрыва train/val loss
  volatility_threshold: 0.1        # Порог нестабильности
  convergence_threshold: 0.01      # Порог медленной сходимости
  
  # Автоматические корректировки
  auto_lr_adjustment: true         # Автокорректировка learning rate
  auto_regularization: true       # Автокорректировка регуляризации
  auto_gradient_clipping: true    # Автовключение gradient clipping
  auto_optimizer_switch: true     # Автосмена оптимизатора

# Настройки Telegram уведомлений
telegram:
  enabled: true
  bot_token: "6228970684:AAGRnDmTPeLQVsGBiBLKw02tHxaigNWSsok"
  chat_id: "5150585721"

# Настройки реестра моделей
model_registry:
  path: "smart_tuner/models"
  max_models: 5
  best_model_name: "best_model.pt"
  primary_metric: "val_loss"
  minimize_metric: true

# Настройки финального обучения
final_training:
  epochs: 200

# Настройки MLflow
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "tacotron2_optimization"

# Порты для компонентов Smart Tuner V2 (последовательно 5000-5009)
ports:
  mlflow: 5000              # Эксперименты и метрики  
  tensorboard: 5001         # Графики обучения
  streamlit: 5002           # TTS Demo
  log_watcher: 5003         # Мониторинг логов в реальном времени
  metrics_store: 5004       # API для метрик и статистики
  optimization_engine: 5005 # Optuna Dashboard
  param_scheduler: 5006     # Планировщик гиперпараметров
  early_stop_controller: 5007  # Контроллер раннего останова
  alert_manager: 5008       # Управление уведомлениями
  model_registry: 5009      # Реестр моделей

# Настройки ресурсов
resources:
  checkpointing:
    path: "output" 