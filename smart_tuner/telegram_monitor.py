#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Telegram Monitor –¥–ª—è Smart Tuner TTS
–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π alignment –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
import torch
import requests
import yaml
import logging
import io
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path

class TelegramMonitor:
    """
    Telegram –º–æ–Ω–∏—Ç–æ—Ä –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ attachment –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∂–¥—ã–µ 1000 —à–∞–≥–æ–≤.
    """
    
    def __init__(self, config_path: str = "smart_tuner/config.yaml"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        self.logger = logging.getLogger(__name__)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Telegram
        telegram_config = self.config.get('telegram', {})
        self.bot_token = telegram_config.get('bot_token')
        self.chat_id = telegram_config.get('chat_id') 
        self.enabled = telegram_config.get('enabled', False)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.notification_interval = 100  # –ß–∞—â–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        self.last_notification_step = 0
        self.training_history = []
        
        plt.style.use('default')
        self.figure_size = (12, 8)
        self.dpi = 150
        
        self.logger.info("üì± Telegram Monitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
    def should_send_notification(self, current_step: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω—É–∂–Ω–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è."""
        if not self.enabled:
            return False
            
        return (current_step - self.last_notification_step) >= self.notification_interval
    
    def send_training_update(self, step: int, metrics: Dict[str, Any],
                           attention_weights: Optional[torch.Tensor] = None,
                           gate_outputs: Optional[torch.Tensor] = None) -> bool:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏."""
        if not self.should_send_notification(step):
            return False
        
        try:
            # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
            analysis = self._analyze_step(step, metrics, attention_weights, gate_outputs)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            message = self._create_message(analysis)
            self._send_text_message(message)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if attention_weights is not None:
                attention_image = self._create_attention_plot(attention_weights, step)
                if attention_image:
                    self._send_image(attention_image, f"attention_{step}.png",
                                   f"üéØ Attention Matrix - –®–∞–≥ {step}")
            
            # –ì—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫
            metrics_image = self._create_metrics_plot(step)
            if metrics_image:
                self._send_image(metrics_image, f"metrics_{step}.png",
                               f"üìä –ú–µ—Ç—Ä–∏–∫–∏ - –®–∞–≥ {step}")
            
            self.last_notification_step = step
            self.logger.info(f"‚úÖ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–ª—è —à–∞–≥–∞ {step}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {e}")
            return False
    
    def _analyze_step(self, step: int, metrics: Dict[str, Any],
                     attention_weights: Optional[torch.Tensor] = None,
                     gate_outputs: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ —à–∞–≥–∞."""
        analysis = {
            'step': step,
            'metrics': metrics,
            'quality_score': 0.0,
            'phase': self._get_phase(step),
            'issues': [],
            'recommendations': []
        }
        
        # –ê–Ω–∞–ª–∏–∑ attention
        if attention_weights is not None:
            attention_analysis = self._analyze_attention(attention_weights)
            analysis['attention'] = attention_analysis
            analysis['quality_score'] += attention_analysis.get('diagonality', 0) * 0.5
        
        # –ê–Ω–∞–ª–∏–∑ gate
        if gate_outputs is not None:
            gate_analysis = self._analyze_gate(gate_outputs)
            analysis['gate'] = gate_analysis
            analysis['quality_score'] += gate_analysis.get('accuracy', 0) * 0.3
        
        # –ê–Ω–∞–ª–∏–∑ loss
        loss_value = metrics.get('loss', 0)
        analysis['quality_score'] += max(0, 1 - loss_value) * 0.2
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
        analysis['issues'] = self._detect_issues(analysis)
        analysis['recommendations'] = self._get_recommendations(analysis)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.training_history.append(analysis)
        
        return analysis
    
    def _create_message(self, analysis: Dict[str, Any]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è."""
        step = analysis['step']
        phase = analysis['phase']
        quality = analysis['quality_score']
        
        # Emoji –∫–∞—á–µ—Å—Ç–≤–∞
        quality_emoji = "üî•" if quality > 0.8 else "‚úÖ" if quality > 0.6 else "‚ö†Ô∏è" if quality > 0.4 else "‚ùå"
        
        message = f"üß† *Smart Tuner - –û—Ç—á–µ—Ç –û–±—É—á–µ–Ω–∏—è*\n\n"
        message += f"üìç **–®–∞–≥:** `{step:,}`\n"
        message += f"üé≠ **–§–∞–∑–∞:** `{phase}`\n"
        message += f"{quality_emoji} **–ö–∞—á–µ—Å—Ç–≤–æ:** `{quality:.1%}`\n\n"
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = analysis['metrics']
        if 'loss' in metrics:
            message += f"üìâ **Loss:** `{metrics['loss']:.4f}`\n"
        
        # –ê–Ω–∞–ª–∏–∑ attention
        attention = analysis.get('attention', {})
        if attention:
            diag = attention.get('diagonality', 0)
            message += f"üéØ **Attention –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:** `{diag:.1%}`\n"
            
            if diag < 0.3:
                message += f"  ‚ö†Ô∏è *–ö—Ä–∏—Ç–∏—á–Ω–æ –Ω–∏–∑–∫–∞—è! –ü—Ä–æ–±–ª–µ–º—ã —Å alignment*\n"
            elif diag > 0.7:
                message += f"  ‚úÖ *–û—Ç–ª–∏—á–Ω–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å!*\n"
        
        # –ü—Ä–æ–±–ª–µ–º—ã
        issues = analysis.get('issues', [])
        if issues:
            message += f"\n‚ö†Ô∏è **–ü—Ä–æ–±–ª–µ–º—ã:**\n"
            for issue in issues[:2]:
                message += f"  ‚Ä¢ {issue}\n"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = analysis.get('recommendations', [])
        if recommendations:
            message += f"\nüí° **–ß—Ç–æ –¥–µ–ª–∞—Ç—å:**\n"
            for rec in recommendations[:2]:
                message += f"  ‚Ä¢ {rec}\n"
        
        message += f"\nüïê {datetime.now().strftime('%H:%M:%S')}"
        
        return message
    
    def _create_attention_plot(self, attention_weights: torch.Tensor, step: int) -> Optional[bytes]:
        """–°–æ–∑–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ attention matrix."""
        try:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
            if attention_weights.dim() == 4:
                attention = attention_weights[0, 0].detach().cpu().numpy()
            elif attention_weights.dim() == 3:
                attention = attention_weights[0].detach().cpu().numpy()
            else:
                attention = attention_weights.detach().cpu().numpy()
            
            # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã attention –º–∞—Ç—Ä–∏—Ü—ã
            if attention.shape[0] < 2 or attention.shape[1] < 2:
                self.logger.warning(f"Attention –º–∞—Ç—Ä–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è: {attention.shape}")
                return self._create_fallback_attention_plot(attention, step)
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # –û—Å–Ω–æ–≤–Ω–æ–π attention plot
            im1 = ax1.imshow(attention.T, aspect='auto', origin='lower',
                           cmap='Blues', interpolation='nearest')
            ax1.set_title(f'Attention Matrix - –®–∞–≥ {step}', fontweight='bold')
            ax1.set_xlabel('Decoder Steps')
            ax1.set_ylabel('Encoder Steps')
            plt.colorbar(im1, ax=ax1)
            
            # –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è
            min_dim = min(attention.shape)
            diag_x = np.linspace(0, attention.shape[1]-1, min_dim)
            diag_y = np.linspace(0, attention.shape[0]-1, min_dim)
            ax1.plot(diag_x, diag_y, 'r--', alpha=0.7, linewidth=2, label='–ò–¥–µ–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å')
            ax1.legend()
            
            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            diagonality = self._calculate_diagonality(attention)
            monotonicity = self._calculate_monotonicity(attention)
            
            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –∑–Ω–∞—á–µ–Ω–∏–π
            ax2.hist(attention.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            ax2.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Attention Values')
            ax2.set_xlabel('Attention Weight')
            ax2.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            textstr = f'–î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {diagonality:.1%}\n–ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å: {monotonicity:.1%}'
            ax2.text(0.05, 0.95, textstr, transform=ax2.transAxes, fontsize=12,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–π—Ç—ã
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=self.dpi, bbox_inches='tight')
            buffer.seek(0)
            image_data = buffer.getvalue()
            plt.close(fig)
            
            return image_data
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è attention plot: {e}")
            return self._create_fallback_attention_plot(None, step)
    
    def _create_fallback_attention_plot(self, attention: Optional[np.ndarray], step: int) -> Optional[bytes]:
        """–°–æ–∑–¥–∞–µ—Ç fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö attention –º–∞—Ç—Ä–∏—Ü."""
        try:
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            message = f"Attention Matrix - –®–∞–≥ {step}\n\n"
            if attention is not None:
                message += f"–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã: {attention.shape}\n"
                message += f"–ú–∏–Ω –∑–Ω–∞—á–µ–Ω–∏–µ: {attention.min():.4f}\n"
                message += f"–ú–∞–∫—Å –∑–Ω–∞—á–µ–Ω–∏–µ: {attention.max():.4f}\n"
                message += f"–°—Ä–µ–¥–Ω–µ–µ: {attention.mean():.4f}\n\n"
                
                if attention.size > 0:
                    message += "–ú–∞—Ç—Ä–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n"
                    message += "–û–±—É—á–µ–Ω–∏–µ –≤ —Ä–∞–Ω–Ω–µ–π —Å—Ç–∞–¥–∏–∏"
                else:
                    message += "–ü—É—Å—Ç–∞—è attention –º–∞—Ç—Ä–∏—Ü–∞"
            else:
                message += "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ attention –¥–∞–Ω–Ω—ã—Ö"
            
            ax.text(0.5, 0.5, message, ha='center', va='center',
                   transform=ax.transAxes, fontsize=12,
                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
            ax.set_title(f'Attention Status - –®–∞–≥ {step}', fontweight='bold')
            ax.axis('off')
            
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–π—Ç—ã
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=self.dpi, bbox_inches='tight')
            buffer.seek(0)
            image_data = buffer.getvalue()
            plt.close(fig)
            
            return image_data
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è fallback attention plot: {e}")
            return None
    
    def _create_metrics_plot(self, step: int) -> Optional[bytes]:
        """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –º–µ—Ç—Ä–∏–∫."""
        try:
            if len(self.training_history) < 3:
                return None
            
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
            steps = [h['step'] for h in self.training_history[-15:]]
            losses = [h['metrics'].get('loss', 0) for h in self.training_history[-15:]]
            quality_scores = [h['quality_score'] for h in self.training_history[-15:]]
            
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=self.figure_size, sharex=True)
            
            # –ì—Ä–∞—Ñ–∏–∫ loss
            ax1.plot(steps, losses, 'b-', linewidth=2, marker='o', markersize=4, label='Training Loss')
            ax1.set_ylabel('Loss')
            ax1.set_title(f'–ü—Ä–æ–≥—Ä–µ—Å—Å –û–±—É—á–µ–Ω–∏—è - –®–∞–≥ {step}', fontweight='bold')
            ax1.grid(True, alpha=0.3)
            ax1.legend()
            
            # –¢—Ä–µ–Ω–¥ loss
            if len(losses) > 3:
                z = np.polyfit(range(len(losses)), losses, 1)
                p = np.poly1d(z)
                ax1.plot(steps, p(range(len(losses))), "r--", alpha=0.8, label='–¢—Ä–µ–Ω–¥')
                ax1.legend()
            
            # –ì—Ä–∞—Ñ–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
            ax2.plot(steps, quality_scores, 'g-', linewidth=2, marker='s', markersize=4, label='Quality Score')
            ax2.set_ylabel('Quality Score')
            ax2.set_xlabel('Training Step')
            ax2.set_ylim(0, 1)
            ax2.grid(True, alpha=0.3)
            ax2.legend()
            
            # –ó–æ–Ω—ã –∫–∞—á–µ—Å—Ç–≤–∞
            ax2.axhspan(0.8, 1.0, alpha=0.2, color='green', label='–û—Ç–ª–∏—á–Ω–æ')
            ax2.axhspan(0.6, 0.8, alpha=0.2, color='yellow', label='–•–æ—Ä–æ—à–æ')
            ax2.axhspan(0.4, 0.6, alpha=0.2, color='orange', label='–°—Ä–µ–¥–Ω–µ')
            ax2.axhspan(0.0, 0.4, alpha=0.2, color='red', label='–ü–ª–æ—Ö–æ')
            
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–π—Ç—ã
            buffer = io.BytesIO()
            plt.savefig(buffer, format='png', dpi=self.dpi, bbox_inches='tight')
            buffer.seek(0)
            image_data = buffer.getvalue()
            plt.close(fig)
            
            return image_data
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è metrics plot: {e}")
            return None
    
    def _send_text_message(self, message: str) -> bool:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ."""
        url = f"https://api.telegram.org/bot{self.bot_token}/sendMessage"
        
        payload = {
            'chat_id': self.chat_id,
            'text': message,
            'parse_mode': 'Markdown',
            'disable_web_page_preview': True
        }
        
        try:
            response = requests.post(url, json=payload, timeout=10)
            response.raise_for_status()
            return True
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            return False
    
    def _send_image(self, image_data: bytes, filename: str, caption: str = "") -> bool:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
        url = f"https://api.telegram.org/bot{self.bot_token}/sendPhoto"
        
        files = {
            'photo': (filename, image_data, 'image/png')
        }
        
        data = {
            'chat_id': self.chat_id,
            'caption': caption
        }
        
        try:
            response = requests.post(url, files=files, data=data, timeout=30)
            response.raise_for_status()
            return True
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return False
    
    def _analyze_attention(self, attention_weights: torch.Tensor) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç attention –∫–∞—á–µ—Å—Ç–≤–æ."""
        if attention_weights.dim() == 4:
            attention = attention_weights[0, 0].detach().cpu().numpy()
        elif attention_weights.dim() == 3:
            attention = attention_weights[0].detach().cpu().numpy()
        else:
            attention = attention_weights.detach().cpu().numpy()
        
        return {
            'diagonality': self._calculate_diagonality(attention),
            'monotonicity': self._calculate_monotonicity(attention),
            'focus': self._calculate_focus(attention)
        }
    
    def _calculate_diagonality(self, attention_matrix: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å."""
        try:
            T_out, T_in = attention_matrix.shape
            if T_out == 0 or T_in == 0:
                return 0.0
            
            # –°–æ–∑–¥–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –¥–∏–∞–≥–æ–Ω–∞–ª—å
            ideal_diagonal = np.zeros_like(attention_matrix)
            min_dim = min(T_out, T_in)
            
            for i in range(min_dim):
                diagonal_pos = int(i * T_in / T_out) if T_out > 0 else i
                if diagonal_pos < T_in:
                    ideal_diagonal[i, diagonal_pos] = 1.0
            
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –∏–¥–µ–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–æ–Ω–∞–ª—å—é
            attention_flat = attention_matrix.flatten()
            ideal_flat = ideal_diagonal.flatten()
            
            if np.std(attention_flat) == 0 or np.std(ideal_flat) == 0:
                return 0.0
            
            correlation = np.corrcoef(attention_flat, ideal_flat)[0, 1]
            return max(0.0, correlation) if not np.isnan(correlation) else 0.0
            
        except Exception:
            return 0.0
    
    def _calculate_monotonicity(self, attention_matrix: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å."""
        try:
            T_out, T_in = attention_matrix.shape
            if T_out <= 1:
                return 1.0
            
            # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫–∏ attention
            peak_positions = np.argmax(attention_matrix, axis=1)
            
            # –°—á–∏—Ç–∞–µ–º –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
            monotonic_transitions = 0
            for i in range(1, len(peak_positions)):
                if peak_positions[i] >= peak_positions[i-1]:
                    monotonic_transitions += 1
            
            return monotonic_transitions / (len(peak_positions) - 1) if len(peak_positions) > 1 else 1.0
            
        except Exception:
            return 0.0
    
    def _calculate_focus(self, attention_matrix: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫—É."""
        try:
            # –≠–Ω—Ç—Ä–æ–ø–∏—è –ø–æ decoder steps
            entropies = []
            for i in range(attention_matrix.shape[0]):
                attention_step = attention_matrix[i] + 1e-8
                entropy = -np.sum(attention_step * np.log(attention_step + 1e-8))
                entropies.append(entropy)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            max_entropy = np.log(attention_matrix.shape[1])
            avg_entropy = np.mean(entropies)
            focus = 1.0 - (avg_entropy / max_entropy)
            
            return max(0.0, min(1.0, focus))
            
        except Exception:
            return 0.0
    
    def _analyze_gate(self, gate_outputs: torch.Tensor) -> Dict[str, float]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç gate –∫–∞—á–µ—Å—Ç–≤–æ."""
        try:
            gates = gate_outputs[0].detach().cpu().numpy()
            gate_binary = (gates > 0.5).astype(float)
            
            stop_positions = np.where(gate_binary > 0.5)[0]
            
            if len(stop_positions) > 0:
                stop_position = stop_positions[0]
                false_stops = np.sum(gate_binary[stop_position+1:] < 0.5) if stop_position < len(gates)-1 else 0
                accuracy = 1.0 - (false_stops / max(1, len(gates) - stop_position - 1))
            else:
                accuracy = 0.0
            
            return {
                'accuracy': accuracy,
                'stop_position': stop_positions[0] if len(stop_positions) > 0 else len(gates)
            }
            
        except Exception:
            return {'accuracy': 0.0, 'stop_position': 0}
    
    def _get_phase(self, step: int) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è."""
        if step < 500:
            return "pre_alignment"
        elif step < 2000:
            return "alignment_learning"
        elif step < 3000:
            return "quality_optimization"
        else:
            return "fine_tuning"
    
    def _detect_issues(self, analysis: Dict[str, Any]) -> List[str]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã."""
        issues = []
        
        attention = analysis.get('attention', {})
        diagonality = attention.get('diagonality', 0)
        
        if diagonality < 0.3:
            issues.append("–ö—Ä–∞–π–Ω–µ –Ω–∏–∑–∫–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention - –º–æ–¥–µ–ª—å –Ω–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç")
        elif diagonality < 0.5:
            issues.append("–ù–∏–∑–∫–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å - –Ω—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å guided attention")
        
        gate = analysis.get('gate', {})
        if gate.get('accuracy', 0) < 0.5:
            issues.append("–ü–ª–æ—Ö–∞—è —Ä–∞–±–æ—Ç–∞ gate - –º–æ–¥–µ–ª—å –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω–µ—Ü")
        
        if analysis['quality_score'] < 0.4:
            issues.append("–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–∏–∑–∫–æ–µ")
        
        return issues
    
    def _get_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """–î–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é."""
        recommendations = []
        
        phase = analysis.get('phase', '')
        attention = analysis.get('attention', {})
        diagonality = attention.get('diagonality', 0)
        
        if diagonality < 0.5:
            if phase == "pre_alignment":
                recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Å guided attention –¥–æ 10.0")
            else:
                recommendations.append("–°–Ω–∏–∑–∏—Ç—å learning rate –∏ —É—Å–∏–ª–∏—Ç—å guided attention")
        
        gate = analysis.get('gate', {})
        if gate.get('accuracy', 0) < 0.7:
            recommendations.append("–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π gate threshold")
        
        if analysis['quality_score'] < 0.6:
            recommendations.append("–í–∫–ª—é—á–∏—Ç—å curriculum learning")
        
        return recommendations 