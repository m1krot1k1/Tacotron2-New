#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Quality Controller –¥–ª—è Smart Tuner TTS
–†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π 2024-2025:
- Very Attentive Tacotron (Google, 2025)
- Llasa: Scaling LLM TTS (2025) 
- Muyan-TTS: Podcast Optimization (2025)
- MonoAlign: Robust LLM TTS (2024)
- Style-BERT-VITS2: Expressive TTS (2025)
"""

import numpy as np
import torch
import torch.nn.functional as F
import yaml
import logging
from typing import Dict, Any, List, Tuple, Optional
from pathlib import Path
import json
import sqlite3
from datetime import datetime

class AdvancedQualityController:
    """
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ TTS —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏:
    1. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ quality –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
    3. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è
    4. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ guided attention
    5. –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    """
    
    def __init__(self, config_path: str = "smart_tuner/config.yaml"):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
            
        self.logger = logging.getLogger(__name__)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        self.quality_config = self.config.get('quality_monitoring', {})
        self.guided_attention_config = self.config.get('guided_attention', {})
        self.tts_metrics_config = self.config.get('tts_metrics', {})
        
        # –ò—Å—Ç–æ—Ä–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è
        self.quality_history = []
        self.attention_history = []
        self.gate_history = []
        self.mel_quality_history = []
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
        self.adaptive_thresholds = {
            'attention_diagonality': 0.7,
            'gate_accuracy': 0.75,
            'mel_clarity': 0.6,
            'training_stability': 0.8
        }
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–∞
        self.current_phase = "pre_alignment"
        self.quality_interventions = []
        self.last_intervention_step = 0
        
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∫–∞—á–µ—Å—Ç–≤–∞
        self.db_path = "smart_tuner/quality_control_history.db"
        self._init_quality_database()
        
    def _init_quality_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
            CREATE TABLE IF NOT EXISTS quality_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                epoch INTEGER,
                training_phase TEXT,
                attention_diagonality REAL,
                attention_monotonicity REAL,
                attention_focus REAL,
                gate_accuracy REAL,
                gate_precision REAL,
                gate_recall REAL,
                mel_spectral_quality REAL,
                mel_temporal_consistency REAL,
                overall_quality_score REAL,
                quality_issues TEXT,
                interventions_applied TEXT
            )
            ''')
            
            conn.commit()
            conn.close()
            self.logger.info(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {self.db_path}")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
    
    def analyze_training_quality(self, epoch: int, metrics: Dict[str, Any], 
                                attention_weights: Optional[torch.Tensor] = None,
                                gate_outputs: Optional[torch.Tensor] = None,
                                mel_outputs: Optional[torch.Tensor] = None) -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è TTS.
        
        Args:
            epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            attention_weights: –í–µ—Å–∞ attention (B, T_out, T_in)
            gate_outputs: –í—ã—Ö–æ–¥—ã gate (B, T_out)
            mel_outputs: Mel —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã (B, n_mels, T_out)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        quality_analysis = {
            'epoch': epoch,
            'phase': self._determine_training_phase(epoch, metrics),
            'attention_quality': {},
            'gate_quality': {},
            'mel_quality': {},
            'overall_quality_score': 0.0,
            'quality_issues': [],
            'recommended_interventions': [],
            'quality_trend': 'stable'
        }
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ attention
        if attention_weights is not None:
            quality_analysis['attention_quality'] = self._analyze_attention_quality(attention_weights)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ gate
        if gate_outputs is not None:
            quality_analysis['gate_quality'] = self._analyze_gate_quality(gate_outputs)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ mel
        if mel_outputs is not None:
            quality_analysis['mel_quality'] = self._analyze_mel_quality(mel_outputs)
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
        quality_issues = self._detect_quality_issues(quality_analysis)
        quality_analysis['quality_issues'] = quality_issues
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤—É
        interventions = self._recommend_quality_interventions(quality_analysis)
        quality_analysis['recommended_interventions'] = interventions
        
        # –û–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
        overall_score = self._calculate_overall_quality_score(quality_analysis)
        quality_analysis['overall_quality_score'] = overall_score
        
        # –¢—Ä–µ–Ω–¥ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_trend = self._analyze_quality_trend()
        quality_analysis['quality_trend'] = quality_trend
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.quality_history.append(quality_analysis)
        self._save_quality_to_database(quality_analysis)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤
        self._update_adaptive_thresholds(quality_analysis)
        
        return quality_analysis
    
    def _analyze_attention_quality(self, attention_weights: torch.Tensor) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ attention matrix –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π.
        """
        # attention_weights: (B, T_out, T_in)
        B, T_out, T_in = attention_weights.shape
        
        attention_quality = {
            'diagonality_score': 0.0,
            'monotonicity_score': 0.0,
            'focus_score': 0.0,
            'entropy_score': 0.0,
            'alignment_consistency': 0.0,
            'attention_drift': 0.0
        }
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ numpy –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        attention_np = attention_weights.detach().cpu().numpy()
        
        for b in range(B):
            att_matrix = attention_np[b]  # (T_out, T_in)
            
            # 1. –î–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–∏–∑ Very Attentive Tacotron)
            diagonality = self._calculate_attention_diagonality(att_matrix)
            attention_quality['diagonality_score'] += diagonality
            
            # 2. –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å (–∏–∑ MonoAlign –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π)
            monotonicity = self._calculate_attention_monotonicity(att_matrix)
            attention_quality['monotonicity_score'] += monotonicity
            
            # 3. –§–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è attention)
            focus = self._calculate_attention_focus(att_matrix)
            attention_quality['focus_score'] += focus
            
            # 4. –≠–Ω—Ç—Ä–æ–ø–∏—è (–º–µ—Ä–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏)
            entropy = self._calculate_attention_entropy(att_matrix)
            attention_quality['entropy_score'] += entropy
            
            # 5. –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å alignment
            consistency = self._calculate_alignment_consistency(att_matrix)
            attention_quality['alignment_consistency'] += consistency
            
            # 6. –î—Ä–µ–π—Ñ attention
            drift = self._calculate_attention_drift(att_matrix)
            attention_quality['attention_drift'] += drift
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ batch
        for key in attention_quality:
            attention_quality[key] /= B
        
        return attention_quality
    
    def _calculate_attention_diagonality(self, attention_matrix: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention matrix.
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ Very Attentive Tacotron (2025).
        """
        T_out, T_in = attention_matrix.shape
        
        # –°–æ–∑–¥–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É
        ideal_diagonal = np.zeros((T_out, T_in))
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å
        for i in range(T_out):
            # –ò–¥–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
            ideal_pos = int(i * T_in / T_out)
            if ideal_pos < T_in:
                ideal_diagonal[i, ideal_pos] = 1.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å –∏–¥–µ–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–æ–Ω–∞–ª—å—é
        attention_flat = attention_matrix.flatten()
        diagonal_flat = ideal_diagonal.flatten()
        
        correlation = np.corrcoef(attention_flat, diagonal_flat)[0, 1]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º NaN (–º–æ–∂–µ—Ç –≤–æ–∑–Ω–∏–∫–Ω—É—Ç—å –ø—Ä–∏ –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏)
        if np.isnan(correlation):
            correlation = 0.0
        
        return max(0.0, correlation)
    
    def _calculate_attention_monotonicity(self, attention_matrix: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å attention.
        –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ MonoAlign –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è—Ö (2024).
        """
        T_out, T_in = attention_matrix.shape
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–∏–∫ attention –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —à–∞–≥–∞
        peak_positions = np.argmax(attention_matrix, axis=1)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å –∫–∞–∫ –¥–æ–ª—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —É–≤–µ–ª–∏—á–µ–Ω–∏–π
        monotonic_steps = 0
        total_steps = len(peak_positions) - 1
        
        if total_steps == 0:
            return 1.0
        
        for i in range(total_steps):
            if peak_positions[i+1] >= peak_positions[i]:
                monotonic_steps += 1
        
        return monotonic_steps / total_steps
    
    def _calculate_attention_focus(self, attention_matrix: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫—É attention (–æ–±—Ä–∞—Ç–Ω–æ–µ –∫ —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏).
        """
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –æ—Å—Ç—Ä–æ—Ç—É attention –ø–æ –≤—Å–µ–º –≤—ã—Ö–æ–¥–Ω—ã–º —à–∞–≥–∞–º
        focus_scores = []
        
        for t_out in range(attention_matrix.shape[0]):
            att_weights = attention_matrix[t_out]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é –∫–∞–∫ –æ–±—Ä–∞—Ç–Ω–æ–µ –∫ —ç–Ω—Ç—Ä–æ–ø–∏–∏
            # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è log(0)
            att_weights_clean = att_weights + 1e-10
            entropy = -np.sum(att_weights_clean * np.log(att_weights_clean))
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é
            max_entropy = np.log(len(att_weights))
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            
            # Focus = 1 - normalized_entropy
            focus = 1.0 - normalized_entropy
            focus_scores.append(focus)
        
        return np.mean(focus_scores)
    
    def _calculate_attention_entropy(self, attention_matrix: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é —ç–Ω—Ç—Ä–æ–ø–∏—é attention.
        """
        entropies = []
        
        for t_out in range(attention_matrix.shape[0]):
            att_weights = attention_matrix[t_out]
            
            # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏
            att_weights_clean = att_weights + 1e-10
            entropy = -np.sum(att_weights_clean * np.log(att_weights_clean))
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            max_entropy = np.log(len(att_weights))
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            
            entropies.append(normalized_entropy)
        
        return np.mean(entropies)
    
    def _calculate_alignment_consistency(self, attention_matrix: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å alignment –º–µ–∂–¥—É —Å–æ—Å–µ–¥–Ω–∏–º–∏ —à–∞–≥–∞–º–∏.
        """
        T_out = attention_matrix.shape[0]
        
        if T_out < 2:
            return 1.0
        
        consistencies = []
        
        for i in range(T_out - 1):
            att_curr = attention_matrix[i]
            att_next = attention_matrix[i + 1]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            dot_product = np.dot(att_curr, att_next)
            norm_curr = np.linalg.norm(att_curr)
            norm_next = np.linalg.norm(att_next)
            
            if norm_curr > 0 and norm_next > 0:
                similarity = dot_product / (norm_curr * norm_next)
                consistencies.append(max(0.0, similarity))
        
        return np.mean(consistencies) if consistencies else 0.0
    
    def _calculate_attention_drift(self, attention_matrix: np.ndarray) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –¥—Ä–µ–π—Ñ attention (–Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫–∞—á–∫–∏).
        """
        peak_positions = np.argmax(attention_matrix, axis=1)
        
        if len(peak_positions) < 2:
            return 0.0
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        expected_step = len(peak_positions[0]) / len(peak_positions)
        drifts = []
        
        for i in range(1, len(peak_positions)):
            expected_pos = peak_positions[0] + i * expected_step
            actual_pos = peak_positions[i]
            drift = abs(actual_pos - expected_pos) / len(peak_positions[0])
            drifts.append(drift)
        
        return np.mean(drifts)
    
    def _analyze_gate_quality(self, gate_outputs: torch.Tensor) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ gate outputs.
        """
        # gate_outputs: (B, T_out)
        gate_np = gate_outputs.detach().cpu().numpy()
        
        gate_quality = {
            'accuracy': 0.0,
            'precision': 0.0,
            'recall': 0.0,
            'f1_score': 0.0,
            'premature_stop_rate': 0.0,
            'gate_stability': 0.0
        }
        
        B, T_out = gate_np.shape
        
        for b in range(B):
            gates = gate_np[b]
            
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ gate –¥–æ–ª–∂–µ–Ω –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å—Å—è –±–ª–∏–∂–µ –∫ –∫–æ–Ω—Ü—É
            # –ò–¥–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è - –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10% –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            ideal_start = int(T_out * 0.9)
            
            # –°–æ–∑–¥–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é gate –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            ideal_gates = np.zeros(T_out)
            ideal_gates[ideal_start:] = 1.0
            
            # –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è gate
            binary_gates = (gates > 0.5).astype(int)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            tp = np.sum((binary_gates == 1) & (ideal_gates == 1))
            fp = np.sum((binary_gates == 1) & (ideal_gates == 0))
            fn = np.sum((binary_gates == 0) & (ideal_gates == 1))
            tn = np.sum((binary_gates == 0) & (ideal_gates == 0))
            
            accuracy = (tp + tn) / T_out if T_out > 0 else 0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            gate_quality['accuracy'] += accuracy
            gate_quality['precision'] += precision
            gate_quality['recall'] += recall
            gate_quality['f1_score'] += f1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω—É—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É
            first_gate_activation = np.argmax(binary_gates)
            premature_rate = 1.0 - (first_gate_activation / T_out) if T_out > 0 else 0
            gate_quality['premature_stop_rate'] += premature_rate
            
            # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å gate (–Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è)
            gate_variance = np.var(gates)
            stability = 1.0 / (1.0 + gate_variance)  # –ú–µ–Ω—å—à–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è = –±–æ–ª—å—à–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            gate_quality['gate_stability'] += stability
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ batch
        for key in gate_quality:
            gate_quality[key] /= B
        
        return gate_quality
    
    def _analyze_mel_quality(self, mel_outputs: torch.Tensor) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ mel —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º.
        """
        # mel_outputs: (B, n_mels, T_out)
        mel_np = mel_outputs.detach().cpu().numpy()
        
        mel_quality = {
            'spectral_quality': 0.0,
            'temporal_consistency': 0.0,
            'harmonic_clarity': 0.0,
            'noise_level': 0.0,
            'dynamic_range': 0.0
        }
        
        B, n_mels, T_out = mel_np.shape
        
        for b in range(B):
            mel_spec = mel_np[b]  # (n_mels, T_out)
            
            # 1. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (—ç–Ω–µ—Ä–≥–∏—è –≤ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç–∞—Ö)
            low_freq_energy = np.mean(mel_spec[:n_mels//3])  # –ù–∏–∑–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
            mid_freq_energy = np.mean(mel_spec[n_mels//3:2*n_mels//3])  # –°—Ä–µ–¥–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã
            high_freq_energy = np.mean(mel_spec[2*n_mels//3:])  # –í—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
            
            # –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –±–∞–ª–∞–Ω—Å —ç–Ω–µ—Ä–≥–∏–π
            spectral_balance = 1.0 - np.std([low_freq_energy, mid_freq_energy, high_freq_energy])
            mel_quality['spectral_quality'] += max(0.0, spectral_balance)
            
            # 2. –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
            temporal_diffs = np.diff(mel_spec, axis=1)
            temporal_variance = np.mean(np.var(temporal_diffs, axis=1))
            temporal_consistency = 1.0 / (1.0 + temporal_variance)
            mel_quality['temporal_consistency'] += temporal_consistency
            
            # 3. –ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–∞—è —á–µ—Ç–∫–æ—Å—Ç—å (–ø–∏–∫–∏ –≤ —Å–ø–µ–∫—Ç—Ä–µ)
            mel_fft = np.fft.fft(mel_spec, axis=0)
            harmonic_peaks = np.sum(np.abs(mel_fft) > np.mean(np.abs(mel_fft)) * 2)
            harmonic_clarity = harmonic_peaks / n_mels
            mel_quality['harmonic_clarity'] += harmonic_clarity
            
            # 4. –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (–≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è)
            noise_level = np.mean(mel_spec[-n_mels//4:])  # –°–∞–º—ã–µ –≤—ã—Å–æ–∫–∏–µ —á–∞—Å—Ç–æ—Ç—ã
            mel_quality['noise_level'] += noise_level
            
            # 5. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
            dynamic_range = np.max(mel_spec) - np.min(mel_spec)
            mel_quality['dynamic_range'] += dynamic_range
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ batch
        for key in mel_quality:
            mel_quality[key] /= B
        
        return mel_quality
    
    def _detect_quality_issues(self, quality_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞.
        """
        issues = []
        
        attention_quality = quality_analysis.get('attention_quality', {})
        gate_quality = quality_analysis.get('gate_quality', {})
        mel_quality = quality_analysis.get('mel_quality', {})
        
        # –ü—Ä–æ–±–ª–µ–º—ã attention
        if attention_quality.get('diagonality_score', 0) < self.adaptive_thresholds['attention_diagonality']:
            issues.append({
                'type': 'attention_misalignment',
                'severity': 'high',
                'description': f"–ù–∏–∑–∫–∞—è –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention: {attention_quality.get('diagonality_score', 0):.3f}",
                'recommended_fix': 'increase_guided_attention_weight'
            })
        
        if attention_quality.get('monotonicity_score', 0) < 0.7:
            issues.append({
                'type': 'attention_non_monotonic',
                'severity': 'medium',
                'description': f"–ù–∞—Ä—É—à–µ–Ω–∏–µ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏: {attention_quality.get('monotonicity_score', 0):.3f}",
                'recommended_fix': 'apply_monotonic_loss'
            })
        
        if attention_quality.get('focus_score', 0) < 0.5:
            issues.append({
                'type': 'attention_blurry',
                'severity': 'medium',
                'description': f"–†–∞–∑–º—ã—Ç–æ–µ attention: {attention_quality.get('focus_score', 0):.3f}",
                'recommended_fix': 'reduce_attention_dropout'
            })
        
        # –ü—Ä–æ–±–ª–µ–º—ã gate
        if gate_quality.get('accuracy', 0) < self.adaptive_thresholds['gate_accuracy']:
            issues.append({
                'type': 'gate_inaccuracy',
                'severity': 'high',
                'description': f"–ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å gate: {gate_quality.get('accuracy', 0):.3f}",
                'recommended_fix': 'increase_gate_loss_weight'
            })
        
        if gate_quality.get('premature_stop_rate', 0) > 0.3:
            issues.append({
                'type': 'premature_stopping',
                'severity': 'high',
                'description': f"–ü—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: {gate_quality.get('premature_stop_rate', 0):.3f}",
                'recommended_fix': 'adjust_gate_threshold'
            })
        
        # –ü—Ä–æ–±–ª–µ–º—ã mel
        if mel_quality.get('spectral_quality', 0) < self.adaptive_thresholds['mel_clarity']:
            issues.append({
                'type': 'poor_spectral_quality',
                'severity': 'medium',
                'description': f"–ù–∏–∑–∫–æ–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {mel_quality.get('spectral_quality', 0):.3f}",
                'recommended_fix': 'add_spectral_loss'
            })
        
        if mel_quality.get('noise_level', 0) > 0.3:
            issues.append({
                'type': 'high_noise_level',
                'severity': 'medium',
                'description': f"–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞: {mel_quality.get('noise_level', 0):.3f}",
                'recommended_fix': 'improve_audio_preprocessing'
            })
        
        return issues
    
    def _recommend_quality_interventions(self, quality_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.
        """
        interventions = []
        issues = quality_analysis.get('quality_issues', [])
        
        for issue in issues:
            recommended_fix = issue.get('recommended_fix')
            
            if recommended_fix == 'increase_guided_attention_weight':
                interventions.append({
                    'type': 'guided_attention_boost',
                    'params': {
                        'guide_loss_weight_multiplier': 1.5,
                        'guide_decay_slower': 0.9995
                    },
                    'description': '–£—Å–∏–ª–µ–Ω–∏–µ guided attention –¥–ª—è –ª—É—á—à–µ–≥–æ alignment'
                })
            
            elif recommended_fix == 'apply_monotonic_loss':
                interventions.append({
                    'type': 'monotonic_loss_addition',
                    'params': {
                        'monotonic_loss_weight': 0.1
                    },
                    'description': '–î–æ–±–∞–≤–ª–µ–Ω–∏–µ monotonic loss –¥–ª—è –ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏—è –∫ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏'
                })
            
            elif recommended_fix == 'reduce_attention_dropout':
                interventions.append({
                    'type': 'attention_dropout_reduction',
                    'params': {
                        'attention_dropout_multiplier': 0.5
                    },
                    'description': '–°–Ω–∏–∂–µ–Ω–∏–µ attention dropout –¥–ª—è –±–æ–ª—å—à–µ–π —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏'
                })
            
            elif recommended_fix == 'increase_gate_loss_weight':
                interventions.append({
                    'type': 'gate_loss_boost',
                    'params': {
                        'gate_loss_weight_multiplier': 1.3
                    },
                    'description': '–£—Å–∏–ª–µ–Ω–∏–µ gate loss –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏'
                })
            
            elif recommended_fix == 'adjust_gate_threshold':
                interventions.append({
                    'type': 'gate_threshold_adjustment',
                    'params': {
                        'gate_threshold_adjustment': -0.1
                    },
                    'description': '–°–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ gate –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–µ–∂–¥–µ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏'
                })
            
            elif recommended_fix == 'add_spectral_loss':
                interventions.append({
                    'type': 'spectral_loss_addition',
                    'params': {
                        'spectral_loss_weight': 0.2
                    },
                    'description': '–î–æ–±–∞–≤–ª–µ–Ω–∏–µ spectral loss –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ mel'
                })
        
        return interventions
    
    def _calculate_overall_quality_score(self, quality_analysis: Dict[str, Any]) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â–∏–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫.
        """
        attention_quality = quality_analysis.get('attention_quality', {})
        gate_quality = quality_analysis.get('gate_quality', {})
        mel_quality = quality_analysis.get('mel_quality', {})
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
        weights = {
            'attention': 0.4,
            'gate': 0.35,
            'mel': 0.25
        }
        
        # Attention score
        attention_score = (
            attention_quality.get('diagonality_score', 0) * 0.3 +
            attention_quality.get('monotonicity_score', 0) * 0.25 +
            attention_quality.get('focus_score', 0) * 0.25 +
            attention_quality.get('alignment_consistency', 0) * 0.2
        )
        
        # Gate score
        gate_score = (
            gate_quality.get('accuracy', 0) * 0.4 +
            gate_quality.get('f1_score', 0) * 0.3 +
            (1.0 - gate_quality.get('premature_stop_rate', 1)) * 0.3
        )
        
        # Mel score
        mel_score = (
            mel_quality.get('spectral_quality', 0) * 0.4 +
            mel_quality.get('temporal_consistency', 0) * 0.3 +
            mel_quality.get('harmonic_clarity', 0) * 0.3
        )
        
        # –û–±—â–∏–π —Å–∫–æ—Ä
        overall_score = (
            attention_score * weights['attention'] +
            gate_score * weights['gate'] +
            mel_score * weights['mel']
        )
        
        return overall_score
    
    def _analyze_quality_trend(self) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–ø–æ—Ö.
        """
        if len(self.quality_history) < 3:
            return 'insufficient_data'
        
        recent_scores = [q['overall_quality_score'] for q in self.quality_history[-5:]]
        
        # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
        x = np.arange(len(recent_scores))
        coeffs = np.polyfit(x, recent_scores, 1)
        slope = coeffs[0]
        
        if slope > 0.01:
            return 'improving'
        elif slope < -0.01:
            return 'degrading'
        else:
            return 'stable'
    
    def _update_adaptive_thresholds(self, quality_analysis: Dict[str, Any]):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∞–∑—ã –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞.
        """
        phase = quality_analysis['phase']
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ä–æ–≥–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–∞–∑—ã
        if phase == 'pre_alignment':
            self.adaptive_thresholds['attention_diagonality'] = 0.5
            self.adaptive_thresholds['gate_accuracy'] = 0.6
        elif phase == 'alignment_learning':
            self.adaptive_thresholds['attention_diagonality'] = 0.7
            self.adaptive_thresholds['gate_accuracy'] = 0.75
        elif phase == 'quality_optimization':
            self.adaptive_thresholds['attention_diagonality'] = 0.85
            self.adaptive_thresholds['gate_accuracy'] = 0.8
        elif phase == 'fine_tuning':
            self.adaptive_thresholds['attention_diagonality'] = 0.9
            self.adaptive_thresholds['gate_accuracy'] = 0.85
    
    def _determine_training_phase(self, epoch: int, metrics: Dict[str, Any]) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â—É—é —Ñ–∞–∑—É –æ–±—É—á–µ–Ω–∏—è.
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–ø–æ—Ö–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–∑—ã
        attention_score = metrics.get('attention_alignment_score', 0)
        gate_accuracy = metrics.get('gate_accuracy', 0)
        
        if epoch < 500 or attention_score < 0.5:
            return 'pre_alignment'
        elif epoch < 2000 and attention_score < 0.8:
            return 'alignment_learning'
        elif epoch < 3000 and attention_score >= 0.8:
            return 'quality_optimization'
        else:
            return 'fine_tuning'
    
    def _save_quality_to_database(self, quality_analysis: Dict[str, Any]):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            attention_quality = quality_analysis.get('attention_quality', {})
            gate_quality = quality_analysis.get('gate_quality', {})
            mel_quality = quality_analysis.get('mel_quality', {})
            
            cursor.execute('''
            INSERT INTO quality_history (
                epoch, training_phase, attention_diagonality, attention_monotonicity,
                attention_focus, gate_accuracy, gate_precision, gate_recall,
                mel_spectral_quality, mel_temporal_consistency, overall_quality_score,
                quality_issues, interventions_applied
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                quality_analysis['epoch'],
                quality_analysis['phase'],
                attention_quality.get('diagonality_score', 0),
                attention_quality.get('monotonicity_score', 0),
                attention_quality.get('focus_score', 0),
                gate_quality.get('accuracy', 0),
                gate_quality.get('precision', 0),
                gate_quality.get('recall', 0),
                mel_quality.get('spectral_quality', 0),
                mel_quality.get('temporal_consistency', 0),
                quality_analysis['overall_quality_score'],
                json.dumps(quality_analysis['quality_issues']),
                json.dumps(quality_analysis['recommended_interventions'])
            ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
    
    def apply_quality_intervention(self, intervention: Dict[str, Any], 
                                  current_hyperparams: Dict[str, Any], 
                                  step: int = 0, telegram_monitor=None) -> Dict[str, Any]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.
        
        Args:
            intervention: –û–ø–∏—Å–∞–Ω–∏–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞
            current_hyperparams: –¢–µ–∫—É—â–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            step: –¢–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è
            telegram_monitor: –ú–æ–Ω–∏—Ç–æ—Ä –¥–ª—è Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
            
        Returns:
            –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        new_hyperparams = current_hyperparams.copy()
        intervention_type = intervention['type']
        params = intervention.get('params', {})
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        old_params = {}
        changed_params = {}
        
        if intervention_type == 'guided_attention_boost':
            multiplier = params.get('guide_loss_weight_multiplier', 1.5)
            old_params['guide_loss_weight'] = current_hyperparams.get('guide_loss_weight', 1.0)
            new_hyperparams['guide_loss_weight'] = old_params['guide_loss_weight'] * multiplier
            changed_params['guide_loss_weight'] = new_hyperparams['guide_loss_weight']
            
            slower_decay = params.get('guide_decay_slower', 0.9995)
            if 'guide_decay' in current_hyperparams:
                old_params['guide_decay'] = current_hyperparams['guide_decay']
                new_hyperparams['guide_decay'] = slower_decay
                changed_params['guide_decay'] = slower_decay
        
        elif intervention_type == 'attention_dropout_reduction':
            multiplier = params.get('attention_dropout_multiplier', 0.5)
            old_params['p_attention_dropout'] = current_hyperparams.get('p_attention_dropout', 0.1)
            new_hyperparams['p_attention_dropout'] = old_params['p_attention_dropout'] * multiplier
            changed_params['p_attention_dropout'] = new_hyperparams['p_attention_dropout']
        
        elif intervention_type == 'gate_loss_boost':
            multiplier = params.get('gate_loss_weight_multiplier', 1.3)
            old_params['gate_loss_weight'] = current_hyperparams.get('gate_loss_weight', 1.0)
            new_hyperparams['gate_loss_weight'] = old_params['gate_loss_weight'] * multiplier
            changed_params['gate_loss_weight'] = new_hyperparams['gate_loss_weight']
        
        elif intervention_type == 'gate_threshold_adjustment':
            adjustment = params.get('gate_threshold_adjustment', -0.1)
            old_params['gate_threshold'] = current_hyperparams.get('gate_threshold', 0.5)
            new_hyperparams['gate_threshold'] = old_params['gate_threshold'] + adjustment
            changed_params['gate_threshold'] = new_hyperparams['gate_threshold']
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ
        self.quality_interventions.append({
            'epoch': len(self.quality_history),
            'intervention': intervention,
            'timestamp': datetime.now().isoformat()
        })
        
        self.last_intervention_step = len(self.quality_history)
        
        self.logger.info(f"‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ: {intervention_type}")
        
        # üì± –û—Ç–ø—Ä–∞–≤–ª—è–µ–º Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —É–ª—É—á—à–µ–Ω–∏–∏
        if telegram_monitor and changed_params:
            reason = intervention.get('reason', f"–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≤ –æ–±–ª–∞—Å—Ç–∏: {intervention_type.replace('_', ' ')}")
            try:
                telegram_monitor.send_auto_improvement_notification(
                    improvement_type=intervention_type,
                    old_params=old_params,
                    new_params=changed_params,
                    reason=reason,
                    step=step
                )
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ: {e}")
        
        return new_hyperparams
    
    def get_quality_summary(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –æ–±—É—á–µ–Ω–∏—è.
        """
        if not self.quality_history:
            return {'status': 'no_data'}
        
        latest_quality = self.quality_history[-1]
        
        summary = {
            'current_phase': latest_quality['phase'],
            'overall_quality_score': latest_quality['overall_quality_score'],
            'quality_trend': latest_quality['quality_trend'],
            'active_issues': len(latest_quality['quality_issues']),
            'interventions_applied': len(self.quality_interventions),
            'training_epochs_analyzed': len(self.quality_history),
            'quality_breakdown': {
                'attention_diagonality': latest_quality['attention_quality'].get('diagonality_score', 0),
                'gate_accuracy': latest_quality['gate_quality'].get('accuracy', 0),
                'mel_quality': latest_quality['mel_quality'].get('spectral_quality', 0)
            },
            'recommendations': latest_quality['recommended_interventions'][:3]  # –¢–æ–ø-3 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        }
        
        return summary 