#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Tuner Integration Module
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Smart Tuner —Å –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è Tacotron2.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è
2. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–æ–º TTS
3. –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è
4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Smart Tuner
"""

import torch
import numpy as np
import yaml
import logging
import os
from typing import Dict, Any, Optional, Tuple
from pathlib import Path

# –ò–º–ø–æ—Ä—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Smart Tuner
try:
    from .optimization_engine import OptimizationEngine
    from .early_stop_controller import EarlyStopController
    from .intelligent_epoch_optimizer import IntelligentEpochOptimizer
    from .advanced_quality_controller import AdvancedQualityController
except ImportError:
    # Fallback –¥–ª—è —Å–ª—É—á–∞–µ–≤, –∫–æ–≥–¥–∞ –º–æ–¥—É–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
    OptimizationEngine = None
    EarlyStopController = None
    IntelligentEpochOptimizer = None
    AdvancedQualityController = None

class SmartTunerIntegration:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Smart Tuner —Å –æ–±—É—á–µ–Ω–∏–µ–º Tacotron2.
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    - –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏  
    - –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π
    - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
    """
    
    def __init__(self, config_path: str = "smart_tuner/config.yaml", enable_all_features: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Smart Tuner Integration.
        
        Args:
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Smart Tuner
            enable_all_features: –í–∫–ª—é—á–∏—Ç—å –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ (–º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
        """
        self.config_path = config_path
        self.enable_all_features = enable_all_features
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.config = self._load_config()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logger = self._setup_logger()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Smart Tuner
        self.optimization_engine = None
        self.early_stop_controller = None
        self.epoch_optimizer = None
        self.quality_controller = None
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        self.is_initialized = False
        self.current_epoch = 0
        self.training_metrics_history = []
        self.hyperparameter_adjustments = []
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_components()
        
    def _load_config(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Smart Tuner."""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            self.logger.warning(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Smart Tuner –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.config_path}")
            return self._get_default_config()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        return {
            'smart_tuner_enabled': True,
            'optimization_enabled': True,
            'quality_control_enabled': True,
            'early_stopping_enabled': True,
            'adaptive_learning_enabled': True
        }
    
    def _setup_logger(self) -> logging.Logger:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Smart Tuner."""
        logger = logging.getLogger('SmartTunerIntegration')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - [SmartTuner] - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Smart Tuner."""
        try:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫
            if OptimizationEngine and self.config.get('optimization_enabled', True):
                self.optimization_engine = OptimizationEngine(self.config_path)
                self.logger.info("‚úÖ Optimization Engine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # –ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä —Ä–∞–Ω–Ω–µ–≥–æ –æ—Å—Ç–∞–Ω–æ–≤–∞
            if EarlyStopController and self.config.get('early_stopping_enabled', True):
                self.early_stop_controller = EarlyStopController(self.config_path)
                self.logger.info("‚úÖ Early Stop Controller –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —ç–ø–æ—Ö
            if IntelligentEpochOptimizer and self.config.get('adaptive_learning_enabled', True):
                self.epoch_optimizer = IntelligentEpochOptimizer(self.config_path)
                self.logger.info("‚úÖ Intelligent Epoch Optimizer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # –ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –∫–∞—á–µ—Å—Ç–≤–∞
            if AdvancedQualityController and self.config.get('quality_control_enabled', True):
                self.quality_controller = AdvancedQualityController(self.config_path)
                self.logger.info("‚úÖ Advanced Quality Controller –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            self.is_initialized = True
            self.logger.info("üöÄ Smart Tuner Integration —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Smart Tuner: {e}")
            self.is_initialized = False
    
    def analyze_dataset_for_training(self, dataset_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            dataset_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ (—Ä–∞–∑–º–µ—Ä, –∫–∞—á–µ—Å—Ç–≤–æ, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)
            
        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—É—á–µ–Ω–∏—é
        """
        recommendations = {
            'optimal_epochs': 3000,
            'recommended_batch_size': 12,
            'learning_rate_range': (1e-6, 5e-5),
            'quality_expectations': 'good',
            'estimated_training_time_hours': 8.0
        }
        
        if self.epoch_optimizer:
            try:
                analysis = self.epoch_optimizer.analyze_dataset(dataset_info)
                recommendations.update(analysis)
                self.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω: {analysis.get('recommended_epochs_range')}")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        
        return recommendations
    
    def on_training_start(self, initial_hyperparams: Dict[str, Any], dataset_info: Optional[Dict] = None) -> Dict[str, Any]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
        
        Args:
            initial_hyperparams: –ù–∞—á–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            dataset_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            
        Returns:
            –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞—Ä—Ç–∞
        """
        if not self.is_initialized:
            self.logger.warning("Smart Tuner –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
            return initial_hyperparams
        
        optimized_params = initial_hyperparams.copy()
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if dataset_info and self.epoch_optimizer:
            dataset_recommendations = self.analyze_dataset_for_training(dataset_info)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
            if 'optimal_epochs' in dataset_recommendations:
                optimized_params['epochs'] = dataset_recommendations['optimal_epochs']
            
            if 'recommended_batch_size' in dataset_recommendations:
                optimized_params['batch_size'] = dataset_recommendations['recommended_batch_size']
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        changes = []
        for key in optimized_params:
            if key in initial_hyperparams and optimized_params[key] != initial_hyperparams[key]:
                changes.append(f"{key}: {initial_hyperparams[key]} ‚Üí {optimized_params[key]}")
        
        if changes:
            self.logger.info(f"üîß –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {', '.join(changes)}")
        
        return optimized_params
    
    def on_epoch_start(self, epoch: int, current_hyperparams: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏.
        
        Args:
            epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏
            current_hyperparams: –¢–µ–∫—É—â–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –í–æ–∑–º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        self.current_epoch = epoch
        
        if not self.is_initialized:
            return current_hyperparams
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —ç–ø–æ—Ö
        if self.epoch_optimizer:
            try:
                # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                basic_metrics = {
                    'epoch': epoch,
                    'learning_rate': current_hyperparams.get('learning_rate', 1e-4)
                }
                
                progress_analysis = self.epoch_optimizer.monitor_training_progress(epoch, basic_metrics)
                
                if progress_analysis.get('recommendations'):
                    recommendations = progress_analysis['recommendations']
                    self.logger.info(f"üìà –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —ç–ø–æ—Ö–∏ {epoch}: {recommendations.get('action', 'continue')}")
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —ç–ø–æ—Ö–∏: {e}")
        
        return current_hyperparams
    
    def on_batch_end(self, epoch: int, batch: int, metrics: Dict[str, float], 
                     model_outputs: Optional[Tuple] = None) -> Dict[str, Any]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ batch –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞.
        
        Args:
            epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏
            batch: –ù–æ–º–µ—Ä batch
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            model_outputs: –í—ã—Ö–æ–¥—ã –º–æ–¥–µ–ª–∏ (mel, gate, attention)
            
        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        analysis_result = {
            'continue_training': True,
            'quality_issues': [],
            'recommended_actions': []
        }
        
        if not self.is_initialized:
            return analysis_result
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —á–µ—Ä–µ–∑ Quality Controller
        if self.quality_controller and model_outputs:
            try:
                mel_outputs, gate_outputs, attention_weights = model_outputs[:3]
                
                quality_analysis = self.quality_controller.analyze_training_quality(
                    epoch=epoch,
                    metrics=metrics,
                    attention_weights=attention_weights,
                    gate_outputs=gate_outputs,
                    mel_outputs=mel_outputs
                )
                
                analysis_result['quality_score'] = quality_analysis.get('overall_quality_score', 0.5)
                analysis_result['quality_issues'] = quality_analysis.get('quality_issues', [])
                analysis_result['recommended_actions'] = quality_analysis.get('recommended_interventions', [])
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞
                high_severity_issues = [
                    issue for issue in analysis_result['quality_issues'] 
                    if issue.get('severity') == 'high'
                ]
                
                if high_severity_issues:
                    self.logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —ç–ø–æ—Ö–µ {epoch}")
                    for issue in high_severity_issues:
                        self.logger.warning(f"   - {issue.get('description', 'Unknown issue')}")
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
        
        return analysis_result
    
    def on_epoch_end(self, epoch: int, metrics: Dict[str, float], 
                     current_hyperparams: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π.
        
        Args:
            epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏
            metrics: –ú–µ—Ç—Ä–∏–∫–∏ —ç–ø–æ—Ö–∏
            current_hyperparams: –¢–µ–∫—É—â–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –†–µ—à–µ–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        decision_result = {
            'continue_training': True,
            'hyperparameter_updates': {},
            'early_stop': False,
            'reason': 'normal_progress'
        }
        
        if not self.is_initialized:
            return decision_result
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.training_metrics_history.append({
            'epoch': epoch,
            'metrics': metrics.copy(),
            'hyperparams': current_hyperparams.copy()
        })
        
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Early Stop Controller
        if self.early_stop_controller:
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä
                self.early_stop_controller.add_metrics(metrics)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É
                should_stop, stop_reason = self.early_stop_controller.should_stop_early(metrics)
                
                if should_stop:
                    decision_result['early_stop'] = True
                    decision_result['reason'] = stop_reason
                    decision_result['continue_training'] = False
                    self.logger.info(f"üõë –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞: {stop_reason}")
                    return decision_result
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
                recommendations = self.early_stop_controller.decide_next_step(current_hyperparams)
                
                if recommendations.get('action') != 'continue':
                    action = recommendations.get('action')
                    reason = recommendations.get('reason', 'Unknown')
                    
                    self.logger.info(f"üîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {action} - {reason}")
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    if 'hyperparameter_updates' in recommendations:
                        decision_result['hyperparameter_updates'] = recommendations['hyperparameter_updates']
                        
                        # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
                        updates = recommendations['hyperparameter_updates']
                        changes = [f"{k}: {current_hyperparams.get(k, 'N/A')} ‚Üí {v}" for k, v in updates.items()]
                        self.logger.info(f"   –ò–∑–º–µ–Ω–µ–Ω–∏—è: {', '.join(changes)}")
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –≤ Early Stop Controller: {e}")
        
        return decision_result
    
    def apply_quality_interventions(self, interventions: list, 
                                   current_hyperparams: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.
        
        Args:
            interventions: –°–ø–∏—Å–æ–∫ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤
            current_hyperparams: –¢–µ–∫—É—â–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            
        Returns:
            –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        updated_params = current_hyperparams.copy()
        
        if not self.quality_controller or not interventions:
            return updated_params
        
        for intervention in interventions:
            try:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ —á–µ—Ä–µ–∑ Quality Controller
                updated_params = self.quality_controller.apply_quality_intervention(
                    intervention, updated_params
                )
                
                self.logger.info(f"‚ú® –ü—Ä–∏–º–µ–Ω–µ–Ω–æ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ: {intervention.get('description', 'Unknown')}")
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞: {e}")
        
        return updated_params
    
    def on_training_complete(self, final_metrics: Dict[str, float], 
                           total_epochs: int, training_time_hours: float) -> Dict[str, Any]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Args:
            final_metrics: –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            total_epochs: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            training_time_hours: –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ —á–∞—Å–∞—Ö
            
        Returns:
            –ê–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        training_summary = {
            'training_success': True,
            'quality_assessment': 'good',
            'efficiency_score': 0.8,
            'recommendations_for_future': [],
            'smart_tuner_interventions': len(self.hyperparameter_adjustments)
        }
        
        if not self.is_initialized:
            return training_summary
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if self.epoch_optimizer:
                self.epoch_optimizer.save_optimization_result(
                    final_metrics, total_epochs, training_time_hours * 60
                )
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è
            if final_metrics.get('val_loss', float('inf')) < 3.0:
                training_summary['quality_assessment'] = 'excellent'
            elif final_metrics.get('val_loss', float('inf')) < 5.0:
                training_summary['quality_assessment'] = 'good'
            else:
                training_summary['quality_assessment'] = 'needs_improvement'
            
            # –û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            expected_time = total_epochs * 0.1  # –ü—Ä–∏–º–µ—Ä–Ω–æ 0.1 —á–∞—Å–∞ –Ω–∞ —ç–ø–æ—Ö—É
            efficiency = min(expected_time / training_time_hours, 1.0) if training_time_hours > 0 else 0.5
            training_summary['efficiency_score'] = efficiency
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–≤–æ–¥–∫—É –∫–∞—á–µ—Å—Ç–≤–∞
            if self.quality_controller:
                quality_summary = self.quality_controller.get_quality_summary()
                training_summary['quality_details'] = quality_summary
            
            self.logger.info(f"üéØ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: –∫–∞—á–µ—Å—Ç–≤–æ={training_summary['quality_assessment']}, "
                           f"—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å={efficiency:.2f}, –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤={len(self.hyperparameter_adjustments)}")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        
        return training_summary
    
    def get_training_recommendations(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞.
        
        Returns:
            –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –±—É–¥—É—â–∏—Ö –æ–±—É—á–µ–Ω–∏–π
        """
        recommendations = {
            'hyperparameter_suggestions': {},
            'training_strategy': 'standard',
            'expected_quality': 'good',
            'confidence': 0.7
        }
        
        if not self.is_initialized or not self.training_metrics_history:
            return recommendations
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –º–µ—Ç—Ä–∏–∫
            recent_metrics = self.training_metrics_history[-10:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–æ—Ö
            
            # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            avg_train_loss = np.mean([m['metrics'].get('train_loss', 0) for m in recent_metrics])
            avg_val_loss = np.mean([m['metrics'].get('val_loss', 0) for m in recent_metrics])
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if avg_val_loss < 3.0:
                recommendations['training_strategy'] = 'high_quality_focused'
                recommendations['expected_quality'] = 'excellent'
            elif avg_val_loss > 8.0:
                recommendations['training_strategy'] = 'stability_focused'
                recommendations['expected_quality'] = 'needs_improvement'
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
            if len(self.hyperparameter_adjustments) > 0:
                last_adjustment = self.hyperparameter_adjustments[-1]
                recommendations['hyperparameter_suggestions'] = last_adjustment.get('successful_params', {})
            
            self.logger.info(f"üìã –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è={recommendations['training_strategy']}")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        
        return recommendations
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Smart Tuner."""
        return self.is_initialized
    
    def get_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å Smart Tuner."""
        return {
            'initialized': self.is_initialized,
            'optimization_engine': self.optimization_engine is not None,
            'early_stop_controller': self.early_stop_controller is not None,
            'epoch_optimizer': self.epoch_optimizer is not None,
            'quality_controller': self.quality_controller is not None,
            'current_epoch': self.current_epoch,
            'interventions_count': len(self.hyperparameter_adjustments)
        } 