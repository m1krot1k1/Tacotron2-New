# üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Tacotron2-New

## üéØ –ù–æ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã Smart Tuner V2

### **Smart Tuner –ø–∞—Ä–∞–º–µ—Ç—Ä—ã**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `use_bucket_batching` | bool | `True` | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å bucket batching –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ |
| `force_model_reinit` | bool | `False` | –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö |
| `xavier_init` | bool | `True` | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤ |
| `use_audio_quality_enhancement` | bool | `True` | –í–∫–ª—é—á–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∞—É–¥–∏–æ |
| `use_mmi` | bool | `False` | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MMI loss |
| `use_guided_attn` | bool | `True` | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å guided attention |
| `guide_loss_weight` | float | `1.0` | –í–µ—Å guided attention loss |
| `guide_loss_initial_weight` | float | `1.0` | –ù–∞—á–∞–ª—å–Ω—ã–π –≤–µ—Å guided attention |
| `use_ddc_loss` | bool | `False` | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DDC loss |
| `ddc_loss_weight` | float | `0.1` | –í–µ—Å DDC loss |

### **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `grad_clip_thresh` | float | `1.0` | –ü–æ—Ä–æ–≥ –æ–±—Ä–µ–∑–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ |
| `dynamic_loss_scaling` | bool | `True` | –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ loss |
| `fp16_run` | bool | `False` | –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å FP16 –æ–±—É—á–µ–Ω–∏–µ |
| `emergency_restart_enabled` | bool | `True` | –í–∫–ª—é—á–∏—Ç—å —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ |
| `max_restart_attempts` | int | `3` | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–≤ |

### **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –¢–∏–ø | –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é | –û–ø–∏—Å–∞–Ω–∏–µ |
|----------|-----|--------------|----------|
| `telegram_monitoring` | bool | `True` | –í–∫–ª—é—á–∏—Ç—å Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ |
| `debug_reporting` | bool | `True` | –í–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É |
| `mlflow_logging` | bool | `True` | –í–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow |
| `tensorboard_logging` | bool | `True` | –í–∫–ª—é—á–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ TensorBoard |

## üöÄ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### **–ë–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ**
```bash
python train.py \
    -o output \
    -l logs \
    --hparams "learning_rate=1e-3,batch_size=32,use_guided_attn=True"
```

### **–û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**
```bash
python train.py \
    -o output \
    -l logs \
    --optimize-hyperparams \
    --n-trials 20 \
    --optimization-timeout 3600
```

### **Distributed –æ–±—É—á–µ–Ω–∏–µ**
```bash
python multiproc.py \
    train.py \
    -o output \
    -l logs \
    --n_gpus 4 \
    --hparams "distributed_run=True,batch_size=16"
```

### **–≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ**
```bash
python emergency_recovery.py
```

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —á–µ—Ä–µ–∑ CLI

### **–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ --hparams**
```bash
# –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
--hparams "learning_rate=1e-4,batch_size=16"

# Smart Tuner –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
--hparams "use_guided_attn=True,guide_loss_weight=2.0,use_mmi=True"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
--hparams "grad_clip_thresh=0.5,fp16_run=True,emergency_restart_enabled=True"

# –ü–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
--hparams "learning_rate=1e-4,batch_size=16,use_guided_attn=True,guide_loss_weight=2.0,use_mmi=True,grad_clip_thresh=0.5,fp16_run=True"
```

### **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**
```bash
# –ë–∞–∑–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (10 trials)
python train.py --optimize-hyperparams

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
python train.py \
    --optimize-hyperparams \
    --n-trials 50 \
    --optimization-timeout 7200 \
    -o output/optimization \
    -l logs/optimization

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
python train.py \
    --optimize-hyperparams \
    --n-trials 30 \
    --hparams "max_restart_attempts=5,emergency_restart_enabled=True"
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

### **Telegram —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è**
```bash
# –í–∫–ª—é—á–∏—Ç—å Telegram –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
--hparams "telegram_monitoring=True"

# –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
--hparams "telegram_notification_interval=1000"
```

### **Debug –æ—Ç—á–µ—Ç—ã**
```bash
# –í–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
--hparams "debug_reporting=True"

# –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –æ—Ç—á–µ—Ç–æ–≤
--hparams "debug_report_interval=500"
```

### **MLflow –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ**
```bash
# –í–∫–ª—é—á–∏—Ç—å MLflow
--hparams "mlflow_logging=True"

# –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
--hparams "mlflow_experiment_name=tacotron2_experiment"
```

## üõ°Ô∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

### **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ**
```bash
# –í–∫–ª—é—á–∏—Ç—å —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫
--hparams "emergency_restart_enabled=True,max_restart_attempts=3"

# –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ—Ä–æ–≥–∏ –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞
--hparams "nan_detection_threshold=1e-6,gradient_explosion_threshold=1000"
```

### **–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è**
```bash
# –°—Ç—Ä–æ–≥–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
--hparams "grad_clip_thresh=0.1"

# –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ loss
--hparams "dynamic_loss_scaling=True"

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ FP16 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
--hparams "fp16_run=False"
```

## üéØ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### **–î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏—è**
```bash
--hparams "learning_rate=1e-3,batch_size=8,use_guided_attn=True,guide_loss_weight=1.0,grad_clip_thresh=1.0,fp16_run=False"
```

### **–î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è**
```bash
--hparams "learning_rate=1e-4,batch_size=16,use_guided_attn=True,guide_loss_weight=2.0,use_mmi=True,grad_clip_thresh=0.5,fp16_run=True,emergency_restart_enabled=True"
```

### **–î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞**
```bash
--hparams "learning_rate=5e-5,batch_size=32,use_guided_attn=True,guide_loss_weight=3.0,use_mmi=True,use_ddc_loss=True,ddc_loss_weight=0.1,grad_clip_thresh=0.3,fp16_run=True,emergency_restart_enabled=True"
```

### **–î–ª—è distributed –æ–±—É—á–µ–Ω–∏—è**
```bash
--hparams "distributed_run=True,batch_size=8,learning_rate=1e-4,use_guided_attn=True,grad_clip_thresh=0.5,fp16_run=True"
```

## üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º

### **–ü—Ä–æ–±–ª–µ–º—ã —Å attention alignment**
```bash
# –£–≤–µ–ª–∏—á–∏—Ç—å guided attention weight
--hparams "use_guided_attn=True,guide_loss_weight=5.0"

# –°–Ω–∏–∑–∏—Ç—å learning rate
--hparams "learning_rate=1e-5"

# –£–≤–µ–ª–∏—á–∏—Ç—å batch size
--hparams "batch_size=32"
```

### **–ü—Ä–æ–±–ª–µ–º—ã —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏**
```bash
# –°—Ç—Ä–æ–≥–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
--hparams "grad_clip_thresh=0.1"

# –û—Ç–∫–ª—é—á–∏—Ç—å FP16
--hparams "fp16_run=False"

# –í–∫–ª—é—á–∏—Ç—å —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫
--hparams "emergency_restart_enabled=True"
```

### **–ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é**
```bash
# –£–º–µ–Ω—å—à–∏—Ç—å batch size
--hparams "batch_size=4"

# –í–∫–ª—é—á–∏—Ç—å bucket batching
--hparams "use_bucket_batching=True"

# –û—Ç–∫–ª—é—á–∏—Ç—å FP16
--hparams "fp16_run=False"
```

## üìà –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å Optuna HPO
python train.py --optimize-hyperparams --n-trials 50

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –≤—Ä–µ–º–µ–Ω–∏
python train.py --optimize-hyperparams --n-trials 20 --optimization-timeout 3600
```

### **–†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞**
```bash
# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ learning rates
for lr in 1e-3 1e-4 5e-5; do
    python train.py --hparams "learning_rate=$lr,batch_size=16"
done

# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å guided attention
for weight in 1.0 2.0 5.0; do
    python train.py --hparams "guide_loss_weight=$weight,use_guided_attn=True"
done
```

## üéâ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–≠—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –Ω–æ–≤—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã Smart Tuner V2. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫:

- `smart_tuner/config.yaml` - –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
- `smart_tuner/README.md` - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Smart Tuner
- `FINAL_ALL_FIXES_REPORT.md` - –æ—Ç—á–µ—Ç –æ –≤—Å–µ—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö 