#!/usr/bin/env python3
"""
üß™ –¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã guided attention

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é UnifiedGuidedAttentionLoss —Å Tacotron2Loss
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å Context-Aware Training Manager
‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—É—é –∑–∞–º–µ–Ω—É –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π
‚úÖ Emergency mode –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞
‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ –º–µ–∂–¥—É —Å–∏—Å—Ç–µ–º–∞–º–∏
"""

import logging
import sys
import traceback
import torch
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def test_imports():
    """üß™ –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤"""
    logger.info("üß™ –¢–µ—Å—Ç 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤")
    logger.info("-" * 50)
    
    try:
        from unified_guided_attention import UnifiedGuidedAttentionLoss, create_unified_guided_attention
        logger.info("‚úÖ UnifiedGuidedAttentionLoss –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
        
        from loss_function import Tacotron2Loss, create_enhanced_loss_function
        logger.info("‚úÖ Tacotron2Loss –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
        
        from context_aware_training_manager import ContextAwareTrainingManager, create_context_aware_manager
        logger.info("‚úÖ ContextAwareTrainingManager –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        logger.error(traceback.format_exc())
        return False

def test_loss_function_integration():
    """üß™ –¢–µ—Å—Ç 2: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å loss —Ñ—É–Ω–∫—Ü–∏–µ–π"""
    logger.info("\nüß™ –¢–µ—Å—Ç 2: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å loss —Ñ—É–Ω–∫—Ü–∏–µ–π")
    logger.info("-" * 50)
    
    try:
        from loss_function import Tacotron2Loss
        
        # –°–æ–∑–¥–∞–µ–º mock hparams
        class MockHParams:
            mel_loss_weight = 1.0
            gate_loss_weight = 1.0
            guide_loss_weight = 2.0
            spectral_loss_weight = 0.3
            perceptual_loss_weight = 0.2
            style_loss_weight = 0.1
            monotonic_loss_weight = 0.1
            guide_decay = 0.9999
            guide_sigma = 0.4
            adaptive_gate_threshold = True
            curriculum_teacher_forcing = True
            use_ddc = False
            ddc_consistency_weight = 0.5
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è UnifiedGuidedAttention
            guide_loss_initial_weight = 5.0
            guide_loss_min_weight = 0.1
            guide_loss_max_weight = 15.0
            guide_loss_decay_start = 2000
            guide_loss_decay_steps = 25000
            guide_loss_decay_factor = 3.0
            guide_sigma_initial = 0.1
            guide_sigma_peak = 0.4
            guide_sigma_final = 0.15
            guide_emergency_weight = 25.0
            attention_emergency_threshold = 0.02
            attention_recovery_threshold = 0.5
            use_context_aware_attention = True
        
        hparams = MockHParams()
        
        # –°–æ–∑–¥–∞–µ–º loss —Ñ—É–Ω–∫—Ü–∏—é
        loss_function = Tacotron2Loss(hparams)
        logger.info("‚úÖ Tacotron2Loss —Å–æ–∑–¥–∞–Ω")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
        if hasattr(loss_function, 'use_unified_guided') and loss_function.use_unified_guided:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è UnifiedGuidedAttentionLoss")
        else:
            logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è legacy guided attention")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_size, mel_len, text_len = 2, 100, 50
        mel_target = torch.rand(batch_size, mel_len, 80)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: [batch, mel_len, mel_channels]
        gate_target = torch.rand(batch_size, mel_len, 1)
        
        model_output = (
            torch.rand(batch_size, mel_len, 80),  # mel_out
            torch.rand(batch_size, mel_len, 80),  # mel_out_postnet
            torch.rand(batch_size, mel_len, 1),   # gate_out
            torch.rand(batch_size, mel_len, text_len)  # alignments
        )
        
        targets = (mel_target, gate_target)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass
        loss_components = loss_function(model_output, targets)
        logger.info(f"‚úÖ Forward pass —É—Å–ø–µ—à–µ–Ω. –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã loss: {len(loss_components)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
        if hasattr(loss_function, 'get_guided_attention_diagnostics'):
            diagnostics = loss_function.get_guided_attention_diagnostics()
            logger.info(f"üìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ guided attention: {diagnostics['system_type']}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å loss —Ñ—É–Ω–∫—Ü–∏–µ–π: {e}")
        logger.error(traceback.format_exc())
        return False

def test_context_aware_integration():
    """üß™ –¢–µ—Å—Ç 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Context-Aware Manager"""
    logger.info("\nüß™ –¢–µ—Å—Ç 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Context-Aware Manager")
    logger.info("-" * 50)
    
    try:
        from context_aware_training_manager import create_context_aware_manager
        from loss_function import Tacotron2Loss
        
        # –°–æ–∑–¥–∞–µ–º mock hparams
        class MockHParams:
            learning_rate = 1e-3
            guide_loss_weight = 4.5
            mel_loss_weight = 1.0
            gate_loss_weight = 1.0
            guide_loss_initial_weight = 5.0
            guide_loss_min_weight = 0.1
            guide_loss_max_weight = 15.0
            use_context_aware_attention = True
        
        hparams = MockHParams()
        
        # –°–æ–∑–¥–∞–µ–º Context-Aware Manager
        context_manager = create_context_aware_manager(hparams)
        logger.info("‚úÖ Context-Aware Manager —Å–æ–∑–¥–∞–Ω")
        
        # –°–æ–∑–¥–∞–µ–º loss —Ñ—É–Ω–∫—Ü–∏—é
        loss_function = Tacotron2Loss(hparams)
        logger.info("‚úÖ Tacotron2Loss —Å–æ–∑–¥–∞–Ω")
        
        # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—ã
        context_manager.integrate_with_loss_function(loss_function)
        logger.info("‚úÖ –°–∏—Å—Ç–µ–º—ã –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∞–¥–∞–ø—Ç–∞—Ü–∏—é
        batch_size, mel_len, text_len = 2, 100, 50
        model_output = (
            torch.rand(batch_size, mel_len, 80),  # mel_out
            torch.rand(batch_size, mel_len, 80),  # mel_out_postnet
            torch.rand(batch_size, mel_len, 1),   # gate_out
            torch.rand(batch_size, mel_len, text_len) * 0.01  # low attention –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è emergency mode
        )
        
        mel_target = torch.rand(batch_size, mel_len, 80)  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: [batch, mel_len, mel_channels]
        gate_target = torch.rand(batch_size, mel_len, 1)
        targets = (mel_target, gate_target)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        for step in range(5):
            loss_components = loss_function(model_output, targets)
            
            # –°–∏–º—É–ª–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            metrics = {
                'loss': sum(loss_components).item() if isinstance(loss_components, (tuple, list)) else loss_components.item(),
                'attention_diagonality': 0.01 + step * 0.01,  # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
                'grad_norm': 1.0,
                'gate_accuracy': 0.5,
                'epoch': 0
            }
            
            # –ê–¥–∞–ø—Ç–∞—Ü–∏—è —á–µ—Ä–µ–∑ Context-Aware Manager
            adaptations = context_manager.analyze_and_adapt(step, metrics)
            
            if step == 0:
                logger.info(f"üìä –ù–∞—á–∞–ª—å–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: {list(adaptations.keys())}")
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
        diagnostics = loss_function.get_guided_attention_diagnostics()
        logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: emergency_mode={diagnostics.get('emergency_mode', 'unknown')}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Context-Aware Manager: {e}")
        logger.error(traceback.format_exc())
        return False

def test_emergency_mode():
    """üß™ –¢–µ—Å—Ç 4: Emergency mode –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞"""
    logger.info("\nüß™ –¢–µ—Å—Ç 4: Emergency mode –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞")
    logger.info("-" * 50)
    
    try:
        from unified_guided_attention import create_unified_guided_attention
        
        # –°–æ–∑–¥–∞–µ–º mock hparams
        class MockHParams:
            guide_loss_initial_weight = 5.0
            guide_loss_min_weight = 0.1
            guide_loss_max_weight = 15.0
            guide_loss_decay_start = 2000
            guide_loss_decay_steps = 25000
            guide_loss_decay_factor = 3.0
            guide_sigma_initial = 0.1
            guide_sigma_peak = 0.4
            guide_sigma_final = 0.15
            guide_emergency_weight = 25.0
            attention_emergency_threshold = 0.02
            attention_recovery_threshold = 0.5
            use_context_aware_attention = False
        
        hparams = MockHParams()
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        guided_attention = create_unified_guided_attention(hparams)
        logger.info("‚úÖ UnifiedGuidedAttentionLoss —Å–æ–∑–¥–∞–Ω")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∏–º attention
        batch_size, mel_len, text_len = 1, 50, 25
        low_attention = torch.rand(batch_size, mel_len, text_len) * 0.001  # –û—á–µ–Ω—å –Ω–∏–∑–∫–æ–µ attention
        
        model_output = (
            torch.rand(batch_size, mel_len, 80),
            torch.rand(batch_size, mel_len, 80),
            torch.rand(batch_size, mel_len, 1),
            low_attention
        )
        
        # –ü–µ—Ä–≤—ã–π forward pass - –¥–æ–ª–∂–µ–Ω –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å emergency mode
        loss1 = guided_attention(model_output)
        diagnostics1 = guided_attention.get_diagnostics()
        logger.info(f"üìä –ü–æ—Å–ª–µ –Ω–∏–∑–∫–æ–≥–æ attention: emergency_mode={diagnostics1['emergency_mode']}, weight={diagnostics1['current_weight']}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å —Ö–æ—Ä–æ—à–∏–º attention
        high_attention = torch.zeros(batch_size, mel_len, text_len)
        # –°–æ–∑–¥–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ–µ attention
        for i in range(min(mel_len, text_len)):
            high_attention[0, i, i] = 1.0
        
        model_output_good = (
            torch.rand(batch_size, mel_len, 80),
            torch.rand(batch_size, mel_len, 80),
            torch.rand(batch_size, mel_len, 1),
            high_attention
        )
        
        # –ù–µ—Å–∫–æ–ª—å–∫–æ forward pass —Å —Ö–æ—Ä–æ—à–∏–º attention
        for _ in range(5):
            loss2 = guided_attention(model_output_good)
        
        diagnostics2 = guided_attention.get_diagnostics()
        logger.info(f"üìä –ü–æ—Å–ª–µ —Ö–æ—Ä–æ—à–µ–≥–æ attention: emergency_mode={diagnostics2['emergency_mode']}, weight={diagnostics2['current_weight']}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ emergency mode –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è/–¥–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç—Å—è
        if diagnostics1['emergency_mode'] and not diagnostics2['emergency_mode']:
            logger.info("‚úÖ Emergency mode –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        else:
            logger.warning("‚ö†Ô∏è Emergency mode —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è emergency mode: {e}")
        logger.error(traceback.format_exc())
        return False

def test_performance_comparison():
    """üß™ –¢–µ—Å—Ç 5: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    logger.info("\nüß™ –¢–µ—Å—Ç 5: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    logger.info("-" * 50)
    
    try:
        import time
        from loss_function import Tacotron2Loss, GuidedAttentionLoss
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        batch_size, mel_len, text_len = 4, 200, 100
        model_output = (
            torch.rand(batch_size, mel_len, 80),
            torch.rand(batch_size, mel_len, 80),
            torch.rand(batch_size, mel_len, 1),
            torch.rand(batch_size, mel_len, text_len)
        )
        
        # –°–æ–∑–¥–∞–µ–º mock hparams
        class MockHParams:
            mel_loss_weight = 1.0
            gate_loss_weight = 1.0
            guide_loss_weight = 2.0
            spectral_loss_weight = 0.3
            perceptual_loss_weight = 0.2
            style_loss_weight = 0.1
            monotonic_loss_weight = 0.1
            guide_decay = 0.9999
            guide_sigma = 0.4
            adaptive_gate_threshold = True
            curriculum_teacher_forcing = True
            use_ddc = False
            ddc_consistency_weight = 0.5
            
            guide_loss_initial_weight = 5.0
            guide_loss_min_weight = 0.1
            guide_loss_max_weight = 15.0
            guide_loss_decay_start = 2000
            guide_loss_decay_steps = 25000
            guide_loss_decay_factor = 3.0
            guide_sigma_initial = 0.1
            guide_sigma_peak = 0.4
            guide_sigma_final = 0.15
            guide_emergency_weight = 25.0
            attention_emergency_threshold = 0.02
            attention_recovery_threshold = 0.5
            use_context_aware_attention = False  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        hparams = MockHParams()
        
        # –¢–µ—Å—Ç –Ω–æ–≤–æ–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
        loss_function_new = Tacotron2Loss(hparams)
        
        start_time = time.time()
        for _ in range(10):
            if hasattr(loss_function_new, 'unified_guided_attention') and loss_function_new.unified_guided_attention:
                loss_new = loss_function_new.unified_guided_attention(model_output)
        unified_time = time.time() - start_time
        
        # –¢–µ—Å—Ç legacy —Å–∏—Å—Ç–µ–º—ã
        legacy_guided = GuidedAttentionLoss()
        
        start_time = time.time()
        for _ in range(10):
            loss_legacy = legacy_guided(model_output)
        legacy_time = time.time() - start_time
        
        speedup = legacy_time / unified_time if unified_time > 0 else float('inf')
        
        logger.info(f"üìä –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
        logger.info(f"   Unified system: {unified_time:.4f}s")
        logger.info(f"   Legacy system:  {legacy_time:.4f}s")
        logger.info(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {speedup:.1f}x")
        
        if speedup > 2.0:
            logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ")
        elif speedup > 1.0:
            logger.info("‚úÖ –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –±—ã—Å—Ç—Ä–µ–µ")
        else:
            logger.warning("‚ö†Ô∏è –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω—É–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")
        logger.error(traceback.format_exc())
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger.info("üöÄ –ù–∞—á–∞–ª–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã guided attention")
    logger.info("=" * 80)
    
    tests = [
        ("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤", test_imports),
        ("–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å loss —Ñ—É–Ω–∫—Ü–∏–µ–π", test_loss_function_integration),
        ("–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Context-Aware Manager", test_context_aware_integration),
        ("Emergency mode –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞", test_emergency_mode),
        ("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", test_performance_comparison),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            if result:
                passed += 1
        except Exception as e:
            logger.error(f"‚ùå –¢–µ—Å—Ç '{test_name}' —É–ø–∞–ª —Å –æ—à–∏–±–∫–æ–π: {e}")
            logger.error(traceback.format_exc())
    
    logger.info("\n" + "=" * 80)
    logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
    logger.info(f"‚úÖ –ü—Ä–æ–π–¥–µ–Ω–æ: {passed}/{total}")
    logger.info(f"‚ùå –ü—Ä–æ–≤–∞–ª–µ–Ω–æ: {total - passed}/{total}")
    
    if passed == total:
        logger.info("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        logger.info("üî• –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ guided attention –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        logger.info("üöÄ –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ –µ–¥–∏–Ω—É—é —É–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É!")
        print("\nüèÜ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("–°–∏—Å—Ç–µ–º–∞ guided attention —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
        return True
    else:
        logger.error("üí• –ù–ï–ö–û–¢–û–†–´–ï –¢–ï–°–¢–´ –ü–†–û–í–ê–õ–ò–õ–ò–°–¨")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 