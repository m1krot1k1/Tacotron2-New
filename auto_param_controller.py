class AutoParamController:
    """–ü—Ä–æ—Å—Ç–æ–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.

    –ê–ª–≥–æ—Ä–∏—Ç–º (–º–æ–∂–Ω–æ —É—Å–ª–æ–∂–Ω—è—Ç—å –ø–æ–∑–∂–µ):
      ‚Ä¢ –µ—Å–ª–∏ –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å attention (alignment_score) < threshold -> —É–¥–≤–æ–∏—Ç—å –≤–µ—Å guided attention (max 10.0).
      ‚Ä¢ –µ—Å–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π validation loss –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –ø—Ä–æ–≤–µ—Ä–æ–∫ –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è 3 –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–¥—Ä—è–¥ ‚Äî —Å–Ω–∏–∂–∞—Ç—å LR –≤–¥–≤–æ–µ (–Ω–µ –Ω–∏–∂–µ min_lr).
    """

    def __init__(self, optimizer, guide_loss, hparams, writer=None,
                 align_threshold: float = 0.15,
                 max_guided_weight: float = 10.0,
                 min_lr: float = 1e-6,
                 lr_decay_factor: float = 0.5,
                 patience: int = 3):
        self.optimizer = optimizer
        self.guide_loss = guide_loss
        self.hparams = hparams
        self.writer = writer
        self.align_threshold = align_threshold
        self.max_guided_weight = max_guided_weight
        self.min_lr = min_lr
        self.lr_decay_factor = lr_decay_factor
        self.patience = patience

        self._best_val_loss = None
        self._no_improve_steps = 0

    # -----------------------------------------------------
    def _log(self, message: str):
        if self.writer is not None:
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ TensorBoard
            try:
                self.writer.add_text("AutoTune", message)
            except Exception:
                pass
        print(message)

    # -----------------------------------------------------
    def after_validation(self, iteration: int, val_loss: float, alignment_score: float):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
        # 1. Guided Attention Weight
        try:
            if alignment_score is not None and alignment_score < self.align_threshold:
                new_weight = min(self.max_guided_weight, self.guide_loss.get_weight() * 2)
                if new_weight > self.guide_loss.get_weight():
                    # –æ–±–Ω–æ–≤–ª—è–µ–º GuideLoss –∏ hparams
                    self.guide_loss.alpha = new_weight
                    self.guide_loss.current_weight = new_weight
                    self.hparams.guided_attn_weight = new_weight
                    self._log(f"üîß AutoTune: –£–≤–µ–ª–∏—á–∏–ª–∏ guided_attn_weight –¥–æ {new_weight} (iter {iteration})")
                    if self.writer:
                        self.writer.add_scalar("autotune.guided_attn_weight", new_weight, iteration)
        except Exception as e:
            self._log(f"‚ö†Ô∏è AutoTune (guided_attn) –æ—à–∏–±–∫–∞: {e}")

        # 2. Learning rate scheduler –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ç—Å—É—Ç—Å–≤–∏—è —É–ª—É—á—à–µ–Ω–∏–π val_loss
        try:
            if self._best_val_loss is None or val_loss < self._best_val_loss - 1e-4:
                self._best_val_loss = val_loss
                self._no_improve_steps = 0
            else:
                self._no_improve_steps += 1

            if self._no_improve_steps >= self.patience:
                # —Å–Ω–∏–∂–∞–µ–º LR
                for group in self.optimizer.param_groups:
                    new_lr = max(self.min_lr, group['lr'] * self.lr_decay_factor)
                    if new_lr < group['lr']:
                        group['lr'] = new_lr
                self._no_improve_steps = 0
                # –û–±–Ω–æ–≤–ª—è–µ–º hparams.learning_rate –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                self.hparams.learning_rate = self.optimizer.param_groups[0]['lr']
                self._log(f"üîß AutoTune: –°–Ω–∏–∑–∏–ª–∏ learning rate –¥–æ {self.hparams.learning_rate} (iter {iteration})")
                if self.writer:
                    self.writer.add_scalar("autotune.learning_rate", self.hparams.learning_rate, iteration)
        except Exception as e:
            self._log(f"‚ö†Ô∏è AutoTune (lr) –æ—à–∏–±–∫–∞: {e}") 